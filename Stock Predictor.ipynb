{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9958be04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from yahoo_fin import stock_info as si\n",
    "from collections import deque\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ea00d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed, so we can get the same results after rerunning several times\n",
    "np.random.seed(1234)\n",
    "tf.random.set_seed(1234)\n",
    "random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ad33da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_STEPS = 50\n",
    "\n",
    "LOOKUP_STEP = 1\n",
    "\n",
    "SCALE = True\n",
    "scale_str = f\"scale-{int(SCALE)}\"\n",
    "\n",
    "SHUFFLE = True\n",
    "shuffle_str = f\"shuffle-{int(SHUFFLE)}\"\n",
    "\n",
    "SPLIT_BY_DATE = False\n",
    "split_by_date_str = f\"split_date-{int(SPLIT_BY_DATE)}\"\n",
    "\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "FEATURE_COLUMNS = [\"adjclose\", \"volume\", \"open\", \"high\", \"low\"]\n",
    "\n",
    "date_now = time.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# model parameters\n",
    "\n",
    "N_LAYERS = 3\n",
    "CELL = LSTM\n",
    "UNITS = 256\n",
    "DROPOUT = 0.7\n",
    "BIDIRECTIONAL = False\n",
    "\n",
    "# training parameters\n",
    "\n",
    "LOSS = \"huber_loss\"\n",
    "OPTIMIZER = \"adam\"\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 800\n",
    "epochs = f\"epochs-{int(EPOCHS)}\"\n",
    "\n",
    "# Choose Ticker\n",
    "ticker = \"SPY\"\n",
    "ticker_data_filename = os.path.join(\"data\", f\"{ticker}_{date_now}.csv\")\n",
    "\n",
    "# model name to save, making it as unique as possible based on parameters\n",
    "model_name = f\"{date_now}_{ticker}-{shuffle_str}-{scale_str}-{split_by_date_str}-{epochs}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41992bf",
   "metadata": {},
   "source": [
    "* ticker (str/pd.DataFrame)\n",
    "* n_steps (int) - default = 50\n",
    "* scale (bool) - default = True\n",
    "* shuffle (bool) - default = True\n",
    "* lookup_step (int) - default = 1 (e.g next day)\n",
    "* split_by_date (bool)\n",
    "* test_size (float) - default = 0.2 \n",
    "* feature_columns (list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7339ca1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_in_unison(a, b):\n",
    "    # shuffle two arrays in the same way\n",
    "    state = np.random.get_state()\n",
    "    np.random.shuffle(a)\n",
    "    np.random.set_state(state)\n",
    "    np.random.shuffle(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26f08fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(ticker, n_steps=50, scale=True, shuffle=True, lookup_step=1, split_by_date=True,\n",
    "                test_size=0.2, feature_columns=['adjclose', 'volume', 'open', 'high', 'low']):\n",
    "    # see if ticker is already a loaded stock from yahoo finance\n",
    "    if isinstance(ticker, str):\n",
    "        # load it from yahoo_fin library\n",
    "        df = si.get_data(ticker)\n",
    "    elif isinstance(ticker, pd.DataFrame):\n",
    "        # already loaded, use it directly\n",
    "        df = ticker\n",
    "    else:\n",
    "        raise TypeError(\"ticker can be either a str or a `pd.DataFrame` instances\")\n",
    "\n",
    "    # this will contain all the elements we want to return from this function\n",
    "    result = {}\n",
    "    # we will also return the original dataframe itself\n",
    "    result['df'] = df.copy()\n",
    "\n",
    "    # add date as a column\n",
    "    if \"date\" not in df.columns:\n",
    "        df[\"date\"] = df.index\n",
    "\n",
    "    if scale:\n",
    "        column_scaler = {}\n",
    "        # scale the data (prices) from 0 to 1\n",
    "        for column in feature_columns:\n",
    "            scaler = preprocessing.MinMaxScaler()\n",
    "            df[column] = scaler.fit_transform(np.expand_dims(df[column].values, axis=1))\n",
    "            column_scaler[column] = scaler\n",
    "\n",
    "        # add the MinMaxScaler instances to the result returned\n",
    "        result[\"column_scaler\"] = column_scaler\n",
    "\n",
    "    # add the target column (label) by shifting by `lookup_step`\n",
    "    df['future'] = df['adjclose'].shift(-lookup_step)\n",
    "\n",
    "    last_sequence = np.array(df[feature_columns].tail(lookup_step))\n",
    "    \n",
    "    # drop NaNs\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    sequence_data = []\n",
    "    sequences = deque(maxlen=n_steps)\n",
    "\n",
    "    for entry, target in zip(df[feature_columns + [\"date\"]].values, df['future'].values):\n",
    "        sequences.append(entry)\n",
    "        if len(sequences) == n_steps:\n",
    "            sequence_data.append([np.array(sequences), target])\n",
    "\n",
    "    last_sequence = list([s[:len(feature_columns)] for s in sequences]) + list(last_sequence)\n",
    "    last_sequence = np.array(last_sequence).astype(np.float32)\n",
    "    result['last_sequence'] = last_sequence\n",
    "    \n",
    "    # construct the X's and y's\n",
    "    X, y = [], []\n",
    "    for seq, target in sequence_data:\n",
    "        X.append(seq)\n",
    "        y.append(target)\n",
    "\n",
    "    # convert to numpy arrays\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    # split dataset by date \n",
    "    if split_by_date:\n",
    "        train_samples = int((1 - test_size) * len(X))\n",
    "        result[\"X_train\"] = X[:train_samples]\n",
    "        result[\"y_train\"] = y[:train_samples]\n",
    "        result[\"X_test\"]  = X[train_samples:]\n",
    "        result[\"y_test\"]  = y[train_samples:]\n",
    "        if shuffle:\n",
    "            # shuffle the datasets for training (if shuffle parameter is set)\n",
    "            shuffle_in_unison(result[\"X_train\"], result[\"y_train\"])\n",
    "            shuffle_in_unison(result[\"X_test\"], result[\"y_test\"])\n",
    "    else:    \n",
    "        # split the dataset randomly\n",
    "        result[\"X_train\"], result[\"X_test\"], result[\"y_train\"], result[\"y_test\"] = train_test_split(X, y, \n",
    "                                                                                test_size=test_size, shuffle=shuffle)\n",
    "\n",
    "    # get the list of test set dates\n",
    "    dates = result[\"X_test\"][:, -1, -1]\n",
    "    # retrieve test features from the original dataframe\n",
    "    result[\"test_df\"] = result[\"df\"].loc[dates]\n",
    "    # remove duplicated dates in the testing dataframe\n",
    "    result[\"test_df\"] = result[\"test_df\"][~result[\"test_df\"].index.duplicated(keep='first')]\n",
    "    # remove dates from the training/testing sets & convert to float32\n",
    "    result[\"X_train\"] = result[\"X_train\"][:, :, :len(feature_columns)].astype(np.float32)\n",
    "    result[\"X_test\"] = result[\"X_test\"][:, :, :len(feature_columns)].astype(np.float32)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3492552c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(sequence_length, n_features, units=256, cell=LSTM, n_layers=2, dropout=0.3,\n",
    "                loss=\"mean_absolute_error\", optimizer=\"rmsprop\", bidirectional=False):\n",
    "    model = Sequential()\n",
    "    for i in range(n_layers):\n",
    "        if i == 0:\n",
    "            # first layer\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=True), batch_input_shape=(None, sequence_length, n_features)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=True, batch_input_shape=(None, sequence_length, n_features)))\n",
    "        elif i == n_layers - 1:\n",
    "            # last layer\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=False)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=False))\n",
    "        else:\n",
    "            # hidden layers\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=True)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=True))\n",
    "        # add dropout after each layer\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation=\"linear\"))\n",
    "    model.compile(loss=loss, metrics=[\"mean_absolute_error\"], optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37561445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graph(test_df):\n",
    "    \"\"\"\n",
    "    This function plots true close price along with predicted close price\n",
    "    with blue and red colors respectively\n",
    "    \"\"\"\n",
    "    plt.plot(test_df[f'true_adjclose_{LOOKUP_STEP}'], c='b')\n",
    "    plt.plot(test_df[f'adjclose_{LOOKUP_STEP}'], c='r')\n",
    "    plt.xlabel(\"Days\")\n",
    "    plt.ylabel(\"Price\")\n",
    "    plt.legend([\"Actual Price\", \"Predicted Price\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99a9b78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_df(model, data):\n",
    "    buy_profit  = lambda current, pred_future, true_future: true_future - current if pred_future > current else 0\n",
    "    sell_profit = lambda current, pred_future, true_future: current - true_future if pred_future < current else 0\n",
    "    X_test = data[\"X_test\"]\n",
    "    y_test = data[\"y_test\"]\n",
    "    # perform prediction and get prices\n",
    "    y_pred = model.predict(X_test)\n",
    "    if SCALE:\n",
    "        y_test = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(np.expand_dims(y_test, axis=0)))\n",
    "        y_pred = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(y_pred))\n",
    "    test_df = data[\"test_df\"]\n",
    "    # add predicted future prices to the dataframe\n",
    "    test_df[f\"adjclose_{LOOKUP_STEP}\"] = y_pred\n",
    "    # add true future prices to the dataframe\n",
    "    test_df[f\"true_adjclose_{LOOKUP_STEP}\"] = y_test\n",
    "    # sort the dataframe by date\n",
    "    test_df.sort_index(inplace=True)\n",
    "    final_df = test_df\n",
    "    # add the buy profit column\n",
    "    final_df[\"buy_profit\"] = list(map(buy_profit, \n",
    "                                    final_df[\"adjclose\"], \n",
    "                                    final_df[f\"adjclose_{LOOKUP_STEP}\"], \n",
    "                                    final_df[f\"true_adjclose_{LOOKUP_STEP}\"])\n",
    "                                    # since we don't have profit for last sequence, add 0's\n",
    "                                    )\n",
    "    # add the sell profit column\n",
    "    final_df[\"sell_profit\"] = list(map(sell_profit, \n",
    "                                    final_df[\"adjclose\"], \n",
    "                                    final_df[f\"adjclose_{LOOKUP_STEP}\"], \n",
    "                                    final_df[f\"true_adjclose_{LOOKUP_STEP}\"])\n",
    "                                    # since we don't have profit for last sequence, add 0's\n",
    "                                    )\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3313f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data):\n",
    "    # retrieve the last sequence from data\n",
    "    last_sequence = data[\"last_sequence\"][-N_STEPS:]\n",
    "    # expand dimension\n",
    "    last_sequence = np.expand_dims(last_sequence, axis=0)\n",
    "    # get the prediction (scaled from 0 to 1)\n",
    "    prediction = model.predict(last_sequence)\n",
    "    # get the price (by inverting the scaling)\n",
    "    if SCALE:\n",
    "        predicted_price = data[\"column_scaler\"][\"adjclose\"].inverse_transform(prediction)[0][0]\n",
    "    else:\n",
    "        predicted_price = prediction[0][0]\n",
    "    return predicted_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e848f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create these folders if they does not exist\n",
    "if not os.path.isdir(\"results\"):\n",
    "    os.mkdir(\"results\")\n",
    "\n",
    "if not os.path.isdir(\"logs\"):\n",
    "    os.mkdir(\"logs\")\n",
    "\n",
    "if not os.path.isdir(\"data\"):\n",
    "    os.mkdir(\"data\")\n",
    "\n",
    "# load the data\n",
    "data = load_data(ticker, N_STEPS, scale=SCALE, split_by_date=SPLIT_BY_DATE, \n",
    "                shuffle=SHUFFLE, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE, \n",
    "                feature_columns=FEATURE_COLUMNS)\n",
    "\n",
    "# save the dataframe\n",
    "data[\"df\"].to_csv(ticker_data_filename)\n",
    "\n",
    "# construct the model\n",
    "model = create_model(N_STEPS, len(FEATURE_COLUMNS), loss=LOSS, units=UNITS, cell=CELL, n_layers=N_LAYERS,\n",
    "                    dropout=DROPOUT, optimizer=OPTIMIZER, bidirectional=BIDIRECTIONAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07464e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/800\n",
      "90/90 [==============================] - 65s 672ms/step - loss: 0.0043 - mean_absolute_error: 0.0530 - val_loss: 8.5656e-05 - val_mean_absolute_error: 0.0100\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00009, saving model to results\\2021-09-07_SPY-shuffle-1-scale-1-split_date-0-epochs-800.h5\n",
      "Epoch 2/800\n",
      "90/90 [==============================] - 61s 681ms/step - loss: 0.0012 - mean_absolute_error: 0.0306 - val_loss: 2.1244e-04 - val_mean_absolute_error: 0.0191\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.00009\n",
      "Epoch 3/800\n",
      "90/90 [==============================] - 50s 558ms/step - loss: 0.0010 - mean_absolute_error: 0.0282 - val_loss: 9.8582e-05 - val_mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00009\n",
      "Epoch 4/800\n",
      "90/90 [==============================] - 54s 601ms/step - loss: 9.6950e-04 - mean_absolute_error: 0.0284 - val_loss: 1.5911e-04 - val_mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00009\n",
      "Epoch 5/800\n",
      "90/90 [==============================] - 43s 478ms/step - loss: 7.9881e-04 - mean_absolute_error: 0.0256 - val_loss: 2.9712e-04 - val_mean_absolute_error: 0.0178\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00009\n",
      "Epoch 6/800\n",
      "90/90 [==============================] - 42s 469ms/step - loss: 7.6221e-04 - mean_absolute_error: 0.0249 - val_loss: 4.5042e-04 - val_mean_absolute_error: 0.0197\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00009\n",
      "Epoch 7/800\n",
      "90/90 [==============================] - 43s 475ms/step - loss: 7.0569e-04 - mean_absolute_error: 0.0243 - val_loss: 2.2968e-04 - val_mean_absolute_error: 0.0159\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00009\n",
      "Epoch 8/800\n",
      "90/90 [==============================] - 42s 465ms/step - loss: 9.2162e-04 - mean_absolute_error: 0.0278 - val_loss: 1.6544e-04 - val_mean_absolute_error: 0.0157\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00009\n",
      "Epoch 9/800\n",
      "90/90 [==============================] - 40s 443ms/step - loss: 6.9362e-04 - mean_absolute_error: 0.0228 - val_loss: 1.2362e-04 - val_mean_absolute_error: 0.0110\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00009\n",
      "Epoch 10/800\n",
      "90/90 [==============================] - 38s 417ms/step - loss: 6.5422e-04 - mean_absolute_error: 0.0228 - val_loss: 5.6069e-05 - val_mean_absolute_error: 0.0082\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00009 to 0.00006, saving model to results\\2021-09-07_SPY-shuffle-1-scale-1-split_date-0-epochs-800.h5\n",
      "Epoch 11/800\n",
      "90/90 [==============================] - 45s 500ms/step - loss: 6.2483e-04 - mean_absolute_error: 0.0221 - val_loss: 4.1875e-05 - val_mean_absolute_error: 0.0061\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00006 to 0.00004, saving model to results\\2021-09-07_SPY-shuffle-1-scale-1-split_date-0-epochs-800.h5\n",
      "Epoch 12/800\n",
      "90/90 [==============================] - 39s 433ms/step - loss: 5.6975e-04 - mean_absolute_error: 0.0214 - val_loss: 1.3277e-04 - val_mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00004\n",
      "Epoch 13/800\n",
      "90/90 [==============================] - 40s 440ms/step - loss: 6.1793e-04 - mean_absolute_error: 0.0222 - val_loss: 2.9463e-04 - val_mean_absolute_error: 0.0163\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00004\n",
      "Epoch 14/800\n",
      "90/90 [==============================] - 40s 448ms/step - loss: 6.2477e-04 - mean_absolute_error: 0.0225 - val_loss: 0.0012 - val_mean_absolute_error: 0.0347\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00004\n",
      "Epoch 15/800\n",
      "90/90 [==============================] - 37s 417ms/step - loss: 5.4844e-04 - mean_absolute_error: 0.0216 - val_loss: 9.4074e-05 - val_mean_absolute_error: 0.0101\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00004\n",
      "Epoch 16/800\n",
      "90/90 [==============================] - 36s 404ms/step - loss: 5.1350e-04 - mean_absolute_error: 0.0198 - val_loss: 3.1202e-05 - val_mean_absolute_error: 0.0049\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00004 to 0.00003, saving model to results\\2021-09-07_SPY-shuffle-1-scale-1-split_date-0-epochs-800.h5\n",
      "Epoch 17/800\n",
      "90/90 [==============================] - 38s 427ms/step - loss: 4.7450e-04 - mean_absolute_error: 0.0198 - val_loss: 1.3212e-04 - val_mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00003\n",
      "Epoch 18/800\n",
      "90/90 [==============================] - 36s 399ms/step - loss: 4.8481e-04 - mean_absolute_error: 0.0199 - val_loss: 6.6738e-05 - val_mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00003\n",
      "Epoch 19/800\n",
      "90/90 [==============================] - 41s 456ms/step - loss: 4.9117e-04 - mean_absolute_error: 0.0201 - val_loss: 2.9369e-05 - val_mean_absolute_error: 0.0048\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00003 to 0.00003, saving model to results\\2021-09-07_SPY-shuffle-1-scale-1-split_date-0-epochs-800.h5\n",
      "Epoch 20/800\n",
      "90/90 [==============================] - 38s 418ms/step - loss: 4.3616e-04 - mean_absolute_error: 0.0187 - val_loss: 4.8731e-05 - val_mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00003\n",
      "Epoch 21/800\n",
      "90/90 [==============================] - 46s 509ms/step - loss: 4.8666e-04 - mean_absolute_error: 0.0194 - val_loss: 1.2851e-04 - val_mean_absolute_error: 0.0102\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00003\n",
      "Epoch 22/800\n",
      "90/90 [==============================] - 44s 492ms/step - loss: 4.4511e-04 - mean_absolute_error: 0.0188 - val_loss: 3.7857e-05 - val_mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00003\n",
      "Epoch 23/800\n",
      "90/90 [==============================] - 46s 517ms/step - loss: 4.6343e-04 - mean_absolute_error: 0.0193 - val_loss: 7.8156e-05 - val_mean_absolute_error: 0.0095\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00003\n",
      "Epoch 24/800\n",
      "90/90 [==============================] - 42s 465ms/step - loss: 4.5076e-04 - mean_absolute_error: 0.0192 - val_loss: 6.8718e-05 - val_mean_absolute_error: 0.0091\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00003\n",
      "Epoch 25/800\n",
      "90/90 [==============================] - 40s 448ms/step - loss: 5.2014e-04 - mean_absolute_error: 0.0205 - val_loss: 3.5282e-05 - val_mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00003\n",
      "Epoch 26/800\n",
      "90/90 [==============================] - 40s 450ms/step - loss: 4.7185e-04 - mean_absolute_error: 0.0195 - val_loss: 3.1113e-05 - val_mean_absolute_error: 0.0051\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00003\n",
      "Epoch 27/800\n",
      "90/90 [==============================] - 40s 449ms/step - loss: 5.4618e-04 - mean_absolute_error: 0.0212 - val_loss: 2.3408e-04 - val_mean_absolute_error: 0.0145\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00003\n",
      "Epoch 28/800\n",
      "90/90 [==============================] - 42s 462ms/step - loss: 4.4788e-04 - mean_absolute_error: 0.0192 - val_loss: 6.9241e-05 - val_mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00003\n",
      "Epoch 29/800\n",
      "90/90 [==============================] - 41s 455ms/step - loss: 4.6198e-04 - mean_absolute_error: 0.0192 - val_loss: 2.0967e-04 - val_mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00003\n",
      "Epoch 30/800\n",
      "90/90 [==============================] - 45s 502ms/step - loss: 4.8817e-04 - mean_absolute_error: 0.0202 - val_loss: 3.0343e-04 - val_mean_absolute_error: 0.0171\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00003\n",
      "Epoch 31/800\n",
      "90/90 [==============================] - 46s 510ms/step - loss: 4.5307e-04 - mean_absolute_error: 0.0190 - val_loss: 1.2784e-04 - val_mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00003\n",
      "Epoch 32/800\n",
      "90/90 [==============================] - 48s 532ms/step - loss: 4.3272e-04 - mean_absolute_error: 0.0190 - val_loss: 5.9129e-05 - val_mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00003\n",
      "Epoch 33/800\n",
      "90/90 [==============================] - 44s 492ms/step - loss: 3.9083e-04 - mean_absolute_error: 0.0180 - val_loss: 2.3336e-04 - val_mean_absolute_error: 0.0163\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00003\n",
      "Epoch 34/800\n",
      "90/90 [==============================] - 44s 493ms/step - loss: 4.0399e-04 - mean_absolute_error: 0.0185 - val_loss: 1.1135e-04 - val_mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00003\n",
      "Epoch 35/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 40s 445ms/step - loss: 5.0794e-04 - mean_absolute_error: 0.0207 - val_loss: 1.4216e-04 - val_mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00003\n",
      "Epoch 36/800\n",
      "90/90 [==============================] - 42s 463ms/step - loss: 4.2166e-04 - mean_absolute_error: 0.0187 - val_loss: 7.3477e-05 - val_mean_absolute_error: 0.0102\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00003\n",
      "Epoch 37/800\n",
      "90/90 [==============================] - 44s 485ms/step - loss: 4.5102e-04 - mean_absolute_error: 0.0193 - val_loss: 7.6109e-05 - val_mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00003\n",
      "Epoch 38/800\n",
      "90/90 [==============================] - 43s 473ms/step - loss: 3.9575e-04 - mean_absolute_error: 0.0183 - val_loss: 2.4611e-04 - val_mean_absolute_error: 0.0170\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00003\n",
      "Epoch 39/800\n",
      "90/90 [==============================] - 39s 435ms/step - loss: 3.8402e-04 - mean_absolute_error: 0.0180 - val_loss: 3.8708e-05 - val_mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00003\n",
      "Epoch 40/800\n",
      "90/90 [==============================] - 44s 492ms/step - loss: 3.9887e-04 - mean_absolute_error: 0.0182 - val_loss: 2.5721e-04 - val_mean_absolute_error: 0.0182\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00003\n",
      "Epoch 41/800\n",
      "90/90 [==============================] - 40s 446ms/step - loss: 4.2210e-04 - mean_absolute_error: 0.0192 - val_loss: 1.1110e-04 - val_mean_absolute_error: 0.0099\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00003\n",
      "Epoch 42/800\n",
      "90/90 [==============================] - 40s 446ms/step - loss: 3.6329e-04 - mean_absolute_error: 0.0179 - val_loss: 1.3512e-04 - val_mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00003\n",
      "Epoch 43/800\n",
      "90/90 [==============================] - 41s 450ms/step - loss: 4.3533e-04 - mean_absolute_error: 0.0195 - val_loss: 1.0422e-04 - val_mean_absolute_error: 0.0114\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00003\n",
      "Epoch 44/800\n",
      "90/90 [==============================] - 38s 427ms/step - loss: 3.9702e-04 - mean_absolute_error: 0.0187 - val_loss: 1.9756e-04 - val_mean_absolute_error: 0.0138\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00003\n",
      "Epoch 45/800\n",
      "90/90 [==============================] - 40s 444ms/step - loss: 4.1671e-04 - mean_absolute_error: 0.0191 - val_loss: 4.6929e-05 - val_mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00003\n",
      "Epoch 46/800\n",
      "90/90 [==============================] - 41s 456ms/step - loss: 4.3545e-04 - mean_absolute_error: 0.0188 - val_loss: 4.4957e-05 - val_mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00003\n",
      "Epoch 47/800\n",
      "90/90 [==============================] - 40s 449ms/step - loss: 3.8275e-04 - mean_absolute_error: 0.0181 - val_loss: 1.5023e-04 - val_mean_absolute_error: 0.0125\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00003\n",
      "Epoch 48/800\n",
      "90/90 [==============================] - 41s 451ms/step - loss: 4.1860e-04 - mean_absolute_error: 0.0190 - val_loss: 4.9696e-05 - val_mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00003\n",
      "Epoch 49/800\n",
      "90/90 [==============================] - 42s 467ms/step - loss: 3.9710e-04 - mean_absolute_error: 0.0188 - val_loss: 2.6017e-05 - val_mean_absolute_error: 0.0046\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00003 to 0.00003, saving model to results\\2021-09-07_SPY-shuffle-1-scale-1-split_date-0-epochs-800.h5\n",
      "Epoch 50/800\n",
      "90/90 [==============================] - 40s 442ms/step - loss: 3.7690e-04 - mean_absolute_error: 0.0182 - val_loss: 8.1641e-05 - val_mean_absolute_error: 0.0100\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00003\n",
      "Epoch 51/800\n",
      "90/90 [==============================] - 40s 447ms/step - loss: 4.0527e-04 - mean_absolute_error: 0.0187 - val_loss: 5.0772e-05 - val_mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00003\n",
      "Epoch 52/800\n",
      "90/90 [==============================] - 41s 458ms/step - loss: 4.5463e-04 - mean_absolute_error: 0.0195 - val_loss: 5.9266e-05 - val_mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00003\n",
      "Epoch 53/800\n",
      "90/90 [==============================] - 41s 455ms/step - loss: 4.0711e-04 - mean_absolute_error: 0.0192 - val_loss: 9.8076e-05 - val_mean_absolute_error: 0.0113\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00003\n",
      "Epoch 54/800\n",
      "90/90 [==============================] - 42s 463ms/step - loss: 3.9036e-04 - mean_absolute_error: 0.0185 - val_loss: 6.3980e-05 - val_mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00003\n",
      "Epoch 55/800\n",
      "90/90 [==============================] - 40s 450ms/step - loss: 4.0844e-04 - mean_absolute_error: 0.0188 - val_loss: 4.1220e-05 - val_mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00003\n",
      "Epoch 56/800\n",
      "90/90 [==============================] - 40s 450ms/step - loss: 3.9362e-04 - mean_absolute_error: 0.0188 - val_loss: 8.5744e-05 - val_mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00003\n",
      "Epoch 57/800\n",
      "90/90 [==============================] - 39s 428ms/step - loss: 3.6773e-04 - mean_absolute_error: 0.0182 - val_loss: 2.6859e-05 - val_mean_absolute_error: 0.0051\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00003\n",
      "Epoch 58/800\n",
      "90/90 [==============================] - 44s 486ms/step - loss: 3.7992e-04 - mean_absolute_error: 0.0186 - val_loss: 6.0752e-05 - val_mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00003\n",
      "Epoch 59/800\n",
      "90/90 [==============================] - 41s 452ms/step - loss: 3.8215e-04 - mean_absolute_error: 0.0181 - val_loss: 9.7451e-05 - val_mean_absolute_error: 0.0110\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00003\n",
      "Epoch 60/800\n",
      "90/90 [==============================] - 40s 440ms/step - loss: 4.6461e-04 - mean_absolute_error: 0.0198 - val_loss: 2.2134e-05 - val_mean_absolute_error: 0.0040\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00003 to 0.00002, saving model to results\\2021-09-07_SPY-shuffle-1-scale-1-split_date-0-epochs-800.h5\n",
      "Epoch 61/800\n",
      "90/90 [==============================] - 40s 440ms/step - loss: 3.5894e-04 - mean_absolute_error: 0.0173 - val_loss: 1.1279e-04 - val_mean_absolute_error: 0.0117\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00002\n",
      "Epoch 62/800\n",
      "90/90 [==============================] - 40s 443ms/step - loss: 3.6272e-04 - mean_absolute_error: 0.0178 - val_loss: 8.0883e-05 - val_mean_absolute_error: 0.0098\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00002\n",
      "Epoch 63/800\n",
      "90/90 [==============================] - 43s 474ms/step - loss: 3.9434e-04 - mean_absolute_error: 0.0186 - val_loss: 1.3134e-04 - val_mean_absolute_error: 0.0134\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00002\n",
      "Epoch 64/800\n",
      "90/90 [==============================] - 42s 469ms/step - loss: 4.0374e-04 - mean_absolute_error: 0.0188 - val_loss: 4.1597e-05 - val_mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00002\n",
      "Epoch 65/800\n",
      "90/90 [==============================] - 42s 465ms/step - loss: 4.0401e-04 - mean_absolute_error: 0.0184 - val_loss: 3.3273e-05 - val_mean_absolute_error: 0.0059\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00002\n",
      "Epoch 66/800\n",
      "90/90 [==============================] - 39s 432ms/step - loss: 4.1179e-04 - mean_absolute_error: 0.0187 - val_loss: 1.0977e-04 - val_mean_absolute_error: 0.0115\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00002\n",
      "Epoch 67/800\n",
      "90/90 [==============================] - 40s 440ms/step - loss: 3.7851e-04 - mean_absolute_error: 0.0183 - val_loss: 7.5521e-05 - val_mean_absolute_error: 0.0104\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00002\n",
      "Epoch 68/800\n",
      "90/90 [==============================] - 40s 443ms/step - loss: 3.8452e-04 - mean_absolute_error: 0.0183 - val_loss: 1.3289e-04 - val_mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00002\n",
      "Epoch 69/800\n",
      "90/90 [==============================] - 39s 438ms/step - loss: 3.8926e-04 - mean_absolute_error: 0.0187 - val_loss: 6.9916e-05 - val_mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00002\n",
      "Epoch 70/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 41s 459ms/step - loss: 3.7414e-04 - mean_absolute_error: 0.0179 - val_loss: 2.9879e-05 - val_mean_absolute_error: 0.0059\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00002\n",
      "Epoch 71/800\n",
      "90/90 [==============================] - 40s 449ms/step - loss: 3.9400e-04 - mean_absolute_error: 0.0191 - val_loss: 6.0844e-05 - val_mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00002\n",
      "Epoch 72/800\n",
      "90/90 [==============================] - 40s 445ms/step - loss: 3.8059e-04 - mean_absolute_error: 0.0182 - val_loss: 5.5065e-05 - val_mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00002\n",
      "Epoch 73/800\n",
      "90/90 [==============================] - 43s 478ms/step - loss: 3.7601e-04 - mean_absolute_error: 0.0182 - val_loss: 9.3600e-05 - val_mean_absolute_error: 0.0099\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00002\n",
      "Epoch 74/800\n",
      "90/90 [==============================] - 42s 471ms/step - loss: 4.1741e-04 - mean_absolute_error: 0.0189 - val_loss: 3.7539e-05 - val_mean_absolute_error: 0.0061\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00002\n",
      "Epoch 75/800\n",
      "90/90 [==============================] - 42s 464ms/step - loss: 3.8001e-04 - mean_absolute_error: 0.0182 - val_loss: 2.4466e-05 - val_mean_absolute_error: 0.0045\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00002\n",
      "Epoch 76/800\n",
      "90/90 [==============================] - 41s 456ms/step - loss: 3.8677e-04 - mean_absolute_error: 0.0184 - val_loss: 3.8960e-05 - val_mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00002\n",
      "Epoch 77/800\n",
      "90/90 [==============================] - 40s 450ms/step - loss: 3.8555e-04 - mean_absolute_error: 0.0185 - val_loss: 9.7708e-05 - val_mean_absolute_error: 0.0102\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00002\n",
      "Epoch 78/800\n",
      "90/90 [==============================] - 40s 448ms/step - loss: 4.0318e-04 - mean_absolute_error: 0.0189 - val_loss: 6.5712e-05 - val_mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00002\n",
      "Epoch 79/800\n",
      "90/90 [==============================] - 42s 472ms/step - loss: 3.9358e-04 - mean_absolute_error: 0.0185 - val_loss: 5.4413e-05 - val_mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00002\n",
      "Epoch 80/800\n",
      "90/90 [==============================] - 41s 457ms/step - loss: 3.7456e-04 - mean_absolute_error: 0.0181 - val_loss: 4.4773e-05 - val_mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00002\n",
      "Epoch 81/800\n",
      "90/90 [==============================] - 40s 442ms/step - loss: 3.5583e-04 - mean_absolute_error: 0.0176 - val_loss: 7.2952e-05 - val_mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00002\n",
      "Epoch 82/800\n",
      "90/90 [==============================] - 41s 456ms/step - loss: 3.9165e-04 - mean_absolute_error: 0.0187 - val_loss: 6.6636e-05 - val_mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00002\n",
      "Epoch 83/800\n",
      "90/90 [==============================] - 42s 470ms/step - loss: 3.9169e-04 - mean_absolute_error: 0.0186 - val_loss: 6.1370e-05 - val_mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00002\n",
      "Epoch 84/800\n",
      "90/90 [==============================] - 40s 447ms/step - loss: 3.5631e-04 - mean_absolute_error: 0.0180 - val_loss: 1.9388e-04 - val_mean_absolute_error: 0.0150\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00002\n",
      "Epoch 85/800\n",
      "90/90 [==============================] - 43s 479ms/step - loss: 3.7603e-04 - mean_absolute_error: 0.0181 - val_loss: 9.3292e-05 - val_mean_absolute_error: 0.0112\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00002\n",
      "Epoch 86/800\n",
      "90/90 [==============================] - 43s 483ms/step - loss: 4.1750e-04 - mean_absolute_error: 0.0194 - val_loss: 9.6099e-05 - val_mean_absolute_error: 0.0099\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00002\n",
      "Epoch 87/800\n",
      "90/90 [==============================] - 40s 448ms/step - loss: 3.6352e-04 - mean_absolute_error: 0.0178 - val_loss: 1.2002e-04 - val_mean_absolute_error: 0.0103\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00002\n",
      "Epoch 88/800\n",
      "90/90 [==============================] - 42s 464ms/step - loss: 3.5925e-04 - mean_absolute_error: 0.0177 - val_loss: 2.1956e-05 - val_mean_absolute_error: 0.0044\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.00002 to 0.00002, saving model to results\\2021-09-07_SPY-shuffle-1-scale-1-split_date-0-epochs-800.h5\n",
      "Epoch 89/800\n",
      "90/90 [==============================] - 41s 461ms/step - loss: 3.6895e-04 - mean_absolute_error: 0.0180 - val_loss: 1.3732e-04 - val_mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00002\n",
      "Epoch 90/800\n",
      "90/90 [==============================] - 40s 445ms/step - loss: 3.4399e-04 - mean_absolute_error: 0.0175 - val_loss: 2.4557e-05 - val_mean_absolute_error: 0.0050\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00002\n",
      "Epoch 91/800\n",
      "90/90 [==============================] - 40s 450ms/step - loss: 3.7184e-04 - mean_absolute_error: 0.0178 - val_loss: 2.1340e-05 - val_mean_absolute_error: 0.0043\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00002 to 0.00002, saving model to results\\2021-09-07_SPY-shuffle-1-scale-1-split_date-0-epochs-800.h5\n",
      "Epoch 92/800\n",
      "90/90 [==============================] - 40s 443ms/step - loss: 3.6999e-04 - mean_absolute_error: 0.0182 - val_loss: 6.8833e-05 - val_mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00002\n",
      "Epoch 93/800\n",
      "90/90 [==============================] - 40s 449ms/step - loss: 4.1156e-04 - mean_absolute_error: 0.0190 - val_loss: 1.1749e-04 - val_mean_absolute_error: 0.0115\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00002\n",
      "Epoch 94/800\n",
      "90/90 [==============================] - 42s 463ms/step - loss: 3.6235e-04 - mean_absolute_error: 0.0182 - val_loss: 2.4570e-05 - val_mean_absolute_error: 0.0050\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00002\n",
      "Epoch 95/800\n",
      "90/90 [==============================] - 41s 460ms/step - loss: 3.5638e-04 - mean_absolute_error: 0.0175 - val_loss: 9.6608e-05 - val_mean_absolute_error: 0.0115\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00002\n",
      "Epoch 96/800\n",
      "90/90 [==============================] - 41s 452ms/step - loss: 3.8969e-04 - mean_absolute_error: 0.0186 - val_loss: 8.5790e-05 - val_mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00002\n",
      "Epoch 97/800\n",
      "90/90 [==============================] - 42s 461ms/step - loss: 3.5348e-04 - mean_absolute_error: 0.0173 - val_loss: 3.3487e-05 - val_mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00002\n",
      "Epoch 98/800\n",
      "90/90 [==============================] - 41s 455ms/step - loss: 3.6113e-04 - mean_absolute_error: 0.0180 - val_loss: 3.4845e-05 - val_mean_absolute_error: 0.0060\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00002\n",
      "Epoch 99/800\n",
      "90/90 [==============================] - 40s 449ms/step - loss: 3.2786e-04 - mean_absolute_error: 0.0168 - val_loss: 1.4342e-04 - val_mean_absolute_error: 0.0107\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00002\n",
      "Epoch 100/800\n",
      "90/90 [==============================] - 43s 476ms/step - loss: 3.8713e-04 - mean_absolute_error: 0.0182 - val_loss: 1.3468e-04 - val_mean_absolute_error: 0.0129\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00002\n",
      "Epoch 101/800\n",
      "90/90 [==============================] - 41s 457ms/step - loss: 3.8838e-04 - mean_absolute_error: 0.0187 - val_loss: 9.0411e-05 - val_mean_absolute_error: 0.0103\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.00002\n",
      "Epoch 102/800\n",
      "90/90 [==============================] - 44s 485ms/step - loss: 4.2422e-04 - mean_absolute_error: 0.0192 - val_loss: 1.0739e-04 - val_mean_absolute_error: 0.0112\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.00002\n",
      "Epoch 103/800\n",
      "90/90 [==============================] - 40s 440ms/step - loss: 4.2652e-04 - mean_absolute_error: 0.0194 - val_loss: 8.9716e-05 - val_mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.00002\n",
      "Epoch 104/800\n",
      "90/90 [==============================] - 41s 456ms/step - loss: 4.0611e-04 - mean_absolute_error: 0.0185 - val_loss: 1.5010e-04 - val_mean_absolute_error: 0.0125\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.00002\n",
      "Epoch 105/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 45s 497ms/step - loss: 3.7433e-04 - mean_absolute_error: 0.0184 - val_loss: 2.8711e-05 - val_mean_absolute_error: 0.0056\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.00002\n",
      "Epoch 106/800\n",
      "90/90 [==============================] - 40s 441ms/step - loss: 3.9123e-04 - mean_absolute_error: 0.0183 - val_loss: 2.5176e-05 - val_mean_absolute_error: 0.0053\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.00002\n",
      "Epoch 107/800\n",
      "90/90 [==============================] - 39s 431ms/step - loss: 3.7815e-04 - mean_absolute_error: 0.0175 - val_loss: 1.2164e-04 - val_mean_absolute_error: 0.0145\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.00002\n",
      "Epoch 108/800\n",
      "90/90 [==============================] - 38s 426ms/step - loss: 3.6710e-04 - mean_absolute_error: 0.0179 - val_loss: 3.7897e-05 - val_mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.00002\n",
      "Epoch 109/800\n",
      "90/90 [==============================] - 39s 431ms/step - loss: 3.4681e-04 - mean_absolute_error: 0.0175 - val_loss: 6.0838e-05 - val_mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.00002\n",
      "Epoch 110/800\n",
      "90/90 [==============================] - 37s 413ms/step - loss: 3.5671e-04 - mean_absolute_error: 0.0179 - val_loss: 3.5129e-05 - val_mean_absolute_error: 0.0056\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.00002\n",
      "Epoch 111/800\n",
      "90/90 [==============================] - 37s 417ms/step - loss: 3.4968e-04 - mean_absolute_error: 0.0175 - val_loss: 2.9903e-05 - val_mean_absolute_error: 0.0055\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.00002\n",
      "Epoch 112/800\n",
      "90/90 [==============================] - 37s 411ms/step - loss: 3.4602e-04 - mean_absolute_error: 0.0175 - val_loss: 1.0245e-04 - val_mean_absolute_error: 0.0115\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.00002\n",
      "Epoch 113/800\n",
      "90/90 [==============================] - 39s 438ms/step - loss: 3.4935e-04 - mean_absolute_error: 0.0176 - val_loss: 3.4048e-05 - val_mean_absolute_error: 0.0057\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.00002\n",
      "Epoch 114/800\n",
      "90/90 [==============================] - 38s 418ms/step - loss: 3.7952e-04 - mean_absolute_error: 0.0178 - val_loss: 9.5496e-05 - val_mean_absolute_error: 0.0106\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.00002\n",
      "Epoch 115/800\n",
      "90/90 [==============================] - 36s 404ms/step - loss: 3.1585e-04 - mean_absolute_error: 0.0171 - val_loss: 4.9572e-05 - val_mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.00002\n",
      "Epoch 116/800\n",
      "90/90 [==============================] - 38s 427ms/step - loss: 3.5455e-04 - mean_absolute_error: 0.0178 - val_loss: 3.2435e-05 - val_mean_absolute_error: 0.0058\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.00002\n",
      "Epoch 117/800\n",
      "90/90 [==============================] - 38s 421ms/step - loss: 3.4207e-04 - mean_absolute_error: 0.0172 - val_loss: 5.4016e-05 - val_mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.00002\n",
      "Epoch 118/800\n",
      "90/90 [==============================] - 38s 420ms/step - loss: 3.1079e-04 - mean_absolute_error: 0.0167 - val_loss: 1.2746e-04 - val_mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.00002\n",
      "Epoch 119/800\n",
      "90/90 [==============================] - 37s 411ms/step - loss: 3.7491e-04 - mean_absolute_error: 0.0179 - val_loss: 1.1831e-04 - val_mean_absolute_error: 0.0105\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.00002\n",
      "Epoch 120/800\n",
      "90/90 [==============================] - 39s 426ms/step - loss: 3.7616e-04 - mean_absolute_error: 0.0178 - val_loss: 5.8317e-05 - val_mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.00002\n",
      "Epoch 121/800\n",
      "90/90 [==============================] - 39s 432ms/step - loss: 3.4178e-04 - mean_absolute_error: 0.0174 - val_loss: 3.8457e-05 - val_mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.00002\n",
      "Epoch 122/800\n",
      "90/90 [==============================] - 37s 414ms/step - loss: 2.9813e-04 - mean_absolute_error: 0.0164 - val_loss: 9.8270e-05 - val_mean_absolute_error: 0.0117\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.00002\n",
      "Epoch 123/800\n",
      "90/90 [==============================] - 39s 430ms/step - loss: 3.1272e-04 - mean_absolute_error: 0.0166 - val_loss: 1.5693e-04 - val_mean_absolute_error: 0.0118\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.00002\n",
      "Epoch 124/800\n",
      "90/90 [==============================] - 38s 428ms/step - loss: 3.6299e-04 - mean_absolute_error: 0.0177 - val_loss: 6.7366e-05 - val_mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.00002\n",
      "Epoch 125/800\n",
      "90/90 [==============================] - 36s 401ms/step - loss: 3.3885e-04 - mean_absolute_error: 0.0169 - val_loss: 3.6560e-05 - val_mean_absolute_error: 0.0060\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.00002\n",
      "Epoch 126/800\n",
      "90/90 [==============================] - 39s 429ms/step - loss: 3.6795e-04 - mean_absolute_error: 0.0179 - val_loss: 2.0039e-04 - val_mean_absolute_error: 0.0140\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.00002\n",
      "Epoch 127/800\n",
      "90/90 [==============================] - 37s 414ms/step - loss: 3.6370e-04 - mean_absolute_error: 0.0175 - val_loss: 1.0415e-04 - val_mean_absolute_error: 0.0100\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.00002\n",
      "Epoch 128/800\n",
      "90/90 [==============================] - 38s 419ms/step - loss: 3.2781e-04 - mean_absolute_error: 0.0170 - val_loss: 1.2525e-04 - val_mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.00002\n",
      "Epoch 129/800\n",
      "90/90 [==============================] - 39s 434ms/step - loss: 3.9396e-04 - mean_absolute_error: 0.0186 - val_loss: 3.4116e-05 - val_mean_absolute_error: 0.0061\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.00002\n",
      "Epoch 130/800\n",
      "90/90 [==============================] - 37s 412ms/step - loss: 3.5362e-04 - mean_absolute_error: 0.0174 - val_loss: 2.1058e-05 - val_mean_absolute_error: 0.0042\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.00002 to 0.00002, saving model to results\\2021-09-07_SPY-shuffle-1-scale-1-split_date-0-epochs-800.h5\n",
      "Epoch 131/800\n",
      "90/90 [==============================] - 36s 401ms/step - loss: 3.2450e-04 - mean_absolute_error: 0.0170 - val_loss: 5.1868e-05 - val_mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.00002\n",
      "Epoch 132/800\n",
      "90/90 [==============================] - 38s 419ms/step - loss: 3.6989e-04 - mean_absolute_error: 0.0178 - val_loss: 2.7098e-05 - val_mean_absolute_error: 0.0048\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.00002\n",
      "Epoch 133/800\n",
      "90/90 [==============================] - 38s 419ms/step - loss: 3.1855e-04 - mean_absolute_error: 0.0170 - val_loss: 3.9799e-05 - val_mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.00002\n",
      "Epoch 134/800\n",
      "90/90 [==============================] - 39s 437ms/step - loss: 3.7381e-04 - mean_absolute_error: 0.0182 - val_loss: 1.0738e-04 - val_mean_absolute_error: 0.0112\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.00002\n",
      "Epoch 135/800\n",
      "90/90 [==============================] - 37s 411ms/step - loss: 3.5870e-04 - mean_absolute_error: 0.0180 - val_loss: 4.9624e-05 - val_mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.00002\n",
      "Epoch 136/800\n",
      "90/90 [==============================] - 38s 423ms/step - loss: 3.3838e-04 - mean_absolute_error: 0.0172 - val_loss: 3.8665e-05 - val_mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.00002\n",
      "Epoch 137/800\n",
      "90/90 [==============================] - 38s 427ms/step - loss: 3.2992e-04 - mean_absolute_error: 0.0169 - val_loss: 2.6204e-05 - val_mean_absolute_error: 0.0046\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.00002\n",
      "Epoch 138/800\n",
      "90/90 [==============================] - 37s 411ms/step - loss: 3.3812e-04 - mean_absolute_error: 0.0169 - val_loss: 1.7693e-04 - val_mean_absolute_error: 0.0144\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.00002\n",
      "Epoch 139/800\n",
      "90/90 [==============================] - 37s 411ms/step - loss: 3.3488e-04 - mean_absolute_error: 0.0169 - val_loss: 3.1577e-05 - val_mean_absolute_error: 0.0053\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.00002\n",
      "Epoch 140/800\n",
      "90/90 [==============================] - 38s 420ms/step - loss: 3.2985e-04 - mean_absolute_error: 0.0172 - val_loss: 4.6758e-05 - val_mean_absolute_error: 0.0063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00140: val_loss did not improve from 0.00002\n",
      "Epoch 141/800\n",
      "90/90 [==============================] - 37s 415ms/step - loss: 3.6123e-04 - mean_absolute_error: 0.0179 - val_loss: 4.6168e-05 - val_mean_absolute_error: 0.0059\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.00002\n",
      "Epoch 142/800\n",
      "90/90 [==============================] - 38s 419ms/step - loss: 3.1700e-04 - mean_absolute_error: 0.0166 - val_loss: 2.8102e-05 - val_mean_absolute_error: 0.0055\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.00002\n",
      "Epoch 143/800\n",
      "90/90 [==============================] - 38s 419ms/step - loss: 3.5613e-04 - mean_absolute_error: 0.0179 - val_loss: 3.0184e-05 - val_mean_absolute_error: 0.0059\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.00002\n",
      "Epoch 144/800\n",
      "90/90 [==============================] - 39s 429ms/step - loss: 3.4742e-04 - mean_absolute_error: 0.0175 - val_loss: 3.3712e-05 - val_mean_absolute_error: 0.0058\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.00002\n",
      "Epoch 145/800\n",
      "90/90 [==============================] - 38s 428ms/step - loss: 3.3256e-04 - mean_absolute_error: 0.0169 - val_loss: 2.8155e-05 - val_mean_absolute_error: 0.0051\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.00002\n",
      "Epoch 146/800\n",
      "90/90 [==============================] - 37s 407ms/step - loss: 3.3667e-04 - mean_absolute_error: 0.0170 - val_loss: 1.8935e-04 - val_mean_absolute_error: 0.0153\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.00002\n",
      "Epoch 147/800\n",
      "90/90 [==============================] - 39s 435ms/step - loss: 3.1909e-04 - mean_absolute_error: 0.0164 - val_loss: 3.5673e-05 - val_mean_absolute_error: 0.0061\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.00002\n",
      "Epoch 148/800\n",
      "90/90 [==============================] - 38s 418ms/step - loss: 3.2498e-04 - mean_absolute_error: 0.0166 - val_loss: 5.9823e-05 - val_mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.00002\n",
      "Epoch 149/800\n",
      "90/90 [==============================] - 39s 430ms/step - loss: 3.4851e-04 - mean_absolute_error: 0.0177 - val_loss: 4.6471e-05 - val_mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.00002\n",
      "Epoch 150/800\n",
      "90/90 [==============================] - 39s 439ms/step - loss: 3.1179e-04 - mean_absolute_error: 0.0168 - val_loss: 3.3819e-05 - val_mean_absolute_error: 0.0050\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.00002\n",
      "Epoch 151/800\n",
      "90/90 [==============================] - 37s 414ms/step - loss: 3.1312e-04 - mean_absolute_error: 0.0167 - val_loss: 1.1853e-04 - val_mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.00002\n",
      "Epoch 152/800\n",
      "90/90 [==============================] - 36s 403ms/step - loss: 3.4836e-04 - mean_absolute_error: 0.0172 - val_loss: 6.2990e-05 - val_mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.00002\n",
      "Epoch 153/800\n",
      "90/90 [==============================] - 39s 434ms/step - loss: 3.3448e-04 - mean_absolute_error: 0.0173 - val_loss: 1.9170e-05 - val_mean_absolute_error: 0.0039\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.00002 to 0.00002, saving model to results\\2021-09-07_SPY-shuffle-1-scale-1-split_date-0-epochs-800.h5\n",
      "Epoch 154/800\n",
      "90/90 [==============================] - 38s 425ms/step - loss: 3.4938e-04 - mean_absolute_error: 0.0172 - val_loss: 2.6393e-05 - val_mean_absolute_error: 0.0052\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.00002\n",
      "Epoch 155/800\n",
      "90/90 [==============================] - 38s 425ms/step - loss: 3.5134e-04 - mean_absolute_error: 0.0169 - val_loss: 5.4296e-05 - val_mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.00002\n",
      "Epoch 156/800\n",
      "90/90 [==============================] - 39s 429ms/step - loss: 3.3141e-04 - mean_absolute_error: 0.0170 - val_loss: 4.6456e-05 - val_mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.00002\n",
      "Epoch 157/800\n",
      "90/90 [==============================] - 38s 420ms/step - loss: 3.4959e-04 - mean_absolute_error: 0.0171 - val_loss: 4.7562e-05 - val_mean_absolute_error: 0.0082\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.00002\n",
      "Epoch 158/800\n",
      "90/90 [==============================] - 38s 420ms/step - loss: 3.4559e-04 - mean_absolute_error: 0.0173 - val_loss: 5.2785e-05 - val_mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.00002\n",
      "Epoch 159/800\n",
      "90/90 [==============================] - 38s 418ms/step - loss: 3.6069e-04 - mean_absolute_error: 0.0177 - val_loss: 4.9044e-05 - val_mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.00002\n",
      "Epoch 160/800\n",
      "90/90 [==============================] - 37s 416ms/step - loss: 3.1753e-04 - mean_absolute_error: 0.0168 - val_loss: 2.4541e-05 - val_mean_absolute_error: 0.0044\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.00002\n",
      "Epoch 161/800\n",
      "90/90 [==============================] - 39s 435ms/step - loss: 3.3372e-04 - mean_absolute_error: 0.0173 - val_loss: 5.5500e-05 - val_mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.00002\n",
      "Epoch 162/800\n",
      "90/90 [==============================] - 37s 415ms/step - loss: 3.0675e-04 - mean_absolute_error: 0.0166 - val_loss: 4.0260e-05 - val_mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.00002\n",
      "Epoch 163/800\n",
      "90/90 [==============================] - 39s 429ms/step - loss: 3.3882e-04 - mean_absolute_error: 0.0169 - val_loss: 3.4792e-05 - val_mean_absolute_error: 0.0055\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.00002\n",
      "Epoch 164/800\n",
      "90/90 [==============================] - 38s 427ms/step - loss: 3.3472e-04 - mean_absolute_error: 0.0169 - val_loss: 4.2066e-05 - val_mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.00002\n",
      "Epoch 165/800\n",
      "90/90 [==============================] - 38s 425ms/step - loss: 3.1502e-04 - mean_absolute_error: 0.0166 - val_loss: 3.0407e-05 - val_mean_absolute_error: 0.0060\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.00002\n",
      "Epoch 166/800\n",
      "90/90 [==============================] - 39s 437ms/step - loss: 3.0357e-04 - mean_absolute_error: 0.0164 - val_loss: 2.7882e-05 - val_mean_absolute_error: 0.0047\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.00002\n",
      "Epoch 167/800\n",
      "90/90 [==============================] - 38s 419ms/step - loss: 3.3368e-04 - mean_absolute_error: 0.0169 - val_loss: 5.6184e-05 - val_mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.00002\n",
      "Epoch 168/800\n",
      "90/90 [==============================] - 37s 407ms/step - loss: 3.2125e-04 - mean_absolute_error: 0.0166 - val_loss: 2.1207e-05 - val_mean_absolute_error: 0.0043\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.00002\n",
      "Epoch 169/800\n",
      "90/90 [==============================] - 39s 435ms/step - loss: 3.3772e-04 - mean_absolute_error: 0.0172 - val_loss: 3.6832e-05 - val_mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.00002\n",
      "Epoch 170/800\n",
      "90/90 [==============================] - 39s 437ms/step - loss: 3.2567e-04 - mean_absolute_error: 0.0168 - val_loss: 5.0325e-05 - val_mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.00002\n",
      "Epoch 171/800\n",
      "90/90 [==============================] - 39s 431ms/step - loss: 3.2810e-04 - mean_absolute_error: 0.0165 - val_loss: 3.4594e-05 - val_mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.00002\n",
      "Epoch 172/800\n",
      "90/90 [==============================] - 39s 429ms/step - loss: 3.2234e-04 - mean_absolute_error: 0.0166 - val_loss: 2.8795e-05 - val_mean_absolute_error: 0.0048\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.00002\n",
      "Epoch 173/800\n",
      "90/90 [==============================] - 38s 424ms/step - loss: 3.5750e-04 - mean_absolute_error: 0.0176 - val_loss: 1.2956e-04 - val_mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.00002\n",
      "Epoch 174/800\n",
      "90/90 [==============================] - 39s 429ms/step - loss: 3.6083e-04 - mean_absolute_error: 0.0175 - val_loss: 4.9142e-05 - val_mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.00002\n",
      "Epoch 175/800\n",
      "90/90 [==============================] - 38s 422ms/step - loss: 3.4276e-04 - mean_absolute_error: 0.0174 - val_loss: 2.2634e-05 - val_mean_absolute_error: 0.0041\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.00002\n",
      "Epoch 176/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 38s 417ms/step - loss: 2.9206e-04 - mean_absolute_error: 0.0162 - val_loss: 2.2843e-05 - val_mean_absolute_error: 0.0044\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.00002\n",
      "Epoch 177/800\n",
      "90/90 [==============================] - 39s 434ms/step - loss: 3.0668e-04 - mean_absolute_error: 0.0160 - val_loss: 4.5752e-05 - val_mean_absolute_error: 0.0060\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.00002\n",
      "Epoch 178/800\n",
      "90/90 [==============================] - 38s 418ms/step - loss: 2.9139e-04 - mean_absolute_error: 0.0158 - val_loss: 5.1546e-05 - val_mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.00002\n",
      "Epoch 179/800\n",
      "90/90 [==============================] - 39s 437ms/step - loss: 2.9484e-04 - mean_absolute_error: 0.0162 - val_loss: 4.9439e-05 - val_mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.00002\n",
      "Epoch 180/800\n",
      "90/90 [==============================] - 39s 433ms/step - loss: 3.0835e-04 - mean_absolute_error: 0.0165 - val_loss: 1.0619e-04 - val_mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.00002\n",
      "Epoch 181/800\n",
      "90/90 [==============================] - 37s 414ms/step - loss: 3.3576e-04 - mean_absolute_error: 0.0173 - val_loss: 3.6959e-05 - val_mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.00002\n",
      "Epoch 182/800\n",
      "90/90 [==============================] - 38s 423ms/step - loss: 3.0317e-04 - mean_absolute_error: 0.0164 - val_loss: 1.0968e-04 - val_mean_absolute_error: 0.0111\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.00002\n",
      "Epoch 183/800\n",
      "90/90 [==============================] - 38s 423ms/step - loss: 3.2565e-04 - mean_absolute_error: 0.0166 - val_loss: 1.6242e-04 - val_mean_absolute_error: 0.0127\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.00002\n",
      "Epoch 184/800\n",
      "90/90 [==============================] - 39s 430ms/step - loss: 3.2419e-04 - mean_absolute_error: 0.0168 - val_loss: 4.5745e-05 - val_mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.00002\n",
      "Epoch 185/800\n",
      "90/90 [==============================] - 40s 439ms/step - loss: 3.1674e-04 - mean_absolute_error: 0.0166 - val_loss: 7.8651e-05 - val_mean_absolute_error: 0.0097\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.00002\n",
      "Epoch 186/800\n",
      "90/90 [==============================] - 38s 419ms/step - loss: 3.4588e-04 - mean_absolute_error: 0.0173 - val_loss: 1.2424e-04 - val_mean_absolute_error: 0.0112\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.00002\n",
      "Epoch 187/800\n",
      "90/90 [==============================] - 39s 437ms/step - loss: 3.4257e-04 - mean_absolute_error: 0.0171 - val_loss: 1.0911e-04 - val_mean_absolute_error: 0.0105\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.00002\n",
      "Epoch 188/800\n",
      "90/90 [==============================] - 39s 436ms/step - loss: 3.3065e-04 - mean_absolute_error: 0.0170 - val_loss: 8.8327e-05 - val_mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.00002\n",
      "Epoch 189/800\n",
      "90/90 [==============================] - 37s 416ms/step - loss: 3.1241e-04 - mean_absolute_error: 0.0165 - val_loss: 5.3271e-05 - val_mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.00002\n",
      "Epoch 190/800\n",
      "90/90 [==============================] - 38s 427ms/step - loss: 3.0441e-04 - mean_absolute_error: 0.0163 - val_loss: 1.9566e-05 - val_mean_absolute_error: 0.0041\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.00002\n",
      "Epoch 191/800\n",
      "90/90 [==============================] - 38s 420ms/step - loss: 3.2469e-04 - mean_absolute_error: 0.0169 - val_loss: 6.2339e-05 - val_mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.00002\n",
      "Epoch 192/800\n",
      "90/90 [==============================] - 38s 424ms/step - loss: 3.3601e-04 - mean_absolute_error: 0.0173 - val_loss: 2.7581e-05 - val_mean_absolute_error: 0.0056\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.00002\n",
      "Epoch 193/800\n",
      "90/90 [==============================] - 37s 411ms/step - loss: 3.4037e-04 - mean_absolute_error: 0.0171 - val_loss: 1.8008e-04 - val_mean_absolute_error: 0.0134\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.00002\n",
      "Epoch 194/800\n",
      "90/90 [==============================] - 38s 423ms/step - loss: 2.9625e-04 - mean_absolute_error: 0.0160 - val_loss: 3.1186e-05 - val_mean_absolute_error: 0.0059\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.00002\n",
      "Epoch 195/800\n",
      "90/90 [==============================] - 37s 411ms/step - loss: 3.1053e-04 - mean_absolute_error: 0.0164 - val_loss: 3.3073e-05 - val_mean_absolute_error: 0.0060\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.00002\n",
      "Epoch 196/800\n",
      "90/90 [==============================] - 39s 428ms/step - loss: 3.2524e-04 - mean_absolute_error: 0.0166 - val_loss: 5.1752e-05 - val_mean_absolute_error: 0.0058\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.00002\n",
      "Epoch 197/800\n",
      "90/90 [==============================] - 38s 417ms/step - loss: 3.1068e-04 - mean_absolute_error: 0.0163 - val_loss: 2.2018e-05 - val_mean_absolute_error: 0.0044\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.00002\n",
      "Epoch 198/800\n",
      "90/90 [==============================] - 36s 396ms/step - loss: 2.9859e-04 - mean_absolute_error: 0.0163 - val_loss: 6.1561e-05 - val_mean_absolute_error: 0.0060\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.00002\n",
      "Epoch 199/800\n",
      "90/90 [==============================] - 38s 426ms/step - loss: 3.6642e-04 - mean_absolute_error: 0.0174 - val_loss: 4.8796e-05 - val_mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.00002\n",
      "Epoch 200/800\n",
      "90/90 [==============================] - 37s 414ms/step - loss: 3.2099e-04 - mean_absolute_error: 0.0167 - val_loss: 9.5653e-05 - val_mean_absolute_error: 0.0111\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.00002\n",
      "Epoch 201/800\n",
      "90/90 [==============================] - 38s 423ms/step - loss: 3.5196e-04 - mean_absolute_error: 0.0173 - val_loss: 2.8225e-05 - val_mean_absolute_error: 0.0048\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.00002\n",
      "Epoch 202/800\n",
      "90/90 [==============================] - 37s 413ms/step - loss: 3.0095e-04 - mean_absolute_error: 0.0164 - val_loss: 2.3997e-05 - val_mean_absolute_error: 0.0051\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.00002\n",
      "Epoch 203/800\n",
      "90/90 [==============================] - 37s 417ms/step - loss: 3.1167e-04 - mean_absolute_error: 0.0165 - val_loss: 2.2984e-05 - val_mean_absolute_error: 0.0040\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.00002\n",
      "Epoch 204/800\n",
      "90/90 [==============================] - 37s 410ms/step - loss: 3.0598e-04 - mean_absolute_error: 0.0161 - val_loss: 3.5332e-05 - val_mean_absolute_error: 0.0061\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.00002\n",
      "Epoch 205/800\n",
      "90/90 [==============================] - 37s 411ms/step - loss: 2.9644e-04 - mean_absolute_error: 0.0161 - val_loss: 1.8567e-05 - val_mean_absolute_error: 0.0037\n",
      "\n",
      "Epoch 00205: val_loss improved from 0.00002 to 0.00002, saving model to results\\2021-09-07_SPY-shuffle-1-scale-1-split_date-0-epochs-800.h5\n",
      "Epoch 206/800\n",
      "90/90 [==============================] - 36s 404ms/step - loss: 3.0060e-04 - mean_absolute_error: 0.0163 - val_loss: 1.1680e-04 - val_mean_absolute_error: 0.0102\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.00002\n",
      "Epoch 207/800\n",
      "90/90 [==============================] - 37s 410ms/step - loss: 3.2852e-04 - mean_absolute_error: 0.0168 - val_loss: 6.4531e-05 - val_mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.00002\n",
      "Epoch 208/800\n",
      "90/90 [==============================] - 36s 404ms/step - loss: 3.2067e-04 - mean_absolute_error: 0.0168 - val_loss: 4.8793e-05 - val_mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.00002\n",
      "Epoch 209/800\n",
      "90/90 [==============================] - 38s 423ms/step - loss: 3.1732e-04 - mean_absolute_error: 0.0168 - val_loss: 3.7877e-05 - val_mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.00002\n",
      "Epoch 210/800\n",
      "90/90 [==============================] - 36s 400ms/step - loss: 3.2707e-04 - mean_absolute_error: 0.0167 - val_loss: 7.0538e-05 - val_mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.00002\n",
      "Epoch 211/800\n",
      "90/90 [==============================] - 36s 401ms/step - loss: 2.8550e-04 - mean_absolute_error: 0.0160 - val_loss: 9.3955e-05 - val_mean_absolute_error: 0.0085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00211: val_loss did not improve from 0.00002\n",
      "Epoch 212/800\n",
      "90/90 [==============================] - 37s 416ms/step - loss: 3.0156e-04 - mean_absolute_error: 0.0161 - val_loss: 2.9474e-05 - val_mean_absolute_error: 0.0051\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.00002\n",
      "Epoch 213/800\n",
      "90/90 [==============================] - 36s 404ms/step - loss: 3.2841e-04 - mean_absolute_error: 0.0168 - val_loss: 2.7569e-05 - val_mean_absolute_error: 0.0043\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.00002\n",
      "Epoch 214/800\n",
      "90/90 [==============================] - 39s 433ms/step - loss: 3.2298e-04 - mean_absolute_error: 0.0167 - val_loss: 3.8429e-05 - val_mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.00002\n",
      "Epoch 215/800\n",
      "90/90 [==============================] - 37s 408ms/step - loss: 3.0734e-04 - mean_absolute_error: 0.0163 - val_loss: 5.5001e-05 - val_mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.00002\n",
      "Epoch 216/800\n",
      "90/90 [==============================] - 37s 416ms/step - loss: 3.1153e-04 - mean_absolute_error: 0.0165 - val_loss: 2.9561e-05 - val_mean_absolute_error: 0.0055\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.00002\n",
      "Epoch 217/800\n",
      "90/90 [==============================] - 39s 436ms/step - loss: 3.1628e-04 - mean_absolute_error: 0.0164 - val_loss: 5.6794e-05 - val_mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.00002\n",
      "Epoch 218/800\n",
      "90/90 [==============================] - 38s 419ms/step - loss: 2.8151e-04 - mean_absolute_error: 0.0160 - val_loss: 3.2208e-05 - val_mean_absolute_error: 0.0051\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.00002\n",
      "Epoch 219/800\n",
      "90/90 [==============================] - 38s 421ms/step - loss: 2.9793e-04 - mean_absolute_error: 0.0159 - val_loss: 3.8299e-05 - val_mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.00002\n",
      "Epoch 220/800\n",
      "90/90 [==============================] - 37s 411ms/step - loss: 2.9378e-04 - mean_absolute_error: 0.0160 - val_loss: 4.1296e-05 - val_mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.00002\n",
      "Epoch 221/800\n",
      "90/90 [==============================] - 36s 397ms/step - loss: 2.9468e-04 - mean_absolute_error: 0.0160 - val_loss: 2.3905e-05 - val_mean_absolute_error: 0.0046\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.00002\n",
      "Epoch 222/800\n",
      "90/90 [==============================] - 39s 430ms/step - loss: 3.3663e-04 - mean_absolute_error: 0.0168 - val_loss: 5.5613e-05 - val_mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.00002\n",
      "Epoch 223/800\n",
      "90/90 [==============================] - 37s 408ms/step - loss: 3.1168e-04 - mean_absolute_error: 0.0161 - val_loss: 6.0712e-05 - val_mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.00002\n",
      "Epoch 224/800\n",
      "90/90 [==============================] - 36s 399ms/step - loss: 3.0462e-04 - mean_absolute_error: 0.0162 - val_loss: 8.6549e-05 - val_mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.00002\n",
      "Epoch 225/800\n",
      "90/90 [==============================] - 37s 414ms/step - loss: 3.1772e-04 - mean_absolute_error: 0.0163 - val_loss: 1.0257e-04 - val_mean_absolute_error: 0.0116\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.00002\n",
      "Epoch 226/800\n",
      "90/90 [==============================] - 37s 407ms/step - loss: 3.1786e-04 - mean_absolute_error: 0.0165 - val_loss: 2.1358e-05 - val_mean_absolute_error: 0.0042\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.00002\n",
      "Epoch 227/800\n",
      "90/90 [==============================] - 38s 428ms/step - loss: 2.9869e-04 - mean_absolute_error: 0.0159 - val_loss: 3.7255e-05 - val_mean_absolute_error: 0.0055\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.00002\n",
      "Epoch 228/800\n",
      "90/90 [==============================] - 37s 409ms/step - loss: 3.0326e-04 - mean_absolute_error: 0.0162 - val_loss: 3.5787e-05 - val_mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.00002\n",
      "Epoch 229/800\n",
      "90/90 [==============================] - 38s 427ms/step - loss: 2.9558e-04 - mean_absolute_error: 0.0159 - val_loss: 8.1392e-05 - val_mean_absolute_error: 0.0088\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.00002\n",
      "Epoch 230/800\n",
      "90/90 [==============================] - 38s 428ms/step - loss: 3.1727e-04 - mean_absolute_error: 0.0165 - val_loss: 8.0931e-05 - val_mean_absolute_error: 0.0100\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.00002\n",
      "Epoch 231/800\n",
      "90/90 [==============================] - 37s 414ms/step - loss: 3.0659e-04 - mean_absolute_error: 0.0161 - val_loss: 2.6622e-05 - val_mean_absolute_error: 0.0047\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.00002\n",
      "Epoch 232/800\n",
      "90/90 [==============================] - 38s 417ms/step - loss: 2.9693e-04 - mean_absolute_error: 0.0160 - val_loss: 2.6950e-05 - val_mean_absolute_error: 0.0053\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.00002\n",
      "Epoch 233/800\n",
      "90/90 [==============================] - 37s 413ms/step - loss: 2.9784e-04 - mean_absolute_error: 0.0162 - val_loss: 1.8466e-05 - val_mean_absolute_error: 0.0042\n",
      "\n",
      "Epoch 00233: val_loss improved from 0.00002 to 0.00002, saving model to results\\2021-09-07_SPY-shuffle-1-scale-1-split_date-0-epochs-800.h5\n",
      "Epoch 234/800\n",
      "90/90 [==============================] - 37s 406ms/step - loss: 3.1320e-04 - mean_absolute_error: 0.0167 - val_loss: 7.0771e-05 - val_mean_absolute_error: 0.0100\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.00002\n",
      "Epoch 235/800\n",
      "90/90 [==============================] - 38s 427ms/step - loss: 3.0942e-04 - mean_absolute_error: 0.0163 - val_loss: 7.1428e-05 - val_mean_absolute_error: 0.0093\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.00002\n",
      "Epoch 236/800\n",
      "90/90 [==============================] - 36s 405ms/step - loss: 3.0844e-04 - mean_absolute_error: 0.0162 - val_loss: 1.8174e-05 - val_mean_absolute_error: 0.0042\n",
      "\n",
      "Epoch 00236: val_loss improved from 0.00002 to 0.00002, saving model to results\\2021-09-07_SPY-shuffle-1-scale-1-split_date-0-epochs-800.h5\n",
      "Epoch 237/800\n",
      "90/90 [==============================] - 38s 425ms/step - loss: 2.9393e-04 - mean_absolute_error: 0.0159 - val_loss: 4.9752e-05 - val_mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.00002\n",
      "Epoch 238/800\n",
      "90/90 [==============================] - 38s 418ms/step - loss: 2.8887e-04 - mean_absolute_error: 0.0158 - val_loss: 4.0018e-05 - val_mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.00002\n",
      "Epoch 239/800\n",
      "90/90 [==============================] - 37s 409ms/step - loss: 2.7834e-04 - mean_absolute_error: 0.0157 - val_loss: 2.1879e-05 - val_mean_absolute_error: 0.0041\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.00002\n",
      "Epoch 240/800\n",
      "90/90 [==============================] - 38s 426ms/step - loss: 3.1437e-04 - mean_absolute_error: 0.0164 - val_loss: 2.0801e-05 - val_mean_absolute_error: 0.0042\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.00002\n",
      "Epoch 241/800\n",
      "90/90 [==============================] - 36s 406ms/step - loss: 2.9504e-04 - mean_absolute_error: 0.0163 - val_loss: 2.3825e-05 - val_mean_absolute_error: 0.0049\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.00002\n",
      "Epoch 242/800\n",
      "90/90 [==============================] - 37s 412ms/step - loss: 2.8344e-04 - mean_absolute_error: 0.0156 - val_loss: 3.9679e-05 - val_mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.00002\n",
      "Epoch 243/800\n",
      "90/90 [==============================] - 38s 422ms/step - loss: 3.2086e-04 - mean_absolute_error: 0.0164 - val_loss: 7.6215e-05 - val_mean_absolute_error: 0.0104\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.00002\n",
      "Epoch 244/800\n",
      "90/90 [==============================] - 37s 407ms/step - loss: 3.2259e-04 - mean_absolute_error: 0.0169 - val_loss: 1.9799e-05 - val_mean_absolute_error: 0.0042\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.00002\n",
      "Epoch 245/800\n",
      "90/90 [==============================] - 37s 413ms/step - loss: 2.9789e-04 - mean_absolute_error: 0.0164 - val_loss: 1.3199e-04 - val_mean_absolute_error: 0.0113\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.00002\n",
      "Epoch 246/800\n",
      "90/90 [==============================] - 37s 415ms/step - loss: 2.8421e-04 - mean_absolute_error: 0.0158 - val_loss: 7.6163e-05 - val_mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.00002\n",
      "Epoch 247/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 37s 417ms/step - loss: 2.9985e-04 - mean_absolute_error: 0.0159 - val_loss: 4.5105e-05 - val_mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.00002\n",
      "Epoch 248/800\n",
      "90/90 [==============================] - 38s 426ms/step - loss: 3.2944e-04 - mean_absolute_error: 0.0168 - val_loss: 3.7723e-05 - val_mean_absolute_error: 0.0058\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 0.00002\n",
      "Epoch 249/800\n",
      "90/90 [==============================] - 36s 403ms/step - loss: 3.2320e-04 - mean_absolute_error: 0.0165 - val_loss: 2.2605e-04 - val_mean_absolute_error: 0.0141\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.00002\n",
      "Epoch 250/800\n",
      "90/90 [==============================] - 38s 421ms/step - loss: 3.1048e-04 - mean_absolute_error: 0.0164 - val_loss: 2.3762e-05 - val_mean_absolute_error: 0.0052\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.00002\n",
      "Epoch 251/800\n",
      "90/90 [==============================] - 37s 409ms/step - loss: 2.9486e-04 - mean_absolute_error: 0.0162 - val_loss: 2.0784e-05 - val_mean_absolute_error: 0.0045\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 0.00002\n",
      "Epoch 252/800\n",
      "90/90 [==============================] - 35s 395ms/step - loss: 2.9652e-04 - mean_absolute_error: 0.0158 - val_loss: 2.5410e-05 - val_mean_absolute_error: 0.0054\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 0.00002\n",
      "Epoch 253/800\n",
      "90/90 [==============================] - 38s 425ms/step - loss: 3.2008e-04 - mean_absolute_error: 0.0164 - val_loss: 3.0576e-05 - val_mean_absolute_error: 0.0058\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 0.00002\n",
      "Epoch 254/800\n",
      "90/90 [==============================] - 37s 409ms/step - loss: 2.8150e-04 - mean_absolute_error: 0.0155 - val_loss: 3.6058e-05 - val_mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 0.00002\n",
      "Epoch 255/800\n",
      "90/90 [==============================] - 35s 394ms/step - loss: 3.0015e-04 - mean_absolute_error: 0.0163 - val_loss: 1.0116e-04 - val_mean_absolute_error: 0.0110\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 0.00002\n",
      "Epoch 256/800\n",
      "90/90 [==============================] - 38s 420ms/step - loss: 3.1478e-04 - mean_absolute_error: 0.0164 - val_loss: 3.6438e-05 - val_mean_absolute_error: 0.0053\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 0.00002\n",
      "Epoch 257/800\n",
      "90/90 [==============================] - 36s 402ms/step - loss: 2.9140e-04 - mean_absolute_error: 0.0158 - val_loss: 2.5652e-05 - val_mean_absolute_error: 0.0048\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 0.00002\n",
      "Epoch 258/800\n",
      "90/90 [==============================] - 38s 424ms/step - loss: 2.9442e-04 - mean_absolute_error: 0.0158 - val_loss: 2.8537e-05 - val_mean_absolute_error: 0.0056\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 0.00002\n",
      "Epoch 259/800\n",
      "90/90 [==============================] - 37s 406ms/step - loss: 3.1290e-04 - mean_absolute_error: 0.0165 - val_loss: 7.7441e-05 - val_mean_absolute_error: 0.0104\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 0.00002\n",
      "Epoch 260/800\n",
      "90/90 [==============================] - 37s 415ms/step - loss: 2.8582e-04 - mean_absolute_error: 0.0157 - val_loss: 2.4629e-05 - val_mean_absolute_error: 0.0048\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 0.00002\n",
      "Epoch 261/800\n",
      "90/90 [==============================] - 38s 428ms/step - loss: 2.7966e-04 - mean_absolute_error: 0.0153 - val_loss: 3.3786e-05 - val_mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 0.00002\n",
      "Epoch 262/800\n",
      "90/90 [==============================] - 37s 410ms/step - loss: 2.9861e-04 - mean_absolute_error: 0.0160 - val_loss: 2.0578e-04 - val_mean_absolute_error: 0.0142\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 0.00002\n",
      "Epoch 263/800\n",
      "90/90 [==============================] - 38s 428ms/step - loss: 2.8860e-04 - mean_absolute_error: 0.0159 - val_loss: 3.7127e-05 - val_mean_absolute_error: 0.0059\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 0.00002\n",
      "Epoch 264/800\n",
      "90/90 [==============================] - 38s 428ms/step - loss: 3.1854e-04 - mean_absolute_error: 0.0165 - val_loss: 7.1228e-05 - val_mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 0.00002\n",
      "Epoch 265/800\n",
      "90/90 [==============================] - 36s 397ms/step - loss: 3.1596e-04 - mean_absolute_error: 0.0163 - val_loss: 1.0763e-04 - val_mean_absolute_error: 0.0106\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 0.00002\n",
      "Epoch 266/800\n",
      "90/90 [==============================] - 37s 407ms/step - loss: 3.0069e-04 - mean_absolute_error: 0.0161 - val_loss: 2.1163e-05 - val_mean_absolute_error: 0.0047\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 0.00002\n",
      "Epoch 267/800\n",
      "90/90 [==============================] - 36s 403ms/step - loss: 2.8572e-04 - mean_absolute_error: 0.0157 - val_loss: 3.4089e-05 - val_mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 0.00002\n",
      "Epoch 268/800\n",
      "90/90 [==============================] - 37s 412ms/step - loss: 3.0611e-04 - mean_absolute_error: 0.0162 - val_loss: 2.6392e-05 - val_mean_absolute_error: 0.0055\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 0.00002\n",
      "Epoch 269/800\n",
      "90/90 [==============================] - 37s 409ms/step - loss: 2.8886e-04 - mean_absolute_error: 0.0157 - val_loss: 2.9454e-05 - val_mean_absolute_error: 0.0049\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 0.00002\n",
      "Epoch 270/800\n",
      "90/90 [==============================] - 36s 399ms/step - loss: 3.0680e-04 - mean_absolute_error: 0.0161 - val_loss: 6.0315e-05 - val_mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 0.00002\n",
      "Epoch 271/800\n",
      "90/90 [==============================] - 38s 426ms/step - loss: 3.1093e-04 - mean_absolute_error: 0.0163 - val_loss: 1.0758e-04 - val_mean_absolute_error: 0.0113\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 0.00002\n",
      "Epoch 272/800\n",
      "90/90 [==============================] - 37s 412ms/step - loss: 2.9474e-04 - mean_absolute_error: 0.0161 - val_loss: 5.0489e-05 - val_mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 0.00002\n",
      "Epoch 273/800\n",
      "90/90 [==============================] - 35s 394ms/step - loss: 3.1296e-04 - mean_absolute_error: 0.0164 - val_loss: 2.6554e-05 - val_mean_absolute_error: 0.0050\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 0.00002\n",
      "Epoch 274/800\n",
      "90/90 [==============================] - 36s 404ms/step - loss: 2.9785e-04 - mean_absolute_error: 0.0157 - val_loss: 2.4748e-05 - val_mean_absolute_error: 0.0040\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 0.00002\n",
      "Epoch 275/800\n",
      "90/90 [==============================] - 36s 403ms/step - loss: 2.9415e-04 - mean_absolute_error: 0.0158 - val_loss: 2.6012e-05 - val_mean_absolute_error: 0.0042\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 0.00002\n",
      "Epoch 276/800\n",
      "90/90 [==============================] - 37s 414ms/step - loss: 3.1512e-04 - mean_absolute_error: 0.0162 - val_loss: 3.4815e-05 - val_mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 0.00002\n",
      "Epoch 277/800\n",
      "90/90 [==============================] - 37s 416ms/step - loss: 2.9627e-04 - mean_absolute_error: 0.0161 - val_loss: 4.7455e-05 - val_mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 0.00002\n",
      "Epoch 278/800\n",
      "90/90 [==============================] - 37s 415ms/step - loss: 3.0275e-04 - mean_absolute_error: 0.0162 - val_loss: 4.0463e-05 - val_mean_absolute_error: 0.0061\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 0.00002\n",
      "Epoch 279/800\n",
      "90/90 [==============================] - 39s 433ms/step - loss: 3.0785e-04 - mean_absolute_error: 0.0162 - val_loss: 1.9190e-05 - val_mean_absolute_error: 0.0043\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 0.00002\n",
      "Epoch 280/800\n",
      "90/90 [==============================] - 37s 412ms/step - loss: 2.9891e-04 - mean_absolute_error: 0.0157 - val_loss: 2.6607e-05 - val_mean_absolute_error: 0.0054\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 0.00002\n",
      "Epoch 281/800\n",
      "90/90 [==============================] - 37s 413ms/step - loss: 2.9125e-04 - mean_absolute_error: 0.0158 - val_loss: 5.1164e-05 - val_mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 0.00002\n",
      "Epoch 282/800\n",
      "90/90 [==============================] - 37s 413ms/step - loss: 2.9105e-04 - mean_absolute_error: 0.0159 - val_loss: 2.3159e-05 - val_mean_absolute_error: 0.0050\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 0.00002\n",
      "Epoch 283/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 35s 394ms/step - loss: 2.9440e-04 - mean_absolute_error: 0.0161 - val_loss: 2.2478e-05 - val_mean_absolute_error: 0.0040\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 0.00002\n",
      "Epoch 284/800\n",
      "90/90 [==============================] - 38s 419ms/step - loss: 2.9185e-04 - mean_absolute_error: 0.0158 - val_loss: 5.9406e-05 - val_mean_absolute_error: 0.0098\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 0.00002\n",
      "Epoch 285/800\n",
      "90/90 [==============================] - 37s 412ms/step - loss: 3.1872e-04 - mean_absolute_error: 0.0165 - val_loss: 4.0130e-05 - val_mean_absolute_error: 0.0061\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 0.00002\n",
      "Epoch 286/800\n",
      "90/90 [==============================] - 38s 423ms/step - loss: 3.0572e-04 - mean_absolute_error: 0.0162 - val_loss: 4.7892e-05 - val_mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 0.00002\n",
      "Epoch 287/800\n",
      "90/90 [==============================] - 38s 425ms/step - loss: 3.0773e-04 - mean_absolute_error: 0.0161 - val_loss: 4.7876e-05 - val_mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 0.00002\n",
      "Epoch 288/800\n",
      "90/90 [==============================] - 38s 423ms/step - loss: 2.8291e-04 - mean_absolute_error: 0.0158 - val_loss: 2.9925e-05 - val_mean_absolute_error: 0.0055\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 0.00002\n",
      "Epoch 289/800\n",
      "90/90 [==============================] - 37s 417ms/step - loss: 2.9383e-04 - mean_absolute_error: 0.0160 - val_loss: 7.5995e-05 - val_mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 0.00002\n",
      "Epoch 290/800\n",
      "90/90 [==============================] - 37s 410ms/step - loss: 3.1544e-04 - mean_absolute_error: 0.0162 - val_loss: 2.7408e-05 - val_mean_absolute_error: 0.0059\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 0.00002\n",
      "Epoch 291/800\n",
      "90/90 [==============================] - 36s 405ms/step - loss: 3.2849e-04 - mean_absolute_error: 0.0166 - val_loss: 1.7013e-05 - val_mean_absolute_error: 0.0037\n",
      "\n",
      "Epoch 00291: val_loss improved from 0.00002 to 0.00002, saving model to results\\2021-09-07_SPY-shuffle-1-scale-1-split_date-0-epochs-800.h5\n",
      "Epoch 292/800\n",
      "90/90 [==============================] - 39s 434ms/step - loss: 2.9098e-04 - mean_absolute_error: 0.0159 - val_loss: 3.0014e-05 - val_mean_absolute_error: 0.0054\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 0.00002\n",
      "Epoch 293/800\n",
      "90/90 [==============================] - 37s 408ms/step - loss: 2.8057e-04 - mean_absolute_error: 0.0154 - val_loss: 2.7071e-05 - val_mean_absolute_error: 0.0053\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 0.00002\n",
      "Epoch 294/800\n",
      "90/90 [==============================] - 35s 395ms/step - loss: 2.8614e-04 - mean_absolute_error: 0.0155 - val_loss: 3.2445e-05 - val_mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 0.00002\n",
      "Epoch 295/800\n",
      "90/90 [==============================] - 37s 416ms/step - loss: 2.8257e-04 - mean_absolute_error: 0.0156 - val_loss: 3.1685e-05 - val_mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 0.00002\n",
      "Epoch 296/800\n",
      "90/90 [==============================] - 37s 404ms/step - loss: 3.0684e-04 - mean_absolute_error: 0.0160 - val_loss: 2.5353e-05 - val_mean_absolute_error: 0.0053\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 0.00002\n",
      "Epoch 297/800\n",
      "90/90 [==============================] - 38s 425ms/step - loss: 2.9811e-04 - mean_absolute_error: 0.0161 - val_loss: 3.1898e-05 - val_mean_absolute_error: 0.0047\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 0.00002\n",
      "Epoch 298/800\n",
      "90/90 [==============================] - 36s 402ms/step - loss: 2.8105e-04 - mean_absolute_error: 0.0158 - val_loss: 4.9733e-05 - val_mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 0.00002\n",
      "Epoch 299/800\n",
      "90/90 [==============================] - 38s 427ms/step - loss: 2.7049e-04 - mean_absolute_error: 0.0155 - val_loss: 2.5200e-05 - val_mean_absolute_error: 0.0045\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 0.00002\n",
      "Epoch 300/800\n",
      "90/90 [==============================] - 37s 409ms/step - loss: 3.1240e-04 - mean_absolute_error: 0.0162 - val_loss: 2.6661e-05 - val_mean_absolute_error: 0.0052\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 0.00002\n",
      "Epoch 301/800\n",
      "90/90 [==============================] - 36s 401ms/step - loss: 2.8955e-04 - mean_absolute_error: 0.0159 - val_loss: 3.6978e-05 - val_mean_absolute_error: 0.0061\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 0.00002\n",
      "Epoch 302/800\n",
      "90/90 [==============================] - 37s 414ms/step - loss: 2.6677e-04 - mean_absolute_error: 0.0152 - val_loss: 2.7121e-05 - val_mean_absolute_error: 0.0055\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 0.00002\n",
      "Epoch 303/800\n",
      "90/90 [==============================] - 37s 410ms/step - loss: 3.0356e-04 - mean_absolute_error: 0.0160 - val_loss: 2.7004e-05 - val_mean_absolute_error: 0.0052\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 0.00002\n",
      "Epoch 304/800\n",
      "90/90 [==============================] - 36s 404ms/step - loss: 2.7820e-04 - mean_absolute_error: 0.0154 - val_loss: 5.1401e-05 - val_mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 0.00002\n",
      "Epoch 305/800\n",
      "90/90 [==============================] - 38s 425ms/step - loss: 2.8344e-04 - mean_absolute_error: 0.0155 - val_loss: 6.8820e-05 - val_mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 0.00002\n",
      "Epoch 306/800\n",
      "90/90 [==============================] - 37s 404ms/step - loss: 2.8459e-04 - mean_absolute_error: 0.0155 - val_loss: 4.2510e-05 - val_mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 0.00002\n",
      "Epoch 307/800\n",
      "90/90 [==============================] - 36s 396ms/step - loss: 2.8960e-04 - mean_absolute_error: 0.0155 - val_loss: 1.7538e-05 - val_mean_absolute_error: 0.0040\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 0.00002\n",
      "Epoch 308/800\n",
      "90/90 [==============================] - 37s 411ms/step - loss: 2.7628e-04 - mean_absolute_error: 0.0153 - val_loss: 4.1377e-05 - val_mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 0.00002\n",
      "Epoch 309/800\n",
      "90/90 [==============================] - 37s 417ms/step - loss: 2.8409e-04 - mean_absolute_error: 0.0157 - val_loss: 3.8358e-05 - val_mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 0.00002\n",
      "Epoch 310/800\n",
      "90/90 [==============================] - 38s 423ms/step - loss: 2.8520e-04 - mean_absolute_error: 0.0156 - val_loss: 6.6731e-05 - val_mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 0.00002\n",
      "Epoch 311/800\n",
      "90/90 [==============================] - 36s 405ms/step - loss: 2.8540e-04 - mean_absolute_error: 0.0157 - val_loss: 3.0036e-05 - val_mean_absolute_error: 0.0058\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 0.00002\n",
      "Epoch 312/800\n",
      "90/90 [==============================] - 37s 412ms/step - loss: 2.8458e-04 - mean_absolute_error: 0.0159 - val_loss: 3.0510e-05 - val_mean_absolute_error: 0.0053\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 0.00002\n",
      "Epoch 313/800\n",
      "90/90 [==============================] - 37s 412ms/step - loss: 2.7736e-04 - mean_absolute_error: 0.0156 - val_loss: 3.6874e-05 - val_mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 0.00002\n",
      "Epoch 314/800\n",
      "90/90 [==============================] - 35s 393ms/step - loss: 2.8164e-04 - mean_absolute_error: 0.0154 - val_loss: 4.2196e-05 - val_mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 0.00002\n",
      "Epoch 315/800\n",
      "90/90 [==============================] - 38s 424ms/step - loss: 2.8701e-04 - mean_absolute_error: 0.0154 - val_loss: 3.1241e-05 - val_mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 0.00002\n",
      "Epoch 316/800\n",
      "90/90 [==============================] - 38s 419ms/step - loss: 3.0967e-04 - mean_absolute_error: 0.0159 - val_loss: 3.1336e-05 - val_mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 0.00002\n",
      "Epoch 317/800\n",
      "90/90 [==============================] - 38s 420ms/step - loss: 2.9403e-04 - mean_absolute_error: 0.0160 - val_loss: 3.4166e-05 - val_mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 0.00002\n",
      "Epoch 318/800\n",
      "90/90 [==============================] - 38s 419ms/step - loss: 2.9326e-04 - mean_absolute_error: 0.0158 - val_loss: 2.4786e-05 - val_mean_absolute_error: 0.0050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00318: val_loss did not improve from 0.00002\n",
      "Epoch 319/800\n",
      "90/90 [==============================] - 38s 424ms/step - loss: 2.7732e-04 - mean_absolute_error: 0.0153 - val_loss: 2.0014e-05 - val_mean_absolute_error: 0.0043\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 0.00002\n",
      "Epoch 320/800\n",
      "90/90 [==============================] - 38s 424ms/step - loss: 2.7917e-04 - mean_absolute_error: 0.0154 - val_loss: 3.5886e-05 - val_mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 0.00002\n",
      "Epoch 321/800\n",
      "90/90 [==============================] - 37s 408ms/step - loss: 2.7110e-04 - mean_absolute_error: 0.0153 - val_loss: 2.2491e-05 - val_mean_absolute_error: 0.0046\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 0.00002\n",
      "Epoch 322/800\n",
      "90/90 [==============================] - 35s 391ms/step - loss: 2.8135e-04 - mean_absolute_error: 0.0157 - val_loss: 1.9004e-05 - val_mean_absolute_error: 0.0042\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 0.00002\n",
      "Epoch 323/800\n",
      "90/90 [==============================] - 37s 410ms/step - loss: 2.6162e-04 - mean_absolute_error: 0.0149 - val_loss: 1.3751e-04 - val_mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 0.00002\n",
      "Epoch 324/800\n",
      "90/90 [==============================] - 36s 401ms/step - loss: 3.0789e-04 - mean_absolute_error: 0.0159 - val_loss: 4.0286e-05 - val_mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 0.00002\n",
      "Epoch 325/800\n",
      "90/90 [==============================] - 37s 413ms/step - loss: 2.9395e-04 - mean_absolute_error: 0.0158 - val_loss: 2.2224e-05 - val_mean_absolute_error: 0.0048\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 0.00002\n",
      "Epoch 326/800\n",
      "90/90 [==============================] - 37s 412ms/step - loss: 3.0067e-04 - mean_absolute_error: 0.0159 - val_loss: 6.3721e-05 - val_mean_absolute_error: 0.0091\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 0.00002\n",
      "Epoch 327/800\n",
      "90/90 [==============================] - 36s 395ms/step - loss: 3.1424e-04 - mean_absolute_error: 0.0166 - val_loss: 1.0358e-04 - val_mean_absolute_error: 0.0114\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 0.00002\n",
      "Epoch 328/800\n",
      "90/90 [==============================] - 39s 435ms/step - loss: 2.8870e-04 - mean_absolute_error: 0.0159 - val_loss: 5.5072e-05 - val_mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 0.00002\n",
      "Epoch 329/800\n",
      "90/90 [==============================] - 37s 405ms/step - loss: 2.7560e-04 - mean_absolute_error: 0.0156 - val_loss: 3.0315e-05 - val_mean_absolute_error: 0.0061\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 0.00002\n",
      "Epoch 330/800\n",
      "90/90 [==============================] - 37s 411ms/step - loss: 2.6848e-04 - mean_absolute_error: 0.0152 - val_loss: 2.0140e-05 - val_mean_absolute_error: 0.0048\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 0.00002\n",
      "Epoch 331/800\n",
      "90/90 [==============================] - 37s 409ms/step - loss: 2.6698e-04 - mean_absolute_error: 0.0152 - val_loss: 2.9954e-05 - val_mean_absolute_error: 0.0061\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 0.00002\n",
      "Epoch 332/800\n",
      "90/90 [==============================] - 37s 411ms/step - loss: 2.9975e-04 - mean_absolute_error: 0.0164 - val_loss: 7.6914e-05 - val_mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 0.00002\n",
      "Epoch 333/800\n",
      "90/90 [==============================] - 37s 408ms/step - loss: 2.5866e-04 - mean_absolute_error: 0.0152 - val_loss: 2.9194e-05 - val_mean_absolute_error: 0.0060\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 0.00002\n",
      "Epoch 334/800\n",
      "90/90 [==============================] - 36s 405ms/step - loss: 2.6881e-04 - mean_absolute_error: 0.0155 - val_loss: 2.2059e-05 - val_mean_absolute_error: 0.0042\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 0.00002\n",
      "Epoch 335/800\n",
      "90/90 [==============================] - 39s 429ms/step - loss: 2.6577e-04 - mean_absolute_error: 0.0151 - val_loss: 4.1404e-05 - val_mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 0.00002\n",
      "Epoch 336/800\n",
      "90/90 [==============================] - 37s 407ms/step - loss: 2.8007e-04 - mean_absolute_error: 0.0154 - val_loss: 1.8933e-05 - val_mean_absolute_error: 0.0043\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 0.00002\n",
      "Epoch 337/800\n",
      "90/90 [==============================] - 38s 425ms/step - loss: 2.9421e-04 - mean_absolute_error: 0.0159 - val_loss: 2.2508e-05 - val_mean_absolute_error: 0.0047\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 0.00002\n",
      "Epoch 338/800\n",
      "90/90 [==============================] - 37s 413ms/step - loss: 2.6951e-04 - mean_absolute_error: 0.0154 - val_loss: 3.9503e-05 - val_mean_absolute_error: 0.0060\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 0.00002\n",
      "Epoch 339/800\n",
      "90/90 [==============================] - 37s 408ms/step - loss: 2.5578e-04 - mean_absolute_error: 0.0150 - val_loss: 3.8237e-05 - val_mean_absolute_error: 0.0056\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 0.00002\n",
      "Epoch 340/800\n",
      "90/90 [==============================] - 37s 414ms/step - loss: 2.6746e-04 - mean_absolute_error: 0.0151 - val_loss: 4.1897e-05 - val_mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 0.00002\n",
      "Epoch 341/800\n",
      "90/90 [==============================] - 37s 411ms/step - loss: 2.6349e-04 - mean_absolute_error: 0.0152 - val_loss: 4.7365e-05 - val_mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 0.00002\n",
      "Epoch 342/800\n",
      "90/90 [==============================] - 37s 410ms/step - loss: 2.7583e-04 - mean_absolute_error: 0.0156 - val_loss: 3.4041e-05 - val_mean_absolute_error: 0.0057\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 0.00002\n",
      "Epoch 343/800\n",
      "90/90 [==============================] - 36s 402ms/step - loss: 2.8038e-04 - mean_absolute_error: 0.0154 - val_loss: 2.1127e-05 - val_mean_absolute_error: 0.0040\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 0.00002\n",
      "Epoch 344/800\n",
      "90/90 [==============================] - 37s 409ms/step - loss: 2.9499e-04 - mean_absolute_error: 0.0159 - val_loss: 6.2123e-05 - val_mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 0.00002\n",
      "Epoch 345/800\n",
      "90/90 [==============================] - 36s 403ms/step - loss: 2.9095e-04 - mean_absolute_error: 0.0154 - val_loss: 2.2308e-05 - val_mean_absolute_error: 0.0047\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 0.00002\n",
      "Epoch 346/800\n",
      "90/90 [==============================] - 38s 420ms/step - loss: 2.8594e-04 - mean_absolute_error: 0.0158 - val_loss: 3.3495e-05 - val_mean_absolute_error: 0.0059\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 0.00002\n",
      "Epoch 347/800\n",
      "90/90 [==============================] - 36s 403ms/step - loss: 3.2124e-04 - mean_absolute_error: 0.0166 - val_loss: 6.1023e-05 - val_mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 0.00002\n",
      "Epoch 348/800\n",
      "90/90 [==============================] - 36s 395ms/step - loss: 2.6964e-04 - mean_absolute_error: 0.0152 - val_loss: 1.7420e-05 - val_mean_absolute_error: 0.0038\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 0.00002\n",
      "Epoch 349/800\n",
      "90/90 [==============================] - 37s 410ms/step - loss: 2.8854e-04 - mean_absolute_error: 0.0156 - val_loss: 3.0798e-05 - val_mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 0.00002\n",
      "Epoch 350/800\n",
      "90/90 [==============================] - 37s 413ms/step - loss: 2.7125e-04 - mean_absolute_error: 0.0152 - val_loss: 4.3064e-05 - val_mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 0.00002\n",
      "Epoch 351/800\n",
      "90/90 [==============================] - 39s 432ms/step - loss: 2.6934e-04 - mean_absolute_error: 0.0152 - val_loss: 8.8077e-05 - val_mean_absolute_error: 0.0097\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 0.00002\n",
      "Epoch 352/800\n",
      "90/90 [==============================] - 37s 406ms/step - loss: 2.8199e-04 - mean_absolute_error: 0.0154 - val_loss: 2.1188e-05 - val_mean_absolute_error: 0.0046\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 0.00002\n",
      "Epoch 353/800\n",
      "90/90 [==============================] - 37s 414ms/step - loss: 2.6992e-04 - mean_absolute_error: 0.0150 - val_loss: 1.7632e-05 - val_mean_absolute_error: 0.0036\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 0.00002\n",
      "Epoch 354/800\n",
      "90/90 [==============================] - 38s 422ms/step - loss: 2.6271e-04 - mean_absolute_error: 0.0151 - val_loss: 2.9988e-05 - val_mean_absolute_error: 0.0059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00354: val_loss did not improve from 0.00002\n",
      "Epoch 355/800\n",
      "90/90 [==============================] - 36s 401ms/step - loss: 2.7157e-04 - mean_absolute_error: 0.0152 - val_loss: 3.1269e-05 - val_mean_absolute_error: 0.0049\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 0.00002\n",
      "Epoch 356/800\n",
      "90/90 [==============================] - 37s 414ms/step - loss: 2.7911e-04 - mean_absolute_error: 0.0154 - val_loss: 5.4360e-05 - val_mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 0.00002\n",
      "Epoch 357/800\n",
      "90/90 [==============================] - 37s 411ms/step - loss: 2.7583e-04 - mean_absolute_error: 0.0156 - val_loss: 3.7530e-05 - val_mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 0.00002\n",
      "Epoch 358/800\n",
      "90/90 [==============================] - 38s 422ms/step - loss: 3.1156e-04 - mean_absolute_error: 0.0163 - val_loss: 4.7967e-05 - val_mean_absolute_error: 0.0056\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 0.00002\n",
      "Epoch 359/800\n",
      "90/90 [==============================] - 39s 437ms/step - loss: 2.8221e-04 - mean_absolute_error: 0.0157 - val_loss: 3.1755e-05 - val_mean_absolute_error: 0.0048\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 0.00002\n",
      "Epoch 360/800\n",
      "90/90 [==============================] - 37s 409ms/step - loss: 2.7344e-04 - mean_absolute_error: 0.0153 - val_loss: 2.7636e-05 - val_mean_absolute_error: 0.0055\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 0.00002\n",
      "Epoch 361/800\n",
      "90/90 [==============================] - 36s 399ms/step - loss: 2.5182e-04 - mean_absolute_error: 0.0149 - val_loss: 2.0222e-05 - val_mean_absolute_error: 0.0044\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 0.00002\n",
      "Epoch 362/800\n",
      "90/90 [==============================] - 37s 414ms/step - loss: 2.7941e-04 - mean_absolute_error: 0.0153 - val_loss: 2.6869e-05 - val_mean_absolute_error: 0.0055\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 0.00002\n",
      "Epoch 363/800\n",
      "90/90 [==============================] - 36s 400ms/step - loss: 2.9461e-04 - mean_absolute_error: 0.0162 - val_loss: 1.7345e-05 - val_mean_absolute_error: 0.0039\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 0.00002\n",
      "Epoch 364/800\n",
      "90/90 [==============================] - 38s 419ms/step - loss: 2.4669e-04 - mean_absolute_error: 0.0148 - val_loss: 2.2792e-05 - val_mean_absolute_error: 0.0048\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 0.00002\n",
      "Epoch 365/800\n",
      "90/90 [==============================] - 37s 410ms/step - loss: 2.9730e-04 - mean_absolute_error: 0.0159 - val_loss: 6.8014e-05 - val_mean_absolute_error: 0.0092\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 0.00002\n",
      "Epoch 366/800\n",
      "90/90 [==============================] - 38s 425ms/step - loss: 2.7406e-04 - mean_absolute_error: 0.0157 - val_loss: 4.7500e-05 - val_mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 0.00002\n",
      "Epoch 367/800\n",
      "90/90 [==============================] - 38s 420ms/step - loss: 2.8284e-04 - mean_absolute_error: 0.0155 - val_loss: 3.2961e-05 - val_mean_absolute_error: 0.0047\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 0.00002\n",
      "Epoch 368/800\n",
      "90/90 [==============================] - 37s 414ms/step - loss: 2.8383e-04 - mean_absolute_error: 0.0157 - val_loss: 3.5046e-05 - val_mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 0.00002\n",
      "Epoch 369/800\n",
      "90/90 [==============================] - 38s 418ms/step - loss: 2.9312e-04 - mean_absolute_error: 0.0159 - val_loss: 3.6620e-05 - val_mean_absolute_error: 0.0059\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 0.00002\n",
      "Epoch 370/800\n",
      "90/90 [==============================] - 37s 416ms/step - loss: 2.6082e-04 - mean_absolute_error: 0.0148 - val_loss: 2.5980e-05 - val_mean_absolute_error: 0.0058\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 0.00002\n",
      "Epoch 371/800\n",
      "90/90 [==============================] - 38s 417ms/step - loss: 2.8649e-04 - mean_absolute_error: 0.0156 - val_loss: 3.0515e-05 - val_mean_absolute_error: 0.0055\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 0.00002\n",
      "Epoch 372/800\n",
      "90/90 [==============================] - 39s 434ms/step - loss: 3.1003e-04 - mean_absolute_error: 0.0161 - val_loss: 1.0974e-04 - val_mean_absolute_error: 0.0113\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 0.00002\n",
      "Epoch 373/800\n",
      "90/90 [==============================] - 37s 409ms/step - loss: 2.8167e-04 - mean_absolute_error: 0.0156 - val_loss: 3.5092e-05 - val_mean_absolute_error: 0.0055\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 0.00002\n",
      "Epoch 374/800\n",
      "90/90 [==============================] - 38s 427ms/step - loss: 2.7833e-04 - mean_absolute_error: 0.0153 - val_loss: 3.5696e-05 - val_mean_absolute_error: 0.0046\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 0.00002\n",
      "Epoch 375/800\n",
      "90/90 [==============================] - 37s 408ms/step - loss: 2.5213e-04 - mean_absolute_error: 0.0147 - val_loss: 3.4859e-05 - val_mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 0.00002\n",
      "Epoch 376/800\n",
      "90/90 [==============================] - 36s 405ms/step - loss: 2.8150e-04 - mean_absolute_error: 0.0154 - val_loss: 3.9813e-05 - val_mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 0.00002\n",
      "Epoch 377/800\n",
      "90/90 [==============================] - 39s 431ms/step - loss: 2.6556e-04 - mean_absolute_error: 0.0152 - val_loss: 2.7438e-05 - val_mean_absolute_error: 0.0049\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 0.00002\n",
      "Epoch 378/800\n",
      "90/90 [==============================] - 37s 414ms/step - loss: 2.7656e-04 - mean_absolute_error: 0.0151 - val_loss: 3.2478e-05 - val_mean_absolute_error: 0.0059\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 0.00002\n",
      "Epoch 379/800\n",
      "90/90 [==============================] - 37s 409ms/step - loss: 2.8284e-04 - mean_absolute_error: 0.0156 - val_loss: 2.5314e-05 - val_mean_absolute_error: 0.0049\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 0.00002\n",
      "Epoch 380/800\n",
      "90/90 [==============================] - 37s 414ms/step - loss: 2.5718e-04 - mean_absolute_error: 0.0147 - val_loss: 1.0253e-04 - val_mean_absolute_error: 0.0110\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 0.00002\n",
      "Epoch 381/800\n",
      "90/90 [==============================] - 37s 408ms/step - loss: 2.9813e-04 - mean_absolute_error: 0.0154 - val_loss: 3.2061e-05 - val_mean_absolute_error: 0.0052\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 0.00002\n",
      "Epoch 382/800\n",
      "90/90 [==============================] - 38s 421ms/step - loss: 2.7835e-04 - mean_absolute_error: 0.0153 - val_loss: 3.4159e-05 - val_mean_absolute_error: 0.0049\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 0.00002\n",
      "Epoch 383/800\n",
      "90/90 [==============================] - 39s 431ms/step - loss: 2.7339e-04 - mean_absolute_error: 0.0153 - val_loss: 2.2773e-05 - val_mean_absolute_error: 0.0046\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 0.00002\n",
      "Epoch 384/800\n",
      "90/90 [==============================] - 37s 409ms/step - loss: 2.7695e-04 - mean_absolute_error: 0.0154 - val_loss: 2.0688e-05 - val_mean_absolute_error: 0.0045\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 0.00002\n",
      "Epoch 385/800\n",
      "90/90 [==============================] - 39s 429ms/step - loss: 2.6813e-04 - mean_absolute_error: 0.0153 - val_loss: 1.1835e-04 - val_mean_absolute_error: 0.0106\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 0.00002\n",
      "Epoch 386/800\n",
      "90/90 [==============================] - 37s 414ms/step - loss: 2.8519e-04 - mean_absolute_error: 0.0156 - val_loss: 2.6859e-05 - val_mean_absolute_error: 0.0057\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 0.00002\n",
      "Epoch 387/800\n",
      "90/90 [==============================] - 38s 426ms/step - loss: 2.6697e-04 - mean_absolute_error: 0.0153 - val_loss: 3.4711e-05 - val_mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 0.00002\n",
      "Epoch 388/800\n",
      "90/90 [==============================] - 38s 425ms/step - loss: 2.4825e-04 - mean_absolute_error: 0.0147 - val_loss: 3.2568e-05 - val_mean_absolute_error: 0.0058\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 0.00002\n",
      "Epoch 389/800\n",
      "90/90 [==============================] - 37s 416ms/step - loss: 2.7406e-04 - mean_absolute_error: 0.0152 - val_loss: 2.6958e-05 - val_mean_absolute_error: 0.0049\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 0.00002\n",
      "Epoch 390/800\n",
      "90/90 [==============================] - 39s 430ms/step - loss: 3.1731e-04 - mean_absolute_error: 0.0164 - val_loss: 2.3449e-05 - val_mean_absolute_error: 0.0044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00390: val_loss did not improve from 0.00002\n",
      "Epoch 391/800\n",
      "90/90 [==============================] - 37s 415ms/step - loss: 2.5814e-04 - mean_absolute_error: 0.0150 - val_loss: 2.7558e-05 - val_mean_absolute_error: 0.0046\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 0.00002\n",
      "Epoch 392/800\n",
      "90/90 [==============================] - 38s 420ms/step - loss: 2.7700e-04 - mean_absolute_error: 0.0156 - val_loss: 2.2837e-05 - val_mean_absolute_error: 0.0046\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 0.00002\n",
      "Epoch 393/800\n",
      "90/90 [==============================] - 38s 418ms/step - loss: 3.0590e-04 - mean_absolute_error: 0.0162 - val_loss: 2.3939e-05 - val_mean_absolute_error: 0.0050\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 0.00002\n",
      "Epoch 394/800\n",
      "90/90 [==============================] - 37s 408ms/step - loss: 2.8103e-04 - mean_absolute_error: 0.0154 - val_loss: 6.5358e-05 - val_mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 0.00002\n",
      "Epoch 395/800\n",
      "90/90 [==============================] - 36s 398ms/step - loss: 2.6282e-04 - mean_absolute_error: 0.0150 - val_loss: 6.1497e-05 - val_mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 0.00002\n",
      "Epoch 396/800\n",
      "90/90 [==============================] - 39s 433ms/step - loss: 2.6183e-04 - mean_absolute_error: 0.0148 - val_loss: 1.5917e-05 - val_mean_absolute_error: 0.0033\n",
      "\n",
      "Epoch 00396: val_loss improved from 0.00002 to 0.00002, saving model to results\\2021-09-07_SPY-shuffle-1-scale-1-split_date-0-epochs-800.h5\n",
      "Epoch 397/800\n",
      "90/90 [==============================] - 36s 404ms/step - loss: 2.9273e-04 - mean_absolute_error: 0.0159 - val_loss: 3.5563e-05 - val_mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 0.00002\n",
      "Epoch 398/800\n",
      "90/90 [==============================] - 38s 428ms/step - loss: 2.8244e-04 - mean_absolute_error: 0.0153 - val_loss: 1.0024e-04 - val_mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 0.00002\n",
      "Epoch 399/800\n",
      "90/90 [==============================] - 36s 405ms/step - loss: 2.8180e-04 - mean_absolute_error: 0.0155 - val_loss: 4.2449e-05 - val_mean_absolute_error: 0.0060\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 0.00002\n",
      "Epoch 400/800\n",
      "90/90 [==============================] - 38s 427ms/step - loss: 2.7306e-04 - mean_absolute_error: 0.0154 - val_loss: 2.3970e-05 - val_mean_absolute_error: 0.0054\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 0.00002\n",
      "Epoch 401/800\n",
      "90/90 [==============================] - 39s 435ms/step - loss: 2.7014e-04 - mean_absolute_error: 0.0153 - val_loss: 2.0724e-05 - val_mean_absolute_error: 0.0041\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 0.00002\n",
      "Epoch 402/800\n",
      "90/90 [==============================] - 37s 410ms/step - loss: 2.5117e-04 - mean_absolute_error: 0.0147 - val_loss: 4.1395e-05 - val_mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 0.00002\n",
      "Epoch 403/800\n",
      "90/90 [==============================] - 38s 426ms/step - loss: 2.6012e-04 - mean_absolute_error: 0.0150 - val_loss: 4.7032e-05 - val_mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 0.00002\n",
      "Epoch 404/800\n",
      "90/90 [==============================] - 37s 415ms/step - loss: 2.7399e-04 - mean_absolute_error: 0.0155 - val_loss: 7.4502e-05 - val_mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 0.00002\n",
      "Epoch 405/800\n",
      "90/90 [==============================] - 36s 397ms/step - loss: 2.6875e-04 - mean_absolute_error: 0.0152 - val_loss: 2.7292e-05 - val_mean_absolute_error: 0.0044\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 0.00002\n",
      "Epoch 406/800\n",
      "90/90 [==============================] - 37s 409ms/step - loss: 2.7869e-04 - mean_absolute_error: 0.0153 - val_loss: 3.4841e-05 - val_mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 0.00002\n",
      "Epoch 407/800\n",
      "90/90 [==============================] - 37s 408ms/step - loss: 2.8661e-04 - mean_absolute_error: 0.0154 - val_loss: 5.3216e-05 - val_mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 0.00002\n",
      "Epoch 408/800\n",
      "90/90 [==============================] - 38s 426ms/step - loss: 2.9164e-04 - mean_absolute_error: 0.0157 - val_loss: 9.4818e-05 - val_mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 0.00002\n",
      "Epoch 409/800\n",
      "90/90 [==============================] - 38s 427ms/step - loss: 2.6359e-04 - mean_absolute_error: 0.0152 - val_loss: 2.6988e-05 - val_mean_absolute_error: 0.0045\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 0.00002\n",
      "Epoch 410/800\n",
      "90/90 [==============================] - 36s 403ms/step - loss: 2.4717e-04 - mean_absolute_error: 0.0146 - val_loss: 2.5803e-05 - val_mean_absolute_error: 0.0053\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 0.00002\n",
      "Epoch 411/800\n",
      "90/90 [==============================] - 37s 412ms/step - loss: 2.7013e-04 - mean_absolute_error: 0.0153 - val_loss: 2.0462e-05 - val_mean_absolute_error: 0.0045\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 0.00002\n",
      "Epoch 412/800\n",
      "90/90 [==============================] - 38s 420ms/step - loss: 2.6151e-04 - mean_absolute_error: 0.0148 - val_loss: 5.4496e-05 - val_mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 0.00002\n",
      "Epoch 413/800\n",
      "90/90 [==============================] - 37s 414ms/step - loss: 2.6854e-04 - mean_absolute_error: 0.0152 - val_loss: 2.5215e-05 - val_mean_absolute_error: 0.0054\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 0.00002\n",
      "Epoch 414/800\n",
      "90/90 [==============================] - 39s 438ms/step - loss: 2.6524e-04 - mean_absolute_error: 0.0153 - val_loss: 2.6331e-05 - val_mean_absolute_error: 0.0057\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 0.00002\n",
      "Epoch 415/800\n",
      "90/90 [==============================] - 37s 412ms/step - loss: 2.4703e-04 - mean_absolute_error: 0.0146 - val_loss: 2.8998e-05 - val_mean_absolute_error: 0.0046\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 0.00002\n",
      "Epoch 416/800\n",
      "90/90 [==============================] - 36s 402ms/step - loss: 2.7593e-04 - mean_absolute_error: 0.0155 - val_loss: 9.1026e-05 - val_mean_absolute_error: 0.0094\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 0.00002\n",
      "Epoch 417/800\n",
      "90/90 [==============================] - 38s 428ms/step - loss: 2.6186e-04 - mean_absolute_error: 0.0150 - val_loss: 2.6063e-05 - val_mean_absolute_error: 0.0055\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 0.00002\n",
      "Epoch 418/800\n",
      "90/90 [==============================] - 38s 421ms/step - loss: 2.7183e-04 - mean_absolute_error: 0.0150 - val_loss: 5.4708e-05 - val_mean_absolute_error: 0.0060\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 0.00002\n",
      "Epoch 419/800\n",
      "90/90 [==============================] - 39s 434ms/step - loss: 2.8555e-04 - mean_absolute_error: 0.0157 - val_loss: 8.6496e-05 - val_mean_absolute_error: 0.0093\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 0.00002\n",
      "Epoch 420/800\n",
      "90/90 [==============================] - 38s 419ms/step - loss: 2.7739e-04 - mean_absolute_error: 0.0154 - val_loss: 3.1079e-05 - val_mean_absolute_error: 0.0051\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 0.00002\n",
      "Epoch 421/800\n",
      "90/90 [==============================] - 39s 434ms/step - loss: 2.7931e-04 - mean_absolute_error: 0.0153 - val_loss: 2.3693e-05 - val_mean_absolute_error: 0.0048\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 0.00002\n",
      "Epoch 422/800\n",
      "90/90 [==============================] - 40s 446ms/step - loss: 2.8114e-04 - mean_absolute_error: 0.0155 - val_loss: 2.1855e-05 - val_mean_absolute_error: 0.0050\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 0.00002\n",
      "Epoch 423/800\n",
      "90/90 [==============================] - 39s 431ms/step - loss: 2.7600e-04 - mean_absolute_error: 0.0152 - val_loss: 4.9372e-05 - val_mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 0.00002\n",
      "Epoch 424/800\n",
      "90/90 [==============================] - 39s 435ms/step - loss: 2.7259e-04 - mean_absolute_error: 0.0152 - val_loss: 8.5092e-05 - val_mean_absolute_error: 0.0104\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 0.00002\n",
      "Epoch 425/800\n",
      "90/90 [==============================] - 38s 417ms/step - loss: 2.7605e-04 - mean_absolute_error: 0.0151 - val_loss: 1.7249e-05 - val_mean_absolute_error: 0.0041\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 0.00002\n",
      "Epoch 426/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 36s 401ms/step - loss: 2.7471e-04 - mean_absolute_error: 0.0152 - val_loss: 3.0678e-05 - val_mean_absolute_error: 0.0052\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 0.00002\n",
      "Epoch 427/800\n",
      "90/90 [==============================] - 39s 433ms/step - loss: 2.7788e-04 - mean_absolute_error: 0.0153 - val_loss: 2.6989e-05 - val_mean_absolute_error: 0.0043\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 0.00002\n",
      "Epoch 428/800\n",
      "90/90 [==============================] - 37s 410ms/step - loss: 2.8632e-04 - mean_absolute_error: 0.0157 - val_loss: 2.5242e-05 - val_mean_absolute_error: 0.0052\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 0.00002\n",
      "Epoch 429/800\n",
      "90/90 [==============================] - 39s 430ms/step - loss: 2.9082e-04 - mean_absolute_error: 0.0155 - val_loss: 1.8827e-05 - val_mean_absolute_error: 0.0040\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 0.00002\n",
      "Epoch 430/800\n",
      "90/90 [==============================] - 38s 418ms/step - loss: 2.7280e-04 - mean_absolute_error: 0.0154 - val_loss: 1.8813e-05 - val_mean_absolute_error: 0.0043\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 0.00002\n",
      "Epoch 431/800\n",
      "90/90 [==============================] - 37s 416ms/step - loss: 2.5813e-04 - mean_absolute_error: 0.0149 - val_loss: 2.8761e-05 - val_mean_absolute_error: 0.0059\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 0.00002\n",
      "Epoch 432/800\n",
      "90/90 [==============================] - 39s 431ms/step - loss: 2.6146e-04 - mean_absolute_error: 0.0147 - val_loss: 4.3225e-05 - val_mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 0.00002\n",
      "Epoch 433/800\n",
      "90/90 [==============================] - 38s 419ms/step - loss: 2.8565e-04 - mean_absolute_error: 0.0152 - val_loss: 3.2635e-05 - val_mean_absolute_error: 0.0055\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 0.00002\n",
      "Epoch 434/800\n",
      "90/90 [==============================] - 38s 422ms/step - loss: 2.7499e-04 - mean_absolute_error: 0.0151 - val_loss: 1.9322e-05 - val_mean_absolute_error: 0.0041\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 0.00002\n",
      "Epoch 435/800\n",
      "90/90 [==============================] - 38s 419ms/step - loss: 2.7713e-04 - mean_absolute_error: 0.0155 - val_loss: 2.0467e-05 - val_mean_absolute_error: 0.0046\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 0.00002\n",
      "Epoch 436/800\n",
      "90/90 [==============================] - 37s 413ms/step - loss: 2.4957e-04 - mean_absolute_error: 0.0148 - val_loss: 2.5478e-05 - val_mean_absolute_error: 0.0049\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 0.00002\n",
      "Epoch 437/800\n",
      "90/90 [==============================] - 39s 430ms/step - loss: 2.6341e-04 - mean_absolute_error: 0.0151 - val_loss: 3.0263e-05 - val_mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 0.00002\n",
      "Epoch 438/800\n",
      "90/90 [==============================] - 38s 423ms/step - loss: 3.1034e-04 - mean_absolute_error: 0.0161 - val_loss: 2.2453e-05 - val_mean_absolute_error: 0.0045\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 0.00002\n",
      "Epoch 439/800\n",
      "90/90 [==============================] - 36s 403ms/step - loss: 2.7003e-04 - mean_absolute_error: 0.0152 - val_loss: 1.6137e-05 - val_mean_absolute_error: 0.0040\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 0.00002\n",
      "Epoch 440/800\n",
      "90/90 [==============================] - 37s 412ms/step - loss: 2.5972e-04 - mean_absolute_error: 0.0149 - val_loss: 3.3304e-05 - val_mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 0.00002\n",
      "Epoch 441/800\n",
      "90/90 [==============================] - 38s 417ms/step - loss: 2.5462e-04 - mean_absolute_error: 0.0150 - val_loss: 2.0144e-05 - val_mean_absolute_error: 0.0047\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 0.00002\n",
      "Epoch 442/800\n",
      "90/90 [==============================] - 36s 398ms/step - loss: 2.8147e-04 - mean_absolute_error: 0.0150 - val_loss: 7.7401e-05 - val_mean_absolute_error: 0.0097\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 0.00002\n",
      "Epoch 443/800\n",
      "90/90 [==============================] - 39s 430ms/step - loss: 2.5797e-04 - mean_absolute_error: 0.0149 - val_loss: 1.8512e-05 - val_mean_absolute_error: 0.0037\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 0.00002\n",
      "Epoch 444/800\n",
      "90/90 [==============================] - 37s 406ms/step - loss: 2.6701e-04 - mean_absolute_error: 0.0149 - val_loss: 3.6577e-05 - val_mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 0.00002\n",
      "Epoch 445/800\n",
      "90/90 [==============================] - 37s 410ms/step - loss: 2.7099e-04 - mean_absolute_error: 0.0153 - val_loss: 2.8574e-05 - val_mean_absolute_error: 0.0046\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 0.00002\n",
      "Epoch 446/800\n",
      "90/90 [==============================] - 38s 419ms/step - loss: 2.7045e-04 - mean_absolute_error: 0.0154 - val_loss: 3.9153e-05 - val_mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 0.00002\n",
      "Epoch 447/800\n",
      "90/90 [==============================] - 37s 410ms/step - loss: 2.6012e-04 - mean_absolute_error: 0.0149 - val_loss: 4.9049e-05 - val_mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 0.00002\n",
      "Epoch 448/800\n",
      "90/90 [==============================] - 37s 417ms/step - loss: 2.6991e-04 - mean_absolute_error: 0.0151 - val_loss: 1.2156e-04 - val_mean_absolute_error: 0.0109\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 0.00002\n",
      "Epoch 449/800\n",
      "90/90 [==============================] - 37s 415ms/step - loss: 2.6417e-04 - mean_absolute_error: 0.0150 - val_loss: 4.3682e-05 - val_mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 0.00002\n",
      "Epoch 450/800\n",
      "90/90 [==============================] - 38s 423ms/step - loss: 2.8328e-04 - mean_absolute_error: 0.0156 - val_loss: 2.9985e-05 - val_mean_absolute_error: 0.0049\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 0.00002\n",
      "Epoch 451/800\n",
      "90/90 [==============================] - 39s 430ms/step - loss: 2.6346e-04 - mean_absolute_error: 0.0148 - val_loss: 2.6866e-05 - val_mean_absolute_error: 0.0050\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 0.00002\n",
      "Epoch 452/800\n",
      "90/90 [==============================] - 38s 421ms/step - loss: 2.5688e-04 - mean_absolute_error: 0.0148 - val_loss: 2.9945e-05 - val_mean_absolute_error: 0.0053\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 0.00002\n",
      "Epoch 453/800\n",
      "90/90 [==============================] - 38s 425ms/step - loss: 2.7621e-04 - mean_absolute_error: 0.0153 - val_loss: 4.7278e-05 - val_mean_absolute_error: 0.0060\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 0.00002\n",
      "Epoch 454/800\n",
      "90/90 [==============================] - 38s 424ms/step - loss: 2.4917e-04 - mean_absolute_error: 0.0146 - val_loss: 2.2080e-05 - val_mean_absolute_error: 0.0044\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 0.00002\n",
      "Epoch 455/800\n",
      "90/90 [==============================] - 38s 415ms/step - loss: 2.7240e-04 - mean_absolute_error: 0.0152 - val_loss: 2.9255e-05 - val_mean_absolute_error: 0.0059\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 0.00002\n",
      "Epoch 456/800\n",
      "90/90 [==============================] - 38s 426ms/step - loss: 2.8041e-04 - mean_absolute_error: 0.0153 - val_loss: 2.6000e-05 - val_mean_absolute_error: 0.0056\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 0.00002\n",
      "Epoch 457/800\n",
      "90/90 [==============================] - 37s 411ms/step - loss: 2.7504e-04 - mean_absolute_error: 0.0153 - val_loss: 2.2860e-05 - val_mean_absolute_error: 0.0044\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 0.00002\n",
      "Epoch 458/800\n",
      "90/90 [==============================] - 39s 431ms/step - loss: 2.7288e-04 - mean_absolute_error: 0.0152 - val_loss: 1.9304e-05 - val_mean_absolute_error: 0.0042\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 0.00002\n",
      "Epoch 459/800\n",
      "90/90 [==============================] - 38s 428ms/step - loss: 2.5974e-04 - mean_absolute_error: 0.0148 - val_loss: 2.1631e-05 - val_mean_absolute_error: 0.0044\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 0.00002\n",
      "Epoch 460/800\n",
      "90/90 [==============================] - 37s 416ms/step - loss: 2.7272e-04 - mean_absolute_error: 0.0151 - val_loss: 3.0110e-05 - val_mean_absolute_error: 0.0059\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 0.00002\n",
      "Epoch 461/800\n",
      "90/90 [==============================] - 37s 408ms/step - loss: 2.8035e-04 - mean_absolute_error: 0.0154 - val_loss: 3.2704e-05 - val_mean_absolute_error: 0.0051\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 0.00002\n",
      "Epoch 462/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 38s 423ms/step - loss: 2.6852e-04 - mean_absolute_error: 0.0151 - val_loss: 1.6268e-05 - val_mean_absolute_error: 0.0036\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 0.00002\n",
      "Epoch 463/800\n",
      "90/90 [==============================] - 38s 427ms/step - loss: 2.5544e-04 - mean_absolute_error: 0.0151 - val_loss: 7.9203e-05 - val_mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 0.00002\n",
      "Epoch 464/800\n",
      "90/90 [==============================] - 39s 434ms/step - loss: 2.7171e-04 - mean_absolute_error: 0.0153 - val_loss: 1.9996e-05 - val_mean_absolute_error: 0.0044\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 0.00002\n",
      "Epoch 465/800\n",
      "90/90 [==============================] - 38s 418ms/step - loss: 2.6813e-04 - mean_absolute_error: 0.0150 - val_loss: 3.1111e-05 - val_mean_absolute_error: 0.0057\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 0.00002\n",
      "Epoch 466/800\n",
      "90/90 [==============================] - 37s 412ms/step - loss: 2.5821e-04 - mean_absolute_error: 0.0150 - val_loss: 2.3356e-05 - val_mean_absolute_error: 0.0054\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 0.00002\n",
      "Epoch 467/800\n",
      "90/90 [==============================] - 39s 435ms/step - loss: 2.7427e-04 - mean_absolute_error: 0.0155 - val_loss: 5.1437e-05 - val_mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 0.00002\n",
      "Epoch 468/800\n",
      "90/90 [==============================] - 37s 412ms/step - loss: 2.5051e-04 - mean_absolute_error: 0.0147 - val_loss: 3.5694e-05 - val_mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 0.00002\n",
      "Epoch 469/800\n",
      "90/90 [==============================] - 39s 430ms/step - loss: 2.6421e-04 - mean_absolute_error: 0.0150 - val_loss: 1.6888e-05 - val_mean_absolute_error: 0.0042\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 0.00002\n",
      "Epoch 470/800\n",
      "90/90 [==============================] - 38s 418ms/step - loss: 2.5611e-04 - mean_absolute_error: 0.0148 - val_loss: 2.8629e-05 - val_mean_absolute_error: 0.0060\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 0.00002\n",
      "Epoch 471/800\n",
      "90/90 [==============================] - 35s 390ms/step - loss: 2.7721e-04 - mean_absolute_error: 0.0154 - val_loss: 4.0159e-05 - val_mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 0.00002\n",
      "Epoch 472/800\n",
      "90/90 [==============================] - 37s 413ms/step - loss: 2.5691e-04 - mean_absolute_error: 0.0148 - val_loss: 8.6262e-05 - val_mean_absolute_error: 0.0114\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 0.00002\n",
      "Epoch 473/800\n",
      "90/90 [==============================] - 37s 415ms/step - loss: 2.6074e-04 - mean_absolute_error: 0.0152 - val_loss: 6.6387e-05 - val_mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 0.00002\n",
      "Epoch 474/800\n",
      "90/90 [==============================] - 37s 412ms/step - loss: 2.6494e-04 - mean_absolute_error: 0.0149 - val_loss: 1.9087e-05 - val_mean_absolute_error: 0.0042\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 0.00002\n",
      "Epoch 475/800\n",
      "90/90 [==============================] - 38s 420ms/step - loss: 2.5862e-04 - mean_absolute_error: 0.0147 - val_loss: 3.9676e-05 - val_mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 0.00002\n",
      "Epoch 476/800\n",
      "90/90 [==============================] - 38s 425ms/step - loss: 2.8314e-04 - mean_absolute_error: 0.0153 - val_loss: 2.8131e-05 - val_mean_absolute_error: 0.0056\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 0.00002\n",
      "Epoch 477/800\n",
      "90/90 [==============================] - 38s 427ms/step - loss: 2.4991e-04 - mean_absolute_error: 0.0147 - val_loss: 2.0778e-05 - val_mean_absolute_error: 0.0049\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 0.00002\n",
      "Epoch 478/800\n",
      "90/90 [==============================] - 37s 406ms/step - loss: 2.8524e-04 - mean_absolute_error: 0.0156 - val_loss: 3.1367e-05 - val_mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 0.00002\n",
      "Epoch 479/800\n",
      "90/90 [==============================] - 35s 390ms/step - loss: 2.6291e-04 - mean_absolute_error: 0.0153 - val_loss: 2.2740e-05 - val_mean_absolute_error: 0.0045\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 0.00002\n",
      "Epoch 480/800\n",
      "90/90 [==============================] - 37s 413ms/step - loss: 2.6221e-04 - mean_absolute_error: 0.0149 - val_loss: 4.4255e-05 - val_mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 0.00002\n",
      "Epoch 481/800\n",
      "90/90 [==============================] - 37s 406ms/step - loss: 2.6184e-04 - mean_absolute_error: 0.0151 - val_loss: 1.8285e-05 - val_mean_absolute_error: 0.0043\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 0.00002\n",
      "Epoch 482/800\n",
      "90/90 [==============================] - 37s 408ms/step - loss: 2.3689e-04 - mean_absolute_error: 0.0146 - val_loss: 2.0333e-05 - val_mean_absolute_error: 0.0049\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 0.00002\n",
      "Epoch 483/800\n",
      "90/90 [==============================] - 37s 417ms/step - loss: 2.5503e-04 - mean_absolute_error: 0.0149 - val_loss: 1.1790e-04 - val_mean_absolute_error: 0.0111\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 0.00002\n",
      "Epoch 484/800\n",
      "90/90 [==============================] - 37s 413ms/step - loss: 2.6821e-04 - mean_absolute_error: 0.0151 - val_loss: 1.8309e-05 - val_mean_absolute_error: 0.0036\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 0.00002\n",
      "Epoch 485/800\n",
      "90/90 [==============================] - 38s 419ms/step - loss: 2.8273e-04 - mean_absolute_error: 0.0154 - val_loss: 2.0629e-05 - val_mean_absolute_error: 0.0047\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 0.00002\n",
      "Epoch 486/800\n",
      "90/90 [==============================] - 36s 401ms/step - loss: 2.6001e-04 - mean_absolute_error: 0.0148 - val_loss: 2.8107e-05 - val_mean_absolute_error: 0.0053\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 0.00002\n",
      "Epoch 487/800\n",
      "90/90 [==============================] - 36s 393ms/step - loss: 2.6219e-04 - mean_absolute_error: 0.0150 - val_loss: 2.6057e-05 - val_mean_absolute_error: 0.0053\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 0.00002\n",
      "Epoch 488/800\n",
      "90/90 [==============================] - 37s 410ms/step - loss: 2.7539e-04 - mean_absolute_error: 0.0152 - val_loss: 1.6506e-05 - val_mean_absolute_error: 0.0038\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 0.00002\n",
      "Epoch 489/800\n",
      "90/90 [==============================] - 37s 407ms/step - loss: 2.7541e-04 - mean_absolute_error: 0.0154 - val_loss: 1.5098e-05 - val_mean_absolute_error: 0.0034\n",
      "\n",
      "Epoch 00489: val_loss improved from 0.00002 to 0.00002, saving model to results\\2021-09-07_SPY-shuffle-1-scale-1-split_date-0-epochs-800.h5\n",
      "Epoch 490/800\n",
      "90/90 [==============================] - 38s 426ms/step - loss: 2.6738e-04 - mean_absolute_error: 0.0151 - val_loss: 2.7174e-05 - val_mean_absolute_error: 0.0055\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 0.00002\n",
      "Epoch 491/800\n",
      "90/90 [==============================] - 36s 403ms/step - loss: 2.5645e-04 - mean_absolute_error: 0.0147 - val_loss: 1.7698e-05 - val_mean_absolute_error: 0.0042\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 0.00002\n",
      "Epoch 492/800\n",
      "90/90 [==============================] - 37s 413ms/step - loss: 2.6854e-04 - mean_absolute_error: 0.0151 - val_loss: 4.7425e-05 - val_mean_absolute_error: 0.0059\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 0.00002\n",
      "Epoch 493/800\n",
      "90/90 [==============================] - 37s 417ms/step - loss: 2.5353e-04 - mean_absolute_error: 0.0146 - val_loss: 3.2313e-05 - val_mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 0.00002\n",
      "Epoch 494/800\n",
      "90/90 [==============================] - 36s 404ms/step - loss: 2.5447e-04 - mean_absolute_error: 0.0149 - val_loss: 2.2083e-05 - val_mean_absolute_error: 0.0041\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 0.00002\n",
      "Epoch 495/800\n",
      "90/90 [==============================] - 36s 404ms/step - loss: 2.6592e-04 - mean_absolute_error: 0.0151 - val_loss: 2.4479e-05 - val_mean_absolute_error: 0.0053\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 0.00002\n",
      "Epoch 496/800\n",
      "90/90 [==============================] - 36s 405ms/step - loss: 2.5031e-04 - mean_absolute_error: 0.0145 - val_loss: 1.9456e-05 - val_mean_absolute_error: 0.0043\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 0.00002\n",
      "Epoch 497/800\n",
      "90/90 [==============================] - 35s 394ms/step - loss: 2.7548e-04 - mean_absolute_error: 0.0152 - val_loss: 2.6767e-05 - val_mean_absolute_error: 0.0053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00497: val_loss did not improve from 0.00002\n",
      "Epoch 498/800\n",
      "90/90 [==============================] - 38s 422ms/step - loss: 2.6708e-04 - mean_absolute_error: 0.0150 - val_loss: 2.0166e-05 - val_mean_absolute_error: 0.0048\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 0.00002\n",
      "Epoch 499/800\n",
      "90/90 [==============================] - 36s 398ms/step - loss: 2.5869e-04 - mean_absolute_error: 0.0147 - val_loss: 2.6967e-05 - val_mean_absolute_error: 0.0054\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 0.00002\n",
      "Epoch 500/800\n",
      "90/90 [==============================] - 39s 430ms/step - loss: 2.6551e-04 - mean_absolute_error: 0.0150 - val_loss: 4.7132e-05 - val_mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 0.00002\n",
      "Epoch 501/800\n",
      "90/90 [==============================] - 37s 406ms/step - loss: 2.7393e-04 - mean_absolute_error: 0.0153 - val_loss: 3.7490e-05 - val_mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 00501: val_loss did not improve from 0.00002\n",
      "Epoch 502/800\n",
      "90/90 [==============================] - 37s 414ms/step - loss: 2.6383e-04 - mean_absolute_error: 0.0151 - val_loss: 2.0625e-05 - val_mean_absolute_error: 0.0044\n",
      "\n",
      "Epoch 00502: val_loss did not improve from 0.00002\n",
      "Epoch 503/800\n",
      "90/90 [==============================] - 38s 424ms/step - loss: 2.6490e-04 - mean_absolute_error: 0.0149 - val_loss: 4.3769e-05 - val_mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 00503: val_loss did not improve from 0.00002\n",
      "Epoch 504/800\n",
      "90/90 [==============================] - 36s 401ms/step - loss: 2.5244e-04 - mean_absolute_error: 0.0147 - val_loss: 5.0054e-05 - val_mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 00504: val_loss did not improve from 0.00002\n",
      "Epoch 505/800\n",
      "90/90 [==============================] - 37s 412ms/step - loss: 2.6363e-04 - mean_absolute_error: 0.0150 - val_loss: 3.7794e-05 - val_mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 00505: val_loss did not improve from 0.00002\n",
      "Epoch 506/800\n",
      "90/90 [==============================] - 37s 407ms/step - loss: 2.6195e-04 - mean_absolute_error: 0.0151 - val_loss: 1.6613e-05 - val_mean_absolute_error: 0.0039\n",
      "\n",
      "Epoch 00506: val_loss did not improve from 0.00002\n",
      "Epoch 507/800\n",
      "90/90 [==============================] - 35s 393ms/step - loss: 2.5793e-04 - mean_absolute_error: 0.0148 - val_loss: 4.2212e-05 - val_mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 00507: val_loss did not improve from 0.00002\n",
      "Epoch 508/800\n",
      "90/90 [==============================] - 38s 422ms/step - loss: 2.6039e-04 - mean_absolute_error: 0.0152 - val_loss: 4.1112e-05 - val_mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 00508: val_loss did not improve from 0.00002\n",
      "Epoch 509/800\n",
      "90/90 [==============================] - 35s 395ms/step - loss: 2.6923e-04 - mean_absolute_error: 0.0150 - val_loss: 2.6395e-05 - val_mean_absolute_error: 0.0057\n",
      "\n",
      "Epoch 00509: val_loss did not improve from 0.00002\n",
      "Epoch 510/800\n",
      "90/90 [==============================] - 36s 401ms/step - loss: 2.4885e-04 - mean_absolute_error: 0.0146 - val_loss: 3.5268e-05 - val_mean_absolute_error: 0.0047\n",
      "\n",
      "Epoch 00510: val_loss did not improve from 0.00002\n",
      "Epoch 511/800\n",
      "90/90 [==============================] - 37s 409ms/step - loss: 2.6439e-04 - mean_absolute_error: 0.0152 - val_loss: 4.3697e-05 - val_mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 00511: val_loss did not improve from 0.00002\n",
      "Epoch 512/800\n",
      "90/90 [==============================] - 37s 415ms/step - loss: 2.5776e-04 - mean_absolute_error: 0.0151 - val_loss: 2.0244e-05 - val_mean_absolute_error: 0.0045\n",
      "\n",
      "Epoch 00512: val_loss did not improve from 0.00002\n",
      "Epoch 513/800\n",
      "90/90 [==============================] - 38s 424ms/step - loss: 2.7527e-04 - mean_absolute_error: 0.0152 - val_loss: 6.8425e-05 - val_mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 00513: val_loss did not improve from 0.00002\n",
      "Epoch 514/800\n",
      "90/90 [==============================] - 36s 402ms/step - loss: 2.5474e-04 - mean_absolute_error: 0.0146 - val_loss: 1.8055e-05 - val_mean_absolute_error: 0.0038\n",
      "\n",
      "Epoch 00514: val_loss did not improve from 0.00002\n",
      "Epoch 515/800\n",
      "90/90 [==============================] - 38s 422ms/step - loss: 2.6502e-04 - mean_absolute_error: 0.0149 - val_loss: 2.5439e-05 - val_mean_absolute_error: 0.0045\n",
      "\n",
      "Epoch 00515: val_loss did not improve from 0.00002\n",
      "Epoch 516/800\n",
      "90/90 [==============================] - 36s 404ms/step - loss: 2.4647e-04 - mean_absolute_error: 0.0146 - val_loss: 1.9985e-05 - val_mean_absolute_error: 0.0044\n",
      "\n",
      "Epoch 00516: val_loss did not improve from 0.00002\n",
      "Epoch 517/800\n",
      "90/90 [==============================] - 35s 392ms/step - loss: 2.9186e-04 - mean_absolute_error: 0.0155 - val_loss: 2.5892e-05 - val_mean_absolute_error: 0.0057\n",
      "\n",
      "Epoch 00517: val_loss did not improve from 0.00002\n",
      "Epoch 518/800\n",
      "90/90 [==============================] - 38s 425ms/step - loss: 2.5441e-04 - mean_absolute_error: 0.0148 - val_loss: 3.4724e-05 - val_mean_absolute_error: 0.0061\n",
      "\n",
      "Epoch 00518: val_loss did not improve from 0.00002\n",
      "Epoch 519/800\n",
      "90/90 [==============================] - 36s 406ms/step - loss: 2.5750e-04 - mean_absolute_error: 0.0152 - val_loss: 2.7203e-05 - val_mean_absolute_error: 0.0045\n",
      "\n",
      "Epoch 00519: val_loss did not improve from 0.00002\n",
      "Epoch 520/800\n",
      "90/90 [==============================] - 35s 394ms/step - loss: 2.4701e-04 - mean_absolute_error: 0.0145 - val_loss: 2.8791e-05 - val_mean_absolute_error: 0.0055\n",
      "\n",
      "Epoch 00520: val_loss did not improve from 0.00002\n",
      "Epoch 521/800\n",
      "90/90 [==============================] - 38s 417ms/step - loss: 2.4138e-04 - mean_absolute_error: 0.0145 - val_loss: 2.7274e-05 - val_mean_absolute_error: 0.0059\n",
      "\n",
      "Epoch 00521: val_loss did not improve from 0.00002\n",
      "Epoch 522/800\n",
      "90/90 [==============================] - 36s 397ms/step - loss: 2.7331e-04 - mean_absolute_error: 0.0151 - val_loss: 4.1828e-05 - val_mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 00522: val_loss did not improve from 0.00002\n",
      "Epoch 523/800\n",
      "90/90 [==============================] - 39s 429ms/step - loss: 2.6703e-04 - mean_absolute_error: 0.0150 - val_loss: 2.6244e-05 - val_mean_absolute_error: 0.0048\n",
      "\n",
      "Epoch 00523: val_loss did not improve from 0.00002\n",
      "Epoch 524/800\n",
      "90/90 [==============================] - 37s 414ms/step - loss: 2.5328e-04 - mean_absolute_error: 0.0150 - val_loss: 2.7261e-05 - val_mean_absolute_error: 0.0050\n",
      "\n",
      "Epoch 00524: val_loss did not improve from 0.00002\n",
      "Epoch 525/800\n",
      "90/90 [==============================] - 38s 422ms/step - loss: 2.6888e-04 - mean_absolute_error: 0.0151 - val_loss: 1.7914e-05 - val_mean_absolute_error: 0.0042\n",
      "\n",
      "Epoch 00525: val_loss did not improve from 0.00002\n",
      "Epoch 526/800\n",
      "90/90 [==============================] - 37s 408ms/step - loss: 2.6518e-04 - mean_absolute_error: 0.0150 - val_loss: 3.6305e-05 - val_mean_absolute_error: 0.0059\n",
      "\n",
      "Epoch 00526: val_loss did not improve from 0.00002\n",
      "Epoch 527/800\n",
      "90/90 [==============================] - 36s 396ms/step - loss: 2.8157e-04 - mean_absolute_error: 0.0151 - val_loss: 2.8521e-05 - val_mean_absolute_error: 0.0058\n",
      "\n",
      "Epoch 00527: val_loss did not improve from 0.00002\n",
      "Epoch 528/800\n",
      "90/90 [==============================] - 37s 413ms/step - loss: 2.6207e-04 - mean_absolute_error: 0.0148 - val_loss: 2.3286e-05 - val_mean_absolute_error: 0.0047\n",
      "\n",
      "Epoch 00528: val_loss did not improve from 0.00002\n",
      "Epoch 529/800\n",
      "90/90 [==============================] - 36s 401ms/step - loss: 2.5245e-04 - mean_absolute_error: 0.0148 - val_loss: 1.8245e-05 - val_mean_absolute_error: 0.0039\n",
      "\n",
      "Epoch 00529: val_loss did not improve from 0.00002\n",
      "Epoch 530/800\n",
      "90/90 [==============================] - 38s 421ms/step - loss: 2.5274e-04 - mean_absolute_error: 0.0146 - val_loss: 2.1013e-05 - val_mean_absolute_error: 0.0046\n",
      "\n",
      "Epoch 00530: val_loss did not improve from 0.00002\n",
      "Epoch 531/800\n",
      "90/90 [==============================] - 38s 428ms/step - loss: 2.4915e-04 - mean_absolute_error: 0.0146 - val_loss: 2.8391e-05 - val_mean_absolute_error: 0.0049\n",
      "\n",
      "Epoch 00531: val_loss did not improve from 0.00002\n",
      "Epoch 532/800\n",
      "90/90 [==============================] - 37s 406ms/step - loss: 2.6319e-04 - mean_absolute_error: 0.0152 - val_loss: 1.6724e-05 - val_mean_absolute_error: 0.0037\n",
      "\n",
      "Epoch 00532: val_loss did not improve from 0.00002\n",
      "Epoch 533/800\n",
      "90/90 [==============================] - 37s 415ms/step - loss: 2.4316e-04 - mean_absolute_error: 0.0146 - val_loss: 2.2911e-05 - val_mean_absolute_error: 0.0050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00533: val_loss did not improve from 0.00002\n",
      "Epoch 534/800\n",
      "90/90 [==============================] - 37s 417ms/step - loss: 2.6382e-04 - mean_absolute_error: 0.0151 - val_loss: 2.9772e-05 - val_mean_absolute_error: 0.0050\n",
      "\n",
      "Epoch 00534: val_loss did not improve from 0.00002\n",
      "Epoch 535/800\n",
      "90/90 [==============================] - 38s 421ms/step - loss: 2.4673e-04 - mean_absolute_error: 0.0146 - val_loss: 2.0070e-05 - val_mean_absolute_error: 0.0046\n",
      "\n",
      "Epoch 00535: val_loss did not improve from 0.00002\n",
      "Epoch 536/800\n",
      "90/90 [==============================] - 39s 430ms/step - loss: 2.6314e-04 - mean_absolute_error: 0.0149 - val_loss: 3.5968e-05 - val_mean_absolute_error: 0.0060\n",
      "\n",
      "Epoch 00536: val_loss did not improve from 0.00002\n",
      "Epoch 537/800\n",
      "90/90 [==============================] - 36s 405ms/step - loss: 2.6212e-04 - mean_absolute_error: 0.0147 - val_loss: 4.1605e-05 - val_mean_absolute_error: 0.0052\n",
      "\n",
      "Epoch 00537: val_loss did not improve from 0.00002\n",
      "Epoch 538/800\n",
      "90/90 [==============================] - 37s 412ms/step - loss: 2.4923e-04 - mean_absolute_error: 0.0145 - val_loss: 2.3592e-05 - val_mean_absolute_error: 0.0051\n",
      "\n",
      "Epoch 00538: val_loss did not improve from 0.00002\n",
      "Epoch 539/800\n",
      "90/90 [==============================] - 37s 407ms/step - loss: 2.4401e-04 - mean_absolute_error: 0.0145 - val_loss: 2.9148e-05 - val_mean_absolute_error: 0.0058\n",
      "\n",
      "Epoch 00539: val_loss did not improve from 0.00002\n",
      "Epoch 540/800\n",
      "90/90 [==============================] - 35s 391ms/step - loss: 2.4054e-04 - mean_absolute_error: 0.0145 - val_loss: 1.7630e-05 - val_mean_absolute_error: 0.0041\n",
      "\n",
      "Epoch 00540: val_loss did not improve from 0.00002\n",
      "Epoch 541/800\n",
      "90/90 [==============================] - 38s 425ms/step - loss: 2.5664e-04 - mean_absolute_error: 0.0145 - val_loss: 3.1361e-05 - val_mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 00541: val_loss did not improve from 0.00002\n",
      "Epoch 542/800\n",
      "90/90 [==============================] - 36s 405ms/step - loss: 2.7643e-04 - mean_absolute_error: 0.0153 - val_loss: 3.8428e-05 - val_mean_absolute_error: 0.0057\n",
      "\n",
      "Epoch 00542: val_loss did not improve from 0.00002\n",
      "Epoch 543/800\n",
      "90/90 [==============================] - 36s 396ms/step - loss: 2.5091e-04 - mean_absolute_error: 0.0146 - val_loss: 3.1811e-05 - val_mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 00543: val_loss did not improve from 0.00002\n",
      "Epoch 544/800\n",
      "90/90 [==============================] - 37s 412ms/step - loss: 2.6996e-04 - mean_absolute_error: 0.0152 - val_loss: 6.5863e-05 - val_mean_absolute_error: 0.0091\n",
      "\n",
      "Epoch 00544: val_loss did not improve from 0.00002\n",
      "Epoch 545/800\n",
      "90/90 [==============================] - 37s 415ms/step - loss: 2.6857e-04 - mean_absolute_error: 0.0149 - val_loss: 1.8594e-05 - val_mean_absolute_error: 0.0041\n",
      "\n",
      "Epoch 00545: val_loss did not improve from 0.00002\n",
      "Epoch 546/800\n",
      "90/90 [==============================] - 38s 417ms/step - loss: 2.6297e-04 - mean_absolute_error: 0.0149 - val_loss: 3.9144e-05 - val_mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 00546: val_loss did not improve from 0.00002\n",
      "Epoch 547/800\n",
      "90/90 [==============================] - 37s 412ms/step - loss: 2.6193e-04 - mean_absolute_error: 0.0152 - val_loss: 3.9070e-05 - val_mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 00547: val_loss did not improve from 0.00002\n",
      "Epoch 548/800\n",
      "90/90 [==============================] - 37s 409ms/step - loss: 2.7156e-04 - mean_absolute_error: 0.0151 - val_loss: 1.7679e-05 - val_mean_absolute_error: 0.0039\n",
      "\n",
      "Epoch 00548: val_loss did not improve from 0.00002\n",
      "Epoch 549/800\n",
      "90/90 [==============================] - 39s 435ms/step - loss: 2.6622e-04 - mean_absolute_error: 0.0150 - val_loss: 2.0885e-05 - val_mean_absolute_error: 0.0039\n",
      "\n",
      "Epoch 00549: val_loss did not improve from 0.00002\n",
      "Epoch 550/800\n",
      "90/90 [==============================] - 38s 420ms/step - loss: 2.4923e-04 - mean_absolute_error: 0.0146 - val_loss: 2.6662e-05 - val_mean_absolute_error: 0.0055\n",
      "\n",
      "Epoch 00550: val_loss did not improve from 0.00002\n",
      "Epoch 551/800\n",
      "90/90 [==============================] - 37s 416ms/step - loss: 2.6052e-04 - mean_absolute_error: 0.0148 - val_loss: 2.9101e-05 - val_mean_absolute_error: 0.0058\n",
      "\n",
      "Epoch 00551: val_loss did not improve from 0.00002\n",
      "Epoch 552/800\n",
      "90/90 [==============================] - 37s 410ms/step - loss: 2.5835e-04 - mean_absolute_error: 0.0148 - val_loss: 2.4238e-05 - val_mean_absolute_error: 0.0049\n",
      "\n",
      "Epoch 00552: val_loss did not improve from 0.00002\n",
      "Epoch 553/800\n",
      "90/90 [==============================] - 39s 436ms/step - loss: 2.4711e-04 - mean_absolute_error: 0.0145 - val_loss: 6.8366e-05 - val_mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 00553: val_loss did not improve from 0.00002\n",
      "Epoch 554/800\n",
      "90/90 [==============================] - 40s 445ms/step - loss: 2.4691e-04 - mean_absolute_error: 0.0145 - val_loss: 2.2823e-05 - val_mean_absolute_error: 0.0045\n",
      "\n",
      "Epoch 00554: val_loss did not improve from 0.00002\n",
      "Epoch 555/800\n",
      "90/90 [==============================] - 38s 425ms/step - loss: 2.4001e-04 - mean_absolute_error: 0.0144 - val_loss: 2.3082e-05 - val_mean_absolute_error: 0.0047\n",
      "\n",
      "Epoch 00555: val_loss did not improve from 0.00002\n",
      "Epoch 556/800\n",
      "90/90 [==============================] - 38s 418ms/step - loss: 2.5787e-04 - mean_absolute_error: 0.0149 - val_loss: 1.8772e-05 - val_mean_absolute_error: 0.0039\n",
      "\n",
      "Epoch 00556: val_loss did not improve from 0.00002\n",
      "Epoch 557/800\n",
      "90/90 [==============================] - 37s 414ms/step - loss: 2.6868e-04 - mean_absolute_error: 0.0151 - val_loss: 3.6887e-05 - val_mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 00557: val_loss did not improve from 0.00002\n",
      "Epoch 558/800\n",
      "90/90 [==============================] - 37s 414ms/step - loss: 2.5320e-04 - mean_absolute_error: 0.0147 - val_loss: 6.6537e-05 - val_mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 00558: val_loss did not improve from 0.00002\n",
      "Epoch 559/800\n",
      "90/90 [==============================] - 38s 422ms/step - loss: 2.6499e-04 - mean_absolute_error: 0.0149 - val_loss: 3.6202e-05 - val_mean_absolute_error: 0.0057\n",
      "\n",
      "Epoch 00559: val_loss did not improve from 0.00002\n",
      "Epoch 560/800\n",
      "90/90 [==============================] - 38s 420ms/step - loss: 2.8105e-04 - mean_absolute_error: 0.0155 - val_loss: 1.8530e-05 - val_mean_absolute_error: 0.0045\n",
      "\n",
      "Epoch 00560: val_loss did not improve from 0.00002\n",
      "Epoch 561/800\n",
      "90/90 [==============================] - 37s 415ms/step - loss: 2.4650e-04 - mean_absolute_error: 0.0145 - val_loss: 1.1567e-04 - val_mean_absolute_error: 0.0104\n",
      "\n",
      "Epoch 00561: val_loss did not improve from 0.00002\n",
      "Epoch 562/800\n",
      "90/90 [==============================] - 38s 422ms/step - loss: 2.7172e-04 - mean_absolute_error: 0.0152 - val_loss: 4.5790e-05 - val_mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 00562: val_loss did not improve from 0.00002\n",
      "Epoch 563/800\n",
      "90/90 [==============================] - 36s 401ms/step - loss: 2.4407e-04 - mean_absolute_error: 0.0145 - val_loss: 7.3711e-05 - val_mean_absolute_error: 0.0091\n",
      "\n",
      "Epoch 00563: val_loss did not improve from 0.00002\n",
      "Epoch 564/800\n",
      "90/90 [==============================] - 38s 418ms/step - loss: 2.5876e-04 - mean_absolute_error: 0.0148 - val_loss: 4.0735e-05 - val_mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 00564: val_loss did not improve from 0.00002\n",
      "Epoch 565/800\n",
      "90/90 [==============================] - 38s 420ms/step - loss: 2.7720e-04 - mean_absolute_error: 0.0155 - val_loss: 1.4798e-05 - val_mean_absolute_error: 0.0037\n",
      "\n",
      "Epoch 00565: val_loss improved from 0.00002 to 0.00001, saving model to results\\2021-09-07_SPY-shuffle-1-scale-1-split_date-0-epochs-800.h5\n",
      "Epoch 566/800\n",
      "90/90 [==============================] - 36s 400ms/step - loss: 2.5124e-04 - mean_absolute_error: 0.0148 - val_loss: 4.0873e-05 - val_mean_absolute_error: 0.0061\n",
      "\n",
      "Epoch 00566: val_loss did not improve from 0.00001\n",
      "Epoch 567/800\n",
      "90/90 [==============================] - 38s 426ms/step - loss: 2.5638e-04 - mean_absolute_error: 0.0149 - val_loss: 1.9590e-05 - val_mean_absolute_error: 0.0039\n",
      "\n",
      "Epoch 00567: val_loss did not improve from 0.00001\n",
      "Epoch 568/800\n",
      "90/90 [==============================] - 37s 406ms/step - loss: 2.5224e-04 - mean_absolute_error: 0.0146 - val_loss: 2.9798e-05 - val_mean_absolute_error: 0.0058\n",
      "\n",
      "Epoch 00568: val_loss did not improve from 0.00001\n",
      "Epoch 569/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 35s 391ms/step - loss: 2.4882e-04 - mean_absolute_error: 0.0148 - val_loss: 5.4997e-05 - val_mean_absolute_error: 0.0081\n",
      "\n",
      "Epoch 00569: val_loss did not improve from 0.00001\n",
      "Epoch 570/800\n",
      "90/90 [==============================] - 37s 408ms/step - loss: 2.7752e-04 - mean_absolute_error: 0.0151 - val_loss: 3.7548e-05 - val_mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 00570: val_loss did not improve from 0.00001\n",
      "Epoch 571/800\n",
      "90/90 [==============================] - 37s 414ms/step - loss: 2.4910e-04 - mean_absolute_error: 0.0144 - val_loss: 1.9271e-05 - val_mean_absolute_error: 0.0043\n",
      "\n",
      "Epoch 00571: val_loss did not improve from 0.00001\n",
      "Epoch 572/800\n",
      "90/90 [==============================] - 38s 423ms/step - loss: 2.4819e-04 - mean_absolute_error: 0.0145 - val_loss: 7.0220e-05 - val_mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 00572: val_loss did not improve from 0.00001\n",
      "Epoch 573/800\n",
      "90/90 [==============================] - 38s 425ms/step - loss: 2.4500e-04 - mean_absolute_error: 0.0148 - val_loss: 3.0327e-05 - val_mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 00573: val_loss did not improve from 0.00001\n",
      "Epoch 574/800\n",
      "90/90 [==============================] - 36s 404ms/step - loss: 2.5553e-04 - mean_absolute_error: 0.0146 - val_loss: 2.0660e-05 - val_mean_absolute_error: 0.0046\n",
      "\n",
      "Epoch 00574: val_loss did not improve from 0.00001\n",
      "Epoch 575/800\n",
      "90/90 [==============================] - 39s 431ms/step - loss: 2.5522e-04 - mean_absolute_error: 0.0147 - val_loss: 2.2384e-05 - val_mean_absolute_error: 0.0044\n",
      "\n",
      "Epoch 00575: val_loss did not improve from 0.00001\n",
      "Epoch 576/800\n",
      "90/90 [==============================] - 37s 407ms/step - loss: 2.6322e-04 - mean_absolute_error: 0.0151 - val_loss: 2.5168e-05 - val_mean_absolute_error: 0.0053\n",
      "\n",
      "Epoch 00576: val_loss did not improve from 0.00001\n",
      "Epoch 577/800\n",
      "90/90 [==============================] - 38s 420ms/step - loss: 2.4959e-04 - mean_absolute_error: 0.0147 - val_loss: 8.2086e-05 - val_mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 00577: val_loss did not improve from 0.00001\n",
      "Epoch 578/800\n",
      "90/90 [==============================] - 37s 411ms/step - loss: 2.7273e-04 - mean_absolute_error: 0.0150 - val_loss: 4.7378e-05 - val_mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 00578: val_loss did not improve from 0.00001\n",
      "Epoch 579/800\n",
      "90/90 [==============================] - 36s 396ms/step - loss: 2.6763e-04 - mean_absolute_error: 0.0149 - val_loss: 2.6285e-05 - val_mean_absolute_error: 0.0052\n",
      "\n",
      "Epoch 00579: val_loss did not improve from 0.00001\n",
      "Epoch 580/800\n",
      "90/90 [==============================] - 38s 420ms/step - loss: 2.5637e-04 - mean_absolute_error: 0.0146 - val_loss: 3.2837e-05 - val_mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 00580: val_loss did not improve from 0.00001\n",
      "Epoch 581/800\n",
      "90/90 [==============================] - 36s 403ms/step - loss: 2.6931e-04 - mean_absolute_error: 0.0152 - val_loss: 1.7418e-05 - val_mean_absolute_error: 0.0041\n",
      "\n",
      "Epoch 00581: val_loss did not improve from 0.00001\n",
      "Epoch 582/800\n",
      "90/90 [==============================] - 37s 412ms/step - loss: 2.5486e-04 - mean_absolute_error: 0.0145 - val_loss: 1.8698e-05 - val_mean_absolute_error: 0.0041\n",
      "\n",
      "Epoch 00582: val_loss did not improve from 0.00001\n",
      "Epoch 583/800\n",
      "90/90 [==============================] - 38s 422ms/step - loss: 2.5964e-04 - mean_absolute_error: 0.0150 - val_loss: 2.4884e-05 - val_mean_absolute_error: 0.0056\n",
      "\n",
      "Epoch 00583: val_loss did not improve from 0.00001\n",
      "Epoch 584/800\n",
      "90/90 [==============================] - 36s 395ms/step - loss: 2.6909e-04 - mean_absolute_error: 0.0150 - val_loss: 2.5450e-05 - val_mean_absolute_error: 0.0050\n",
      "\n",
      "Epoch 00584: val_loss did not improve from 0.00001\n",
      "Epoch 585/800\n",
      "90/90 [==============================] - 36s 404ms/step - loss: 2.7786e-04 - mean_absolute_error: 0.0153 - val_loss: 8.0894e-05 - val_mean_absolute_error: 0.0096\n",
      "\n",
      "Epoch 00585: val_loss did not improve from 0.00001\n",
      "Epoch 586/800\n",
      "90/90 [==============================] - 37s 411ms/step - loss: 2.5629e-04 - mean_absolute_error: 0.0150 - val_loss: 2.0694e-05 - val_mean_absolute_error: 0.0046\n",
      "\n",
      "Epoch 00586: val_loss did not improve from 0.00001\n",
      "Epoch 587/800\n",
      "90/90 [==============================] - 37s 414ms/step - loss: 2.6653e-04 - mean_absolute_error: 0.0149 - val_loss: 3.1100e-05 - val_mean_absolute_error: 0.0056\n",
      "\n",
      "Epoch 00587: val_loss did not improve from 0.00001\n",
      "Epoch 588/800\n",
      "90/90 [==============================] - 39s 431ms/step - loss: 2.6175e-04 - mean_absolute_error: 0.0149 - val_loss: 2.6274e-05 - val_mean_absolute_error: 0.0059\n",
      "\n",
      "Epoch 00588: val_loss did not improve from 0.00001\n",
      "Epoch 589/800\n",
      "90/90 [==============================] - 37s 404ms/step - loss: 2.5922e-04 - mean_absolute_error: 0.0146 - val_loss: 3.6158e-05 - val_mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 00589: val_loss did not improve from 0.00001\n",
      "Epoch 590/800\n",
      "90/90 [==============================] - 36s 394ms/step - loss: 2.7716e-04 - mean_absolute_error: 0.0151 - val_loss: 2.6783e-05 - val_mean_absolute_error: 0.0053\n",
      "\n",
      "Epoch 00590: val_loss did not improve from 0.00001\n",
      "Epoch 591/800\n",
      "90/90 [==============================] - 37s 415ms/step - loss: 2.5442e-04 - mean_absolute_error: 0.0148 - val_loss: 3.0389e-05 - val_mean_absolute_error: 0.0056\n",
      "\n",
      "Epoch 00591: val_loss did not improve from 0.00001\n",
      "Epoch 592/800\n",
      "90/90 [==============================] - 36s 403ms/step - loss: 2.6046e-04 - mean_absolute_error: 0.0150 - val_loss: 2.1379e-05 - val_mean_absolute_error: 0.0041\n",
      "\n",
      "Epoch 00592: val_loss did not improve from 0.00001\n",
      "Epoch 593/800\n",
      "90/90 [==============================] - 38s 428ms/step - loss: 2.6429e-04 - mean_absolute_error: 0.0152 - val_loss: 2.7818e-05 - val_mean_absolute_error: 0.0054\n",
      "\n",
      "Epoch 00593: val_loss did not improve from 0.00001\n",
      "Epoch 594/800\n",
      "90/90 [==============================] - 36s 405ms/step - loss: 2.5207e-04 - mean_absolute_error: 0.0145 - val_loss: 2.3191e-05 - val_mean_absolute_error: 0.0047\n",
      "\n",
      "Epoch 00594: val_loss did not improve from 0.00001\n",
      "Epoch 595/800\n",
      "90/90 [==============================] - 38s 423ms/step - loss: 2.3831e-04 - mean_absolute_error: 0.0144 - val_loss: 1.9017e-05 - val_mean_absolute_error: 0.0041\n",
      "\n",
      "Epoch 00595: val_loss did not improve from 0.00001\n",
      "Epoch 596/800\n",
      "90/90 [==============================] - 37s 408ms/step - loss: 2.3232e-04 - mean_absolute_error: 0.0142 - val_loss: 1.7313e-05 - val_mean_absolute_error: 0.0038\n",
      "\n",
      "Epoch 00596: val_loss did not improve from 0.00001\n",
      "Epoch 597/800\n",
      "90/90 [==============================] - 36s 405ms/step - loss: 2.5447e-04 - mean_absolute_error: 0.0147 - val_loss: 2.8785e-05 - val_mean_absolute_error: 0.0057\n",
      "\n",
      "Epoch 00597: val_loss did not improve from 0.00001\n",
      "Epoch 598/800\n",
      "90/90 [==============================] - 37s 413ms/step - loss: 2.5732e-04 - mean_absolute_error: 0.0145 - val_loss: 3.3431e-05 - val_mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 00598: val_loss did not improve from 0.00001\n",
      "Epoch 599/800\n",
      "90/90 [==============================] - 37s 408ms/step - loss: 2.6420e-04 - mean_absolute_error: 0.0151 - val_loss: 1.7903e-05 - val_mean_absolute_error: 0.0040\n",
      "\n",
      "Epoch 00599: val_loss did not improve from 0.00001\n",
      "Epoch 600/800\n",
      "90/90 [==============================] - 35s 393ms/step - loss: 2.5492e-04 - mean_absolute_error: 0.0148 - val_loss: 6.4865e-05 - val_mean_absolute_error: 0.0081\n",
      "\n",
      "Epoch 00600: val_loss did not improve from 0.00001\n",
      "Epoch 601/800\n",
      "90/90 [==============================] - 38s 421ms/step - loss: 2.5153e-04 - mean_absolute_error: 0.0148 - val_loss: 5.1360e-05 - val_mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 00601: val_loss did not improve from 0.00001\n",
      "Epoch 602/800\n",
      "90/90 [==============================] - 36s 403ms/step - loss: 2.5146e-04 - mean_absolute_error: 0.0148 - val_loss: 2.6013e-05 - val_mean_absolute_error: 0.0054\n",
      "\n",
      "Epoch 00602: val_loss did not improve from 0.00001\n",
      "Epoch 603/800\n",
      "90/90 [==============================] - 38s 421ms/step - loss: 2.6229e-04 - mean_absolute_error: 0.0146 - val_loss: 5.8838e-05 - val_mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 00603: val_loss did not improve from 0.00001\n",
      "Epoch 604/800\n",
      "90/90 [==============================] - 37s 415ms/step - loss: 2.4956e-04 - mean_absolute_error: 0.0148 - val_loss: 1.4439e-05 - val_mean_absolute_error: 0.0033\n",
      "\n",
      "Epoch 00604: val_loss improved from 0.00001 to 0.00001, saving model to results\\2021-09-07_SPY-shuffle-1-scale-1-split_date-0-epochs-800.h5\n",
      "Epoch 605/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 37s 412ms/step - loss: 2.3963e-04 - mean_absolute_error: 0.0143 - val_loss: 2.7893e-05 - val_mean_absolute_error: 0.0054\n",
      "\n",
      "Epoch 00605: val_loss did not improve from 0.00001\n",
      "Epoch 606/800\n",
      "90/90 [==============================] - 36s 406ms/step - loss: 2.5286e-04 - mean_absolute_error: 0.0148 - val_loss: 2.1948e-05 - val_mean_absolute_error: 0.0047\n",
      "\n",
      "Epoch 00606: val_loss did not improve from 0.00001\n",
      "Epoch 607/800\n",
      "90/90 [==============================] - 38s 424ms/step - loss: 2.4607e-04 - mean_absolute_error: 0.0147 - val_loss: 2.3703e-05 - val_mean_absolute_error: 0.0040\n",
      "\n",
      "Epoch 00607: val_loss did not improve from 0.00001\n",
      "Epoch 608/800\n",
      "90/90 [==============================] - 37s 413ms/step - loss: 2.5949e-04 - mean_absolute_error: 0.0147 - val_loss: 1.7516e-05 - val_mean_absolute_error: 0.0043\n",
      "\n",
      "Epoch 00608: val_loss did not improve from 0.00001\n",
      "Epoch 609/800\n",
      "90/90 [==============================] - 37s 407ms/step - loss: 2.6049e-04 - mean_absolute_error: 0.0149 - val_loss: 3.4941e-05 - val_mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 00609: val_loss did not improve from 0.00001\n",
      "Epoch 610/800\n",
      "90/90 [==============================] - 35s 392ms/step - loss: 2.5031e-04 - mean_absolute_error: 0.0147 - val_loss: 1.3677e-05 - val_mean_absolute_error: 0.0033\n",
      "\n",
      "Epoch 00610: val_loss improved from 0.00001 to 0.00001, saving model to results\\2021-09-07_SPY-shuffle-1-scale-1-split_date-0-epochs-800.h5\n",
      "Epoch 611/800\n",
      "90/90 [==============================] - 38s 422ms/step - loss: 2.6936e-04 - mean_absolute_error: 0.0150 - val_loss: 7.8447e-05 - val_mean_absolute_error: 0.0088\n",
      "\n",
      "Epoch 00611: val_loss did not improve from 0.00001\n",
      "Epoch 612/800\n",
      "90/90 [==============================] - 36s 406ms/step - loss: 2.5991e-04 - mean_absolute_error: 0.0151 - val_loss: 1.6772e-05 - val_mean_absolute_error: 0.0040\n",
      "\n",
      "Epoch 00612: val_loss did not improve from 0.00001\n",
      "Epoch 613/800\n",
      "90/90 [==============================] - 36s 403ms/step - loss: 2.6997e-04 - mean_absolute_error: 0.0150 - val_loss: 1.7279e-05 - val_mean_absolute_error: 0.0042\n",
      "\n",
      "Epoch 00613: val_loss did not improve from 0.00001\n",
      "Epoch 614/800\n",
      "90/90 [==============================] - 37s 408ms/step - loss: 2.4360e-04 - mean_absolute_error: 0.0147 - val_loss: 2.5383e-05 - val_mean_absolute_error: 0.0043\n",
      "\n",
      "Epoch 00614: val_loss did not improve from 0.00001\n",
      "Epoch 615/800\n",
      "90/90 [==============================] - 35s 392ms/step - loss: 2.6976e-04 - mean_absolute_error: 0.0149 - val_loss: 2.0822e-05 - val_mean_absolute_error: 0.0041\n",
      "\n",
      "Epoch 00615: val_loss did not improve from 0.00001\n",
      "Epoch 616/800\n",
      "90/90 [==============================] - 38s 426ms/step - loss: 2.6028e-04 - mean_absolute_error: 0.0149 - val_loss: 1.8840e-05 - val_mean_absolute_error: 0.0037\n",
      "\n",
      "Epoch 00616: val_loss did not improve from 0.00001\n",
      "Epoch 617/800\n",
      "90/90 [==============================] - 37s 407ms/step - loss: 2.6282e-04 - mean_absolute_error: 0.0146 - val_loss: 4.2144e-05 - val_mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 00617: val_loss did not improve from 0.00001\n",
      "Epoch 618/800\n",
      "90/90 [==============================] - 38s 423ms/step - loss: 2.6977e-04 - mean_absolute_error: 0.0153 - val_loss: 3.6230e-05 - val_mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 00618: val_loss did not improve from 0.00001\n",
      "Epoch 619/800\n",
      "90/90 [==============================] - 37s 410ms/step - loss: 2.6055e-04 - mean_absolute_error: 0.0148 - val_loss: 2.4097e-05 - val_mean_absolute_error: 0.0054\n",
      "\n",
      "Epoch 00619: val_loss did not improve from 0.00001\n",
      "Epoch 620/800\n",
      "90/90 [==============================] - 35s 393ms/step - loss: 2.5234e-04 - mean_absolute_error: 0.0148 - val_loss: 2.8461e-05 - val_mean_absolute_error: 0.0057\n",
      "\n",
      "Epoch 00620: val_loss did not improve from 0.00001\n",
      "Epoch 621/800\n",
      "90/90 [==============================] - 37s 407ms/step - loss: 2.6832e-04 - mean_absolute_error: 0.0146 - val_loss: 2.2769e-05 - val_mean_absolute_error: 0.0049\n",
      "\n",
      "Epoch 00621: val_loss did not improve from 0.00001\n",
      "Epoch 622/800\n",
      "90/90 [==============================] - 36s 404ms/step - loss: 2.4734e-04 - mean_absolute_error: 0.0143 - val_loss: 7.0931e-05 - val_mean_absolute_error: 0.0098\n",
      "\n",
      "Epoch 00622: val_loss did not improve from 0.00001\n",
      "Epoch 623/800\n",
      "90/90 [==============================] - 38s 424ms/step - loss: 2.2875e-04 - mean_absolute_error: 0.0142 - val_loss: 1.7514e-05 - val_mean_absolute_error: 0.0039\n",
      "\n",
      "Epoch 00623: val_loss did not improve from 0.00001\n",
      "Epoch 624/800\n",
      "90/90 [==============================] - 37s 408ms/step - loss: 2.5984e-04 - mean_absolute_error: 0.0147 - val_loss: 4.4430e-05 - val_mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 00624: val_loss did not improve from 0.00001\n",
      "Epoch 625/800\n",
      "90/90 [==============================] - 36s 400ms/step - loss: 2.5996e-04 - mean_absolute_error: 0.0148 - val_loss: 2.6798e-05 - val_mean_absolute_error: 0.0047\n",
      "\n",
      "Epoch 00625: val_loss did not improve from 0.00001\n",
      "Epoch 626/800\n",
      "90/90 [==============================] - 38s 417ms/step - loss: 2.4468e-04 - mean_absolute_error: 0.0146 - val_loss: 2.1500e-05 - val_mean_absolute_error: 0.0050\n",
      "\n",
      "Epoch 00626: val_loss did not improve from 0.00001\n",
      "Epoch 627/800\n",
      "90/90 [==============================] - 36s 405ms/step - loss: 2.4074e-04 - mean_absolute_error: 0.0144 - val_loss: 2.9546e-05 - val_mean_absolute_error: 0.0056\n",
      "\n",
      "Epoch 00627: val_loss did not improve from 0.00001\n",
      "Epoch 628/800\n",
      "90/90 [==============================] - 35s 393ms/step - loss: 2.5154e-04 - mean_absolute_error: 0.0147 - val_loss: 2.3073e-05 - val_mean_absolute_error: 0.0046\n",
      "\n",
      "Epoch 00628: val_loss did not improve from 0.00001\n",
      "Epoch 629/800\n",
      "90/90 [==============================] - 36s 405ms/step - loss: 2.6253e-04 - mean_absolute_error: 0.0149 - val_loss: 2.9819e-05 - val_mean_absolute_error: 0.0053\n",
      "\n",
      "Epoch 00629: val_loss did not improve from 0.00001\n",
      "Epoch 630/800\n",
      "90/90 [==============================] - 38s 421ms/step - loss: 2.5774e-04 - mean_absolute_error: 0.0149 - val_loss: 2.5152e-05 - val_mean_absolute_error: 0.0050\n",
      "\n",
      "Epoch 00630: val_loss did not improve from 0.00001\n",
      "Epoch 631/800\n",
      "90/90 [==============================] - 39s 439ms/step - loss: 2.5086e-04 - mean_absolute_error: 0.0148 - val_loss: 1.6001e-05 - val_mean_absolute_error: 0.0035\n",
      "\n",
      "Epoch 00631: val_loss did not improve from 0.00001\n",
      "Epoch 632/800\n",
      "90/90 [==============================] - 38s 421ms/step - loss: 2.5993e-04 - mean_absolute_error: 0.0148 - val_loss: 2.9918e-05 - val_mean_absolute_error: 0.0061\n",
      "\n",
      "Epoch 00632: val_loss did not improve from 0.00001\n",
      "Epoch 633/800\n",
      "90/90 [==============================] - 36s 406ms/step - loss: 2.4207e-04 - mean_absolute_error: 0.0145 - val_loss: 4.8029e-05 - val_mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 00633: val_loss did not improve from 0.00001\n",
      "Epoch 634/800\n",
      "90/90 [==============================] - 37s 411ms/step - loss: 2.5122e-04 - mean_absolute_error: 0.0147 - val_loss: 1.7833e-05 - val_mean_absolute_error: 0.0041\n",
      "\n",
      "Epoch 00634: val_loss did not improve from 0.00001\n",
      "Epoch 635/800\n",
      "90/90 [==============================] - 37s 406ms/step - loss: 2.5530e-04 - mean_absolute_error: 0.0147 - val_loss: 4.1321e-05 - val_mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 00635: val_loss did not improve from 0.00001\n",
      "Epoch 636/800\n",
      "90/90 [==============================] - 38s 427ms/step - loss: 2.3156e-04 - mean_absolute_error: 0.0141 - val_loss: 1.9299e-05 - val_mean_absolute_error: 0.0041\n",
      "\n",
      "Epoch 00636: val_loss did not improve from 0.00001\n",
      "Epoch 637/800\n",
      "90/90 [==============================] - 39s 436ms/step - loss: 2.6742e-04 - mean_absolute_error: 0.0149 - val_loss: 3.9516e-05 - val_mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 00637: val_loss did not improve from 0.00001\n",
      "Epoch 638/800\n",
      "90/90 [==============================] - 39s 432ms/step - loss: 2.5794e-04 - mean_absolute_error: 0.0148 - val_loss: 2.2945e-05 - val_mean_absolute_error: 0.0050\n",
      "\n",
      "Epoch 00638: val_loss did not improve from 0.00001\n",
      "Epoch 639/800\n",
      "90/90 [==============================] - 38s 418ms/step - loss: 2.5446e-04 - mean_absolute_error: 0.0146 - val_loss: 4.5591e-05 - val_mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 00639: val_loss did not improve from 0.00001\n",
      "Epoch 640/800\n",
      "90/90 [==============================] - 37s 411ms/step - loss: 2.5563e-04 - mean_absolute_error: 0.0148 - val_loss: 2.0416e-05 - val_mean_absolute_error: 0.0043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00640: val_loss did not improve from 0.00001\n",
      "Epoch 641/800\n",
      "90/90 [==============================] - 36s 396ms/step - loss: 2.4305e-04 - mean_absolute_error: 0.0147 - val_loss: 3.3877e-05 - val_mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 00641: val_loss did not improve from 0.00001\n",
      "Epoch 642/800\n",
      "90/90 [==============================] - 38s 423ms/step - loss: 2.4809e-04 - mean_absolute_error: 0.0146 - val_loss: 4.8941e-05 - val_mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 00642: val_loss did not improve from 0.00001\n",
      "Epoch 643/800\n",
      "90/90 [==============================] - 37s 413ms/step - loss: 2.7118e-04 - mean_absolute_error: 0.0150 - val_loss: 2.7354e-05 - val_mean_absolute_error: 0.0055\n",
      "\n",
      "Epoch 00643: val_loss did not improve from 0.00001\n",
      "Epoch 644/800\n",
      "90/90 [==============================] - 38s 427ms/step - loss: 2.5773e-04 - mean_absolute_error: 0.0148 - val_loss: 2.0339e-05 - val_mean_absolute_error: 0.0045\n",
      "\n",
      "Epoch 00644: val_loss did not improve from 0.00001\n",
      "Epoch 645/800\n",
      "90/90 [==============================] - 37s 410ms/step - loss: 2.4423e-04 - mean_absolute_error: 0.0144 - val_loss: 1.9841e-05 - val_mean_absolute_error: 0.0044\n",
      "\n",
      "Epoch 00645: val_loss did not improve from 0.00001\n",
      "Epoch 646/800\n",
      "90/90 [==============================] - 38s 417ms/step - loss: 2.3143e-04 - mean_absolute_error: 0.0143 - val_loss: 2.0019e-05 - val_mean_absolute_error: 0.0041\n",
      "\n",
      "Epoch 00646: val_loss did not improve from 0.00001\n",
      "Epoch 647/800\n",
      "90/90 [==============================] - 39s 429ms/step - loss: 2.5650e-04 - mean_absolute_error: 0.0151 - val_loss: 1.7564e-05 - val_mean_absolute_error: 0.0042\n",
      "\n",
      "Epoch 00647: val_loss did not improve from 0.00001\n",
      "Epoch 648/800\n",
      "90/90 [==============================] - 37s 415ms/step - loss: 2.4493e-04 - mean_absolute_error: 0.0145 - val_loss: 1.5644e-05 - val_mean_absolute_error: 0.0037\n",
      "\n",
      "Epoch 00648: val_loss did not improve from 0.00001\n",
      "Epoch 649/800\n",
      "90/90 [==============================] - 36s 398ms/step - loss: 2.5527e-04 - mean_absolute_error: 0.0146 - val_loss: 3.4218e-05 - val_mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 00649: val_loss did not improve from 0.00001\n",
      "Epoch 650/800\n",
      "90/90 [==============================] - 37s 411ms/step - loss: 2.6065e-04 - mean_absolute_error: 0.0149 - val_loss: 6.2105e-05 - val_mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 00650: val_loss did not improve from 0.00001\n",
      "Epoch 651/800\n",
      "90/90 [==============================] - 37s 406ms/step - loss: 2.5976e-04 - mean_absolute_error: 0.0149 - val_loss: 1.5956e-05 - val_mean_absolute_error: 0.0038\n",
      "\n",
      "Epoch 00651: val_loss did not improve from 0.00001\n",
      "Epoch 652/800\n",
      "90/90 [==============================] - 38s 424ms/step - loss: 2.7561e-04 - mean_absolute_error: 0.0148 - val_loss: 2.4865e-05 - val_mean_absolute_error: 0.0051\n",
      "\n",
      "Epoch 00652: val_loss did not improve from 0.00001\n",
      "Epoch 653/800\n",
      "90/90 [==============================] - 36s 404ms/step - loss: 2.5545e-04 - mean_absolute_error: 0.0149 - val_loss: 3.7077e-05 - val_mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 00653: val_loss did not improve from 0.00001\n",
      "Epoch 654/800\n",
      "90/90 [==============================] - 37s 413ms/step - loss: 2.3408e-04 - mean_absolute_error: 0.0142 - val_loss: 1.9644e-05 - val_mean_absolute_error: 0.0044\n",
      "\n",
      "Epoch 00654: val_loss did not improve from 0.00001\n",
      "Epoch 655/800\n",
      "90/90 [==============================] - 37s 410ms/step - loss: 2.4194e-04 - mean_absolute_error: 0.0145 - val_loss: 2.1741e-05 - val_mean_absolute_error: 0.0046\n",
      "\n",
      "Epoch 00655: val_loss did not improve from 0.00001\n",
      "Epoch 656/800\n",
      "90/90 [==============================] - 36s 396ms/step - loss: 2.6312e-04 - mean_absolute_error: 0.0147 - val_loss: 4.8273e-05 - val_mean_absolute_error: 0.0057\n",
      "\n",
      "Epoch 00656: val_loss did not improve from 0.00001\n",
      "Epoch 657/800\n",
      "90/90 [==============================] - 39s 430ms/step - loss: 2.7641e-04 - mean_absolute_error: 0.0153 - val_loss: 2.5155e-05 - val_mean_absolute_error: 0.0044\n",
      "\n",
      "Epoch 00657: val_loss did not improve from 0.00001\n",
      "Epoch 658/800\n",
      "90/90 [==============================] - 36s 404ms/step - loss: 2.5331e-04 - mean_absolute_error: 0.0146 - val_loss: 5.3552e-05 - val_mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 00658: val_loss did not improve from 0.00001\n",
      "Epoch 659/800\n",
      "90/90 [==============================] - 38s 426ms/step - loss: 2.3323e-04 - mean_absolute_error: 0.0141 - val_loss: 2.7336e-05 - val_mean_absolute_error: 0.0057\n",
      "\n",
      "Epoch 00659: val_loss did not improve from 0.00001\n",
      "Epoch 660/800\n",
      "90/90 [==============================] - 38s 427ms/step - loss: 2.2742e-04 - mean_absolute_error: 0.0139 - val_loss: 2.3835e-05 - val_mean_absolute_error: 0.0045\n",
      "\n",
      "Epoch 00660: val_loss did not improve from 0.00001\n",
      "Epoch 661/800\n",
      "90/90 [==============================] - 36s 400ms/step - loss: 2.4940e-04 - mean_absolute_error: 0.0142 - val_loss: 3.7234e-05 - val_mean_absolute_error: 0.0049\n",
      "\n",
      "Epoch 00661: val_loss did not improve from 0.00001\n",
      "Epoch 662/800\n",
      "90/90 [==============================] - 37s 410ms/step - loss: 2.4405e-04 - mean_absolute_error: 0.0143 - val_loss: 1.7537e-05 - val_mean_absolute_error: 0.0040\n",
      "\n",
      "Epoch 00662: val_loss did not improve from 0.00001\n",
      "Epoch 663/800\n",
      "90/90 [==============================] - 37s 415ms/step - loss: 2.6494e-04 - mean_absolute_error: 0.0149 - val_loss: 2.4350e-05 - val_mean_absolute_error: 0.0047\n",
      "\n",
      "Epoch 00663: val_loss did not improve from 0.00001\n",
      "Epoch 664/800\n",
      "90/90 [==============================] - 37s 409ms/step - loss: 2.4542e-04 - mean_absolute_error: 0.0146 - val_loss: 4.4127e-05 - val_mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 00664: val_loss did not improve from 0.00001\n",
      "Epoch 665/800\n",
      "90/90 [==============================] - 38s 428ms/step - loss: 2.3982e-04 - mean_absolute_error: 0.0143 - val_loss: 9.6783e-05 - val_mean_absolute_error: 0.0099\n",
      "\n",
      "Epoch 00665: val_loss did not improve from 0.00001\n",
      "Epoch 666/800\n",
      "90/90 [==============================] - 38s 418ms/step - loss: 2.5590e-04 - mean_absolute_error: 0.0147 - val_loss: 1.6881e-05 - val_mean_absolute_error: 0.0038\n",
      "\n",
      "Epoch 00666: val_loss did not improve from 0.00001\n",
      "Epoch 667/800\n",
      "90/90 [==============================] - 39s 430ms/step - loss: 2.4513e-04 - mean_absolute_error: 0.0143 - val_loss: 2.1436e-05 - val_mean_absolute_error: 0.0045\n",
      "\n",
      "Epoch 00667: val_loss did not improve from 0.00001\n",
      "Epoch 668/800\n",
      "90/90 [==============================] - 37s 407ms/step - loss: 2.5357e-04 - mean_absolute_error: 0.0147 - val_loss: 2.4366e-05 - val_mean_absolute_error: 0.0047\n",
      "\n",
      "Epoch 00668: val_loss did not improve from 0.00001\n",
      "Epoch 669/800\n",
      "90/90 [==============================] - 37s 409ms/step - loss: 2.5798e-04 - mean_absolute_error: 0.0149 - val_loss: 1.8814e-05 - val_mean_absolute_error: 0.0046\n",
      "\n",
      "Epoch 00669: val_loss did not improve from 0.00001\n",
      "Epoch 670/800\n",
      "90/90 [==============================] - 38s 420ms/step - loss: 2.5158e-04 - mean_absolute_error: 0.0147 - val_loss: 1.6872e-05 - val_mean_absolute_error: 0.0039\n",
      "\n",
      "Epoch 00670: val_loss did not improve from 0.00001\n",
      "Epoch 671/800\n",
      "90/90 [==============================] - 38s 419ms/step - loss: 2.4234e-04 - mean_absolute_error: 0.0143 - val_loss: 1.4596e-05 - val_mean_absolute_error: 0.0036\n",
      "\n",
      "Epoch 00671: val_loss did not improve from 0.00001\n",
      "Epoch 672/800\n",
      "90/90 [==============================] - 39s 431ms/step - loss: 2.6518e-04 - mean_absolute_error: 0.0148 - val_loss: 2.1200e-05 - val_mean_absolute_error: 0.0041\n",
      "\n",
      "Epoch 00672: val_loss did not improve from 0.00001\n",
      "Epoch 673/800\n",
      "90/90 [==============================] - 39s 431ms/step - loss: 2.5897e-04 - mean_absolute_error: 0.0148 - val_loss: 2.5030e-05 - val_mean_absolute_error: 0.0047\n",
      "\n",
      "Epoch 00673: val_loss did not improve from 0.00001\n",
      "Epoch 674/800\n",
      "90/90 [==============================] - 37s 414ms/step - loss: 2.5613e-04 - mean_absolute_error: 0.0149 - val_loss: 1.6209e-05 - val_mean_absolute_error: 0.0036\n",
      "\n",
      "Epoch 00674: val_loss did not improve from 0.00001\n",
      "Epoch 675/800\n",
      "90/90 [==============================] - 36s 403ms/step - loss: 2.3869e-04 - mean_absolute_error: 0.0144 - val_loss: 1.5039e-05 - val_mean_absolute_error: 0.0036\n",
      "\n",
      "Epoch 00675: val_loss did not improve from 0.00001\n",
      "Epoch 676/800\n",
      "90/90 [==============================] - 38s 423ms/step - loss: 2.5304e-04 - mean_absolute_error: 0.0148 - val_loss: 2.1427e-05 - val_mean_absolute_error: 0.0048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00676: val_loss did not improve from 0.00001\n",
      "Epoch 677/800\n",
      "90/90 [==============================] - 38s 417ms/step - loss: 2.4343e-04 - mean_absolute_error: 0.0144 - val_loss: 1.1052e-04 - val_mean_absolute_error: 0.0106\n",
      "\n",
      "Epoch 00677: val_loss did not improve from 0.00001\n",
      "Epoch 678/800\n",
      "90/90 [==============================] - 39s 432ms/step - loss: 2.5035e-04 - mean_absolute_error: 0.0146 - val_loss: 1.9429e-05 - val_mean_absolute_error: 0.0041\n",
      "\n",
      "Epoch 00678: val_loss did not improve from 0.00001\n",
      "Epoch 679/800\n",
      "90/90 [==============================] - 37s 410ms/step - loss: 2.5163e-04 - mean_absolute_error: 0.0147 - val_loss: 5.1630e-05 - val_mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 00679: val_loss did not improve from 0.00001\n",
      "Epoch 680/800\n",
      "90/90 [==============================] - 36s 395ms/step - loss: 2.3887e-04 - mean_absolute_error: 0.0144 - val_loss: 1.8774e-05 - val_mean_absolute_error: 0.0044\n",
      "\n",
      "Epoch 00680: val_loss did not improve from 0.00001\n",
      "Epoch 681/800\n",
      "90/90 [==============================] - 38s 425ms/step - loss: 2.5517e-04 - mean_absolute_error: 0.0147 - val_loss: 2.5693e-05 - val_mean_absolute_error: 0.0047\n",
      "\n",
      "Epoch 00681: val_loss did not improve from 0.00001\n",
      "Epoch 682/800\n",
      "90/90 [==============================] - 37s 408ms/step - loss: 2.7972e-04 - mean_absolute_error: 0.0153 - val_loss: 3.1421e-05 - val_mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 00682: val_loss did not improve from 0.00001\n",
      "Epoch 683/800\n",
      "90/90 [==============================] - 38s 420ms/step - loss: 2.4914e-04 - mean_absolute_error: 0.0146 - val_loss: 2.7477e-05 - val_mean_absolute_error: 0.0058\n",
      "\n",
      "Epoch 00683: val_loss did not improve from 0.00001\n",
      "Epoch 684/800\n",
      "90/90 [==============================] - 37s 417ms/step - loss: 2.4577e-04 - mean_absolute_error: 0.0145 - val_loss: 2.2296e-05 - val_mean_absolute_error: 0.0050\n",
      "\n",
      "Epoch 00684: val_loss did not improve from 0.00001\n",
      "Epoch 685/800\n",
      "90/90 [==============================] - 38s 420ms/step - loss: 2.4203e-04 - mean_absolute_error: 0.0145 - val_loss: 5.7097e-05 - val_mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 00685: val_loss did not improve from 0.00001\n",
      "Epoch 686/800\n",
      "90/90 [==============================] - 39s 436ms/step - loss: 2.7070e-04 - mean_absolute_error: 0.0148 - val_loss: 2.5131e-05 - val_mean_absolute_error: 0.0049\n",
      "\n",
      "Epoch 00686: val_loss did not improve from 0.00001\n",
      "Epoch 687/800\n",
      "90/90 [==============================] - 37s 409ms/step - loss: 2.5764e-04 - mean_absolute_error: 0.0147 - val_loss: 1.6837e-05 - val_mean_absolute_error: 0.0039\n",
      "\n",
      "Epoch 00687: val_loss did not improve from 0.00001\n",
      "Epoch 688/800\n",
      "90/90 [==============================] - 39s 430ms/step - loss: 2.6750e-04 - mean_absolute_error: 0.0150 - val_loss: 6.6002e-05 - val_mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 00688: val_loss did not improve from 0.00001\n",
      "Epoch 689/800\n",
      "90/90 [==============================] - 38s 428ms/step - loss: 2.4627e-04 - mean_absolute_error: 0.0149 - val_loss: 1.8062e-05 - val_mean_absolute_error: 0.0040\n",
      "\n",
      "Epoch 00689: val_loss did not improve from 0.00001\n",
      "Epoch 690/800\n",
      "90/90 [==============================] - 37s 416ms/step - loss: 2.7653e-04 - mean_absolute_error: 0.0150 - val_loss: 2.0202e-05 - val_mean_absolute_error: 0.0041\n",
      "\n",
      "Epoch 00690: val_loss did not improve from 0.00001\n",
      "Epoch 691/800\n",
      "90/90 [==============================] - 37s 412ms/step - loss: 2.5668e-04 - mean_absolute_error: 0.0148 - val_loss: 2.2002e-05 - val_mean_absolute_error: 0.0044\n",
      "\n",
      "Epoch 00691: val_loss did not improve from 0.00001\n",
      "Epoch 692/800\n",
      "90/90 [==============================] - 37s 416ms/step - loss: 2.3447e-04 - mean_absolute_error: 0.0143 - val_loss: 1.6537e-05 - val_mean_absolute_error: 0.0038\n",
      "\n",
      "Epoch 00692: val_loss did not improve from 0.00001\n",
      "Epoch 693/800\n",
      "90/90 [==============================] - 36s 406ms/step - loss: 2.5028e-04 - mean_absolute_error: 0.0146 - val_loss: 2.7784e-05 - val_mean_absolute_error: 0.0044\n",
      "\n",
      "Epoch 00693: val_loss did not improve from 0.00001\n",
      "Epoch 694/800\n",
      "90/90 [==============================] - 38s 421ms/step - loss: 2.5099e-04 - mean_absolute_error: 0.0146 - val_loss: 1.8113e-05 - val_mean_absolute_error: 0.0043\n",
      "\n",
      "Epoch 00694: val_loss did not improve from 0.00001\n",
      "Epoch 695/800\n",
      "90/90 [==============================] - 39s 429ms/step - loss: 2.4647e-04 - mean_absolute_error: 0.0145 - val_loss: 2.6055e-05 - val_mean_absolute_error: 0.0042\n",
      "\n",
      "Epoch 00695: val_loss did not improve from 0.00001\n",
      "Epoch 696/800\n",
      "90/90 [==============================] - 39s 431ms/step - loss: 2.5683e-04 - mean_absolute_error: 0.0146 - val_loss: 5.1732e-05 - val_mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 00696: val_loss did not improve from 0.00001\n",
      "Epoch 697/800\n",
      "90/90 [==============================] - 39s 436ms/step - loss: 2.7061e-04 - mean_absolute_error: 0.0151 - val_loss: 1.5813e-05 - val_mean_absolute_error: 0.0037\n",
      "\n",
      "Epoch 00697: val_loss did not improve from 0.00001\n",
      "Epoch 698/800\n",
      "90/90 [==============================] - 38s 423ms/step - loss: 2.2861e-04 - mean_absolute_error: 0.0141 - val_loss: 3.2527e-05 - val_mean_absolute_error: 0.0062\n",
      "\n",
      "Epoch 00698: val_loss did not improve from 0.00001\n",
      "Epoch 699/800\n",
      "90/90 [==============================] - 39s 431ms/step - loss: 2.6008e-04 - mean_absolute_error: 0.0146 - val_loss: 2.2185e-05 - val_mean_absolute_error: 0.0046\n",
      "\n",
      "Epoch 00699: val_loss did not improve from 0.00001\n",
      "Epoch 700/800\n",
      "90/90 [==============================] - 38s 424ms/step - loss: 2.5448e-04 - mean_absolute_error: 0.0147 - val_loss: 1.5765e-05 - val_mean_absolute_error: 0.0039\n",
      "\n",
      "Epoch 00700: val_loss did not improve from 0.00001\n",
      "Epoch 701/800\n",
      "90/90 [==============================] - 38s 421ms/step - loss: 2.6845e-04 - mean_absolute_error: 0.0149 - val_loss: 3.5410e-05 - val_mean_absolute_error: 0.0056\n",
      "\n",
      "Epoch 00701: val_loss did not improve from 0.00001\n",
      "Epoch 702/800\n",
      "90/90 [==============================] - 38s 419ms/step - loss: 2.5613e-04 - mean_absolute_error: 0.0149 - val_loss: 3.0785e-05 - val_mean_absolute_error: 0.0053\n",
      "\n",
      "Epoch 00702: val_loss did not improve from 0.00001\n",
      "Epoch 703/800\n",
      "90/90 [==============================] - 37s 411ms/step - loss: 2.2468e-04 - mean_absolute_error: 0.0139 - val_loss: 2.9027e-05 - val_mean_absolute_error: 0.0056\n",
      "\n",
      "Epoch 00703: val_loss did not improve from 0.00001\n",
      "Epoch 704/800\n",
      "90/90 [==============================] - 38s 421ms/step - loss: 2.5574e-04 - mean_absolute_error: 0.0146 - val_loss: 1.6642e-05 - val_mean_absolute_error: 0.0038\n",
      "\n",
      "Epoch 00704: val_loss did not improve from 0.00001\n",
      "Epoch 705/800\n",
      "90/90 [==============================] - 39s 430ms/step - loss: 2.4085e-04 - mean_absolute_error: 0.0146 - val_loss: 2.2378e-05 - val_mean_absolute_error: 0.0052\n",
      "\n",
      "Epoch 00705: val_loss did not improve from 0.00001\n",
      "Epoch 706/800\n",
      "90/90 [==============================] - 38s 425ms/step - loss: 2.4738e-04 - mean_absolute_error: 0.0144 - val_loss: 1.8274e-05 - val_mean_absolute_error: 0.0038\n",
      "\n",
      "Epoch 00706: val_loss did not improve from 0.00001\n",
      "Epoch 707/800\n",
      "90/90 [==============================] - 39s 435ms/step - loss: 2.4834e-04 - mean_absolute_error: 0.0148 - val_loss: 1.5414e-05 - val_mean_absolute_error: 0.0036\n",
      "\n",
      "Epoch 00707: val_loss did not improve from 0.00001\n",
      "Epoch 708/800\n",
      "90/90 [==============================] - 39s 430ms/step - loss: 2.7240e-04 - mean_absolute_error: 0.0149 - val_loss: 2.2546e-05 - val_mean_absolute_error: 0.0049\n",
      "\n",
      "Epoch 00708: val_loss did not improve from 0.00001\n",
      "Epoch 709/800\n",
      "90/90 [==============================] - 38s 421ms/step - loss: 2.6106e-04 - mean_absolute_error: 0.0145 - val_loss: 2.4309e-05 - val_mean_absolute_error: 0.0053\n",
      "\n",
      "Epoch 00709: val_loss did not improve from 0.00001\n",
      "Epoch 710/800\n",
      "90/90 [==============================] - 39s 436ms/step - loss: 2.6057e-04 - mean_absolute_error: 0.0146 - val_loss: 2.4551e-05 - val_mean_absolute_error: 0.0048\n",
      "\n",
      "Epoch 00710: val_loss did not improve from 0.00001\n",
      "Epoch 711/800\n",
      "90/90 [==============================] - 38s 422ms/step - loss: 2.4919e-04 - mean_absolute_error: 0.0146 - val_loss: 2.8526e-05 - val_mean_absolute_error: 0.0053\n",
      "\n",
      "Epoch 00711: val_loss did not improve from 0.00001\n",
      "Epoch 712/800\n",
      "90/90 [==============================] - 38s 423ms/step - loss: 2.5622e-04 - mean_absolute_error: 0.0150 - val_loss: 2.6417e-05 - val_mean_absolute_error: 0.0047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00712: val_loss did not improve from 0.00001\n",
      "Epoch 713/800\n",
      "90/90 [==============================] - 39s 436ms/step - loss: 2.3120e-04 - mean_absolute_error: 0.0144 - val_loss: 3.0569e-05 - val_mean_absolute_error: 0.0054\n",
      "\n",
      "Epoch 00713: val_loss did not improve from 0.00001\n",
      "Epoch 714/800\n",
      "90/90 [==============================] - 38s 428ms/step - loss: 2.6634e-04 - mean_absolute_error: 0.0146 - val_loss: 2.5921e-05 - val_mean_absolute_error: 0.0057\n",
      "\n",
      "Epoch 00714: val_loss did not improve from 0.00001\n",
      "Epoch 715/800\n",
      "90/90 [==============================] - 38s 427ms/step - loss: 2.3986e-04 - mean_absolute_error: 0.0142 - val_loss: 1.7566e-05 - val_mean_absolute_error: 0.0038\n",
      "\n",
      "Epoch 00715: val_loss did not improve from 0.00001\n",
      "Epoch 716/800\n",
      "90/90 [==============================] - 38s 422ms/step - loss: 2.3546e-04 - mean_absolute_error: 0.0143 - val_loss: 2.7434e-05 - val_mean_absolute_error: 0.0045\n",
      "\n",
      "Epoch 00716: val_loss did not improve from 0.00001\n",
      "Epoch 717/800\n",
      "90/90 [==============================] - 37s 407ms/step - loss: 2.4250e-04 - mean_absolute_error: 0.0143 - val_loss: 2.7929e-05 - val_mean_absolute_error: 0.0056\n",
      "\n",
      "Epoch 00717: val_loss did not improve from 0.00001\n",
      "Epoch 718/800\n",
      "90/90 [==============================] - 38s 427ms/step - loss: 2.4543e-04 - mean_absolute_error: 0.0142 - val_loss: 1.5419e-05 - val_mean_absolute_error: 0.0035\n",
      "\n",
      "Epoch 00718: val_loss did not improve from 0.00001\n",
      "Epoch 719/800\n",
      "90/90 [==============================] - 40s 443ms/step - loss: 2.2663e-04 - mean_absolute_error: 0.0142 - val_loss: 2.6418e-05 - val_mean_absolute_error: 0.0054\n",
      "\n",
      "Epoch 00719: val_loss did not improve from 0.00001\n",
      "Epoch 720/800\n",
      "90/90 [==============================] - 37s 416ms/step - loss: 2.3394e-04 - mean_absolute_error: 0.0142 - val_loss: 3.1001e-05 - val_mean_absolute_error: 0.0059\n",
      "\n",
      "Epoch 00720: val_loss did not improve from 0.00001\n",
      "Epoch 721/800\n",
      "90/90 [==============================] - 38s 423ms/step - loss: 2.3250e-04 - mean_absolute_error: 0.0140 - val_loss: 2.4029e-05 - val_mean_absolute_error: 0.0046\n",
      "\n",
      "Epoch 00721: val_loss did not improve from 0.00001\n",
      "Epoch 722/800\n",
      "90/90 [==============================] - 51s 569ms/step - loss: 2.4907e-04 - mean_absolute_error: 0.0146 - val_loss: 1.6296e-05 - val_mean_absolute_error: 0.0040\n",
      "\n",
      "Epoch 00722: val_loss did not improve from 0.00001\n",
      "Epoch 723/800\n",
      "90/90 [==============================] - 49s 546ms/step - loss: 2.3913e-04 - mean_absolute_error: 0.0144 - val_loss: 3.9066e-05 - val_mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 00723: val_loss did not improve from 0.00001\n",
      "Epoch 724/800\n",
      "90/90 [==============================] - 48s 536ms/step - loss: 2.2504e-04 - mean_absolute_error: 0.0140 - val_loss: 2.5788e-05 - val_mean_absolute_error: 0.0057\n",
      "\n",
      "Epoch 00724: val_loss did not improve from 0.00001\n",
      "Epoch 725/800\n",
      "90/90 [==============================] - 45s 501ms/step - loss: 2.4927e-04 - mean_absolute_error: 0.0144 - val_loss: 2.0999e-05 - val_mean_absolute_error: 0.0038\n",
      "\n",
      "Epoch 00725: val_loss did not improve from 0.00001\n",
      "Epoch 726/800\n",
      "90/90 [==============================] - 38s 426ms/step - loss: 2.4539e-04 - mean_absolute_error: 0.0145 - val_loss: 3.6718e-05 - val_mean_absolute_error: 0.0066\n",
      "\n",
      "Epoch 00726: val_loss did not improve from 0.00001\n",
      "Epoch 727/800\n",
      "90/90 [==============================] - 37s 412ms/step - loss: 2.4932e-04 - mean_absolute_error: 0.0143 - val_loss: 3.8017e-05 - val_mean_absolute_error: 0.0057\n",
      "\n",
      "Epoch 00727: val_loss did not improve from 0.00001\n",
      "Epoch 728/800\n",
      "90/90 [==============================] - 41s 461ms/step - loss: 2.4470e-04 - mean_absolute_error: 0.0144 - val_loss: 6.0974e-05 - val_mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 00728: val_loss did not improve from 0.00001\n",
      "Epoch 729/800\n",
      "90/90 [==============================] - 43s 472ms/step - loss: 2.6432e-04 - mean_absolute_error: 0.0148 - val_loss: 1.4808e-05 - val_mean_absolute_error: 0.0035\n",
      "\n",
      "Epoch 00729: val_loss did not improve from 0.00001\n",
      "Epoch 730/800\n",
      "90/90 [==============================] - 39s 433ms/step - loss: 2.7630e-04 - mean_absolute_error: 0.0149 - val_loss: 4.9502e-05 - val_mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 00730: val_loss did not improve from 0.00001\n",
      "Epoch 731/800\n",
      "90/90 [==============================] - 40s 444ms/step - loss: 2.4575e-04 - mean_absolute_error: 0.0144 - val_loss: 2.0041e-05 - val_mean_absolute_error: 0.0046\n",
      "\n",
      "Epoch 00731: val_loss did not improve from 0.00001\n",
      "Epoch 732/800\n",
      "90/90 [==============================] - 47s 521ms/step - loss: 2.6166e-04 - mean_absolute_error: 0.0149 - val_loss: 2.0271e-05 - val_mean_absolute_error: 0.0042\n",
      "\n",
      "Epoch 00732: val_loss did not improve from 0.00001\n",
      "Epoch 733/800\n",
      "90/90 [==============================] - 38s 428ms/step - loss: 2.4874e-04 - mean_absolute_error: 0.0143 - val_loss: 2.2835e-05 - val_mean_absolute_error: 0.0052\n",
      "\n",
      "Epoch 00733: val_loss did not improve from 0.00001\n",
      "Epoch 734/800\n",
      "90/90 [==============================] - 39s 431ms/step - loss: 2.6122e-04 - mean_absolute_error: 0.0148 - val_loss: 1.6213e-05 - val_mean_absolute_error: 0.0040\n",
      "\n",
      "Epoch 00734: val_loss did not improve from 0.00001\n",
      "Epoch 735/800\n",
      "90/90 [==============================] - 40s 447ms/step - loss: 2.4922e-04 - mean_absolute_error: 0.0147 - val_loss: 1.9603e-05 - val_mean_absolute_error: 0.0043\n",
      "\n",
      "Epoch 00735: val_loss did not improve from 0.00001\n",
      "Epoch 736/800\n",
      "90/90 [==============================] - 43s 474ms/step - loss: 2.4926e-04 - mean_absolute_error: 0.0147 - val_loss: 1.5378e-05 - val_mean_absolute_error: 0.0038\n",
      "\n",
      "Epoch 00736: val_loss did not improve from 0.00001\n",
      "Epoch 737/800\n",
      "90/90 [==============================] - 39s 436ms/step - loss: 2.6100e-04 - mean_absolute_error: 0.0148 - val_loss: 1.2844e-05 - val_mean_absolute_error: 0.0032\n",
      "\n",
      "Epoch 00737: val_loss improved from 0.00001 to 0.00001, saving model to results\\2021-09-07_SPY-shuffle-1-scale-1-split_date-0-epochs-800.h5\n",
      "Epoch 738/800\n",
      "90/90 [==============================] - 39s 433ms/step - loss: 2.4904e-04 - mean_absolute_error: 0.0149 - val_loss: 3.1388e-05 - val_mean_absolute_error: 0.0055\n",
      "\n",
      "Epoch 00738: val_loss did not improve from 0.00001\n",
      "Epoch 739/800\n",
      "90/90 [==============================] - 40s 447ms/step - loss: 2.3844e-04 - mean_absolute_error: 0.0144 - val_loss: 1.4809e-05 - val_mean_absolute_error: 0.0037\n",
      "\n",
      "Epoch 00739: val_loss did not improve from 0.00001\n",
      "Epoch 740/800\n",
      "90/90 [==============================] - 41s 461ms/step - loss: 2.3930e-04 - mean_absolute_error: 0.0140 - val_loss: 1.8941e-05 - val_mean_absolute_error: 0.0044\n",
      "\n",
      "Epoch 00740: val_loss did not improve from 0.00001\n",
      "Epoch 741/800\n",
      "90/90 [==============================] - 39s 431ms/step - loss: 2.4645e-04 - mean_absolute_error: 0.0146 - val_loss: 2.4968e-05 - val_mean_absolute_error: 0.0051\n",
      "\n",
      "Epoch 00741: val_loss did not improve from 0.00001\n",
      "Epoch 742/800\n",
      "90/90 [==============================] - 37s 413ms/step - loss: 2.3393e-04 - mean_absolute_error: 0.0143 - val_loss: 1.4319e-05 - val_mean_absolute_error: 0.0033\n",
      "\n",
      "Epoch 00742: val_loss did not improve from 0.00001\n",
      "Epoch 743/800\n",
      "90/90 [==============================] - 39s 437ms/step - loss: 2.4057e-04 - mean_absolute_error: 0.0142 - val_loss: 1.7928e-05 - val_mean_absolute_error: 0.0040\n",
      "\n",
      "Epoch 00743: val_loss did not improve from 0.00001\n",
      "Epoch 744/800\n",
      "90/90 [==============================] - 47s 521ms/step - loss: 2.3635e-04 - mean_absolute_error: 0.0143 - val_loss: 2.6001e-05 - val_mean_absolute_error: 0.0048\n",
      "\n",
      "Epoch 00744: val_loss did not improve from 0.00001\n",
      "Epoch 745/800\n",
      "90/90 [==============================] - 41s 452ms/step - loss: 2.4027e-04 - mean_absolute_error: 0.0142 - val_loss: 7.3084e-05 - val_mean_absolute_error: 0.0103\n",
      "\n",
      "Epoch 00745: val_loss did not improve from 0.00001\n",
      "Epoch 746/800\n",
      "90/90 [==============================] - 44s 486ms/step - loss: 2.3732e-04 - mean_absolute_error: 0.0145 - val_loss: 2.3877e-05 - val_mean_absolute_error: 0.0047\n",
      "\n",
      "Epoch 00746: val_loss did not improve from 0.00001\n",
      "Epoch 747/800\n",
      "90/90 [==============================] - 38s 424ms/step - loss: 2.4568e-04 - mean_absolute_error: 0.0142 - val_loss: 2.7321e-05 - val_mean_absolute_error: 0.0049\n",
      "\n",
      "Epoch 00747: val_loss did not improve from 0.00001\n",
      "Epoch 748/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 44s 491ms/step - loss: 2.5646e-04 - mean_absolute_error: 0.0144 - val_loss: 3.1374e-05 - val_mean_absolute_error: 0.0052\n",
      "\n",
      "Epoch 00748: val_loss did not improve from 0.00001\n",
      "Epoch 749/800\n",
      "90/90 [==============================] - 41s 461ms/step - loss: 2.5722e-04 - mean_absolute_error: 0.0146 - val_loss: 3.9233e-05 - val_mean_absolute_error: 0.0058\n",
      "\n",
      "Epoch 00749: val_loss did not improve from 0.00001\n",
      "Epoch 750/800\n",
      "90/90 [==============================] - 46s 511ms/step - loss: 2.5039e-04 - mean_absolute_error: 0.0148 - val_loss: 4.2228e-05 - val_mean_absolute_error: 0.0064\n",
      "\n",
      "Epoch 00750: val_loss did not improve from 0.00001\n",
      "Epoch 751/800\n",
      "90/90 [==============================] - 50s 552ms/step - loss: 2.2997e-04 - mean_absolute_error: 0.0140 - val_loss: 2.9604e-05 - val_mean_absolute_error: 0.0055\n",
      "\n",
      "Epoch 00751: val_loss did not improve from 0.00001\n",
      "Epoch 752/800\n",
      "90/90 [==============================] - 48s 537ms/step - loss: 2.8688e-04 - mean_absolute_error: 0.0152 - val_loss: 3.3733e-05 - val_mean_absolute_error: 0.0058\n",
      "\n",
      "Epoch 00752: val_loss did not improve from 0.00001\n",
      "Epoch 753/800\n",
      "90/90 [==============================] - 43s 477ms/step - loss: 2.4292e-04 - mean_absolute_error: 0.0141 - val_loss: 2.8929e-05 - val_mean_absolute_error: 0.0047\n",
      "\n",
      "Epoch 00753: val_loss did not improve from 0.00001\n",
      "Epoch 754/800\n",
      "90/90 [==============================] - 47s 521ms/step - loss: 2.4662e-04 - mean_absolute_error: 0.0143 - val_loss: 2.3218e-05 - val_mean_absolute_error: 0.0049\n",
      "\n",
      "Epoch 00754: val_loss did not improve from 0.00001\n",
      "Epoch 755/800\n",
      "90/90 [==============================] - 41s 452ms/step - loss: 2.3897e-04 - mean_absolute_error: 0.0143 - val_loss: 1.7954e-05 - val_mean_absolute_error: 0.0036\n",
      "\n",
      "Epoch 00755: val_loss did not improve from 0.00001\n",
      "Epoch 756/800\n",
      "90/90 [==============================] - 40s 449ms/step - loss: 2.6432e-04 - mean_absolute_error: 0.0149 - val_loss: 2.1733e-05 - val_mean_absolute_error: 0.0047\n",
      "\n",
      "Epoch 00756: val_loss did not improve from 0.00001\n",
      "Epoch 757/800\n",
      "90/90 [==============================] - 42s 464ms/step - loss: 2.3498e-04 - mean_absolute_error: 0.0140 - val_loss: 3.8339e-05 - val_mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 00757: val_loss did not improve from 0.00001\n",
      "Epoch 758/800\n",
      "90/90 [==============================] - 40s 451ms/step - loss: 2.5676e-04 - mean_absolute_error: 0.0146 - val_loss: 4.1869e-05 - val_mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 00758: val_loss did not improve from 0.00001\n",
      "Epoch 759/800\n",
      "90/90 [==============================] - 42s 468ms/step - loss: 2.4367e-04 - mean_absolute_error: 0.0147 - val_loss: 2.5797e-05 - val_mean_absolute_error: 0.0052\n",
      "\n",
      "Epoch 00759: val_loss did not improve from 0.00001\n",
      "Epoch 760/800\n",
      "90/90 [==============================] - 42s 471ms/step - loss: 2.4669e-04 - mean_absolute_error: 0.0144 - val_loss: 1.8473e-05 - val_mean_absolute_error: 0.0042\n",
      "\n",
      "Epoch 00760: val_loss did not improve from 0.00001\n",
      "Epoch 761/800\n",
      "90/90 [==============================] - 43s 481ms/step - loss: 2.4942e-04 - mean_absolute_error: 0.0145 - val_loss: 2.4931e-05 - val_mean_absolute_error: 0.0044\n",
      "\n",
      "Epoch 00761: val_loss did not improve from 0.00001\n",
      "Epoch 762/800\n",
      "90/90 [==============================] - 40s 447ms/step - loss: 2.4343e-04 - mean_absolute_error: 0.0142 - val_loss: 3.5720e-05 - val_mean_absolute_error: 0.0057\n",
      "\n",
      "Epoch 00762: val_loss did not improve from 0.00001\n",
      "Epoch 763/800\n",
      "90/90 [==============================] - 46s 512ms/step - loss: 2.7392e-04 - mean_absolute_error: 0.0150 - val_loss: 6.0768e-05 - val_mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 00763: val_loss did not improve from 0.00001\n",
      "Epoch 764/800\n",
      "90/90 [==============================] - 41s 458ms/step - loss: 2.5982e-04 - mean_absolute_error: 0.0148 - val_loss: 1.6437e-05 - val_mean_absolute_error: 0.0040\n",
      "\n",
      "Epoch 00764: val_loss did not improve from 0.00001\n",
      "Epoch 765/800\n",
      "90/90 [==============================] - 42s 465ms/step - loss: 2.4873e-04 - mean_absolute_error: 0.0144 - val_loss: 2.3253e-05 - val_mean_absolute_error: 0.0043\n",
      "\n",
      "Epoch 00765: val_loss did not improve from 0.00001\n",
      "Epoch 766/800\n",
      "90/90 [==============================] - 45s 500ms/step - loss: 2.4690e-04 - mean_absolute_error: 0.0144 - val_loss: 2.1493e-05 - val_mean_absolute_error: 0.0047\n",
      "\n",
      "Epoch 00766: val_loss did not improve from 0.00001\n",
      "Epoch 767/800\n",
      "90/90 [==============================] - 45s 498ms/step - loss: 2.5068e-04 - mean_absolute_error: 0.0144 - val_loss: 3.6703e-05 - val_mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 00767: val_loss did not improve from 0.00001\n",
      "Epoch 768/800\n",
      "90/90 [==============================] - 41s 459ms/step - loss: 2.6216e-04 - mean_absolute_error: 0.0148 - val_loss: 5.7704e-05 - val_mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 00768: val_loss did not improve from 0.00001\n",
      "Epoch 769/800\n",
      "90/90 [==============================] - 42s 471ms/step - loss: 2.4619e-04 - mean_absolute_error: 0.0143 - val_loss: 2.6133e-05 - val_mean_absolute_error: 0.0044\n",
      "\n",
      "Epoch 00769: val_loss did not improve from 0.00001\n",
      "Epoch 770/800\n",
      "90/90 [==============================] - 44s 488ms/step - loss: 2.6254e-04 - mean_absolute_error: 0.0146 - val_loss: 2.3401e-05 - val_mean_absolute_error: 0.0049\n",
      "\n",
      "Epoch 00770: val_loss did not improve from 0.00001\n",
      "Epoch 771/800\n",
      "90/90 [==============================] - 50s 556ms/step - loss: 2.4435e-04 - mean_absolute_error: 0.0144 - val_loss: 5.9927e-05 - val_mean_absolute_error: 0.0081\n",
      "\n",
      "Epoch 00771: val_loss did not improve from 0.00001\n",
      "Epoch 772/800\n",
      "90/90 [==============================] - 45s 494ms/step - loss: 2.3121e-04 - mean_absolute_error: 0.0141 - val_loss: 2.5147e-05 - val_mean_absolute_error: 0.0056\n",
      "\n",
      "Epoch 00772: val_loss did not improve from 0.00001\n",
      "Epoch 773/800\n",
      "90/90 [==============================] - 41s 456ms/step - loss: 2.4238e-04 - mean_absolute_error: 0.0141 - val_loss: 1.9192e-05 - val_mean_absolute_error: 0.0037\n",
      "\n",
      "Epoch 00773: val_loss did not improve from 0.00001\n",
      "Epoch 774/800\n",
      "90/90 [==============================] - 41s 452ms/step - loss: 2.5582e-04 - mean_absolute_error: 0.0144 - val_loss: 2.3393e-05 - val_mean_absolute_error: 0.0052\n",
      "\n",
      "Epoch 00774: val_loss did not improve from 0.00001\n",
      "Epoch 775/800\n",
      "90/90 [==============================] - 44s 485ms/step - loss: 2.4244e-04 - mean_absolute_error: 0.0146 - val_loss: 2.0019e-05 - val_mean_absolute_error: 0.0047\n",
      "\n",
      "Epoch 00775: val_loss did not improve from 0.00001\n",
      "Epoch 776/800\n",
      "90/90 [==============================] - 43s 475ms/step - loss: 2.3792e-04 - mean_absolute_error: 0.0141 - val_loss: 2.4749e-05 - val_mean_absolute_error: 0.0051\n",
      "\n",
      "Epoch 00776: val_loss did not improve from 0.00001\n",
      "Epoch 777/800\n",
      "90/90 [==============================] - 43s 477ms/step - loss: 2.5606e-04 - mean_absolute_error: 0.0147 - val_loss: 2.8279e-05 - val_mean_absolute_error: 0.0054\n",
      "\n",
      "Epoch 00777: val_loss did not improve from 0.00001\n",
      "Epoch 778/800\n",
      "90/90 [==============================] - 44s 489ms/step - loss: 2.4191e-04 - mean_absolute_error: 0.0147 - val_loss: 2.3719e-05 - val_mean_absolute_error: 0.0046\n",
      "\n",
      "Epoch 00778: val_loss did not improve from 0.00001\n",
      "Epoch 779/800\n",
      "90/90 [==============================] - 56s 622ms/step - loss: 2.4411e-04 - mean_absolute_error: 0.0143 - val_loss: 2.6452e-05 - val_mean_absolute_error: 0.0052\n",
      "\n",
      "Epoch 00779: val_loss did not improve from 0.00001\n",
      "Epoch 780/800\n",
      "90/90 [==============================] - 48s 529ms/step - loss: 2.4357e-04 - mean_absolute_error: 0.0147 - val_loss: 2.7886e-05 - val_mean_absolute_error: 0.0054\n",
      "\n",
      "Epoch 00780: val_loss did not improve from 0.00001\n",
      "Epoch 781/800\n",
      "90/90 [==============================] - 49s 544ms/step - loss: 2.4693e-04 - mean_absolute_error: 0.0145 - val_loss: 6.2570e-05 - val_mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 00781: val_loss did not improve from 0.00001\n",
      "Epoch 782/800\n",
      "90/90 [==============================] - 47s 527ms/step - loss: 2.5021e-04 - mean_absolute_error: 0.0147 - val_loss: 2.6654e-05 - val_mean_absolute_error: 0.0051\n",
      "\n",
      "Epoch 00782: val_loss did not improve from 0.00001\n",
      "Epoch 783/800\n",
      "90/90 [==============================] - 48s 540ms/step - loss: 2.3379e-04 - mean_absolute_error: 0.0143 - val_loss: 2.2003e-05 - val_mean_absolute_error: 0.0043\n",
      "\n",
      "Epoch 00783: val_loss did not improve from 0.00001\n",
      "Epoch 784/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 48s 531ms/step - loss: 2.5312e-04 - mean_absolute_error: 0.0146 - val_loss: 1.8463e-05 - val_mean_absolute_error: 0.0039\n",
      "\n",
      "Epoch 00784: val_loss did not improve from 0.00001\n",
      "Epoch 785/800\n",
      "90/90 [==============================] - 47s 520ms/step - loss: 2.5224e-04 - mean_absolute_error: 0.0145 - val_loss: 5.5789e-05 - val_mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 00785: val_loss did not improve from 0.00001\n",
      "Epoch 786/800\n",
      "90/90 [==============================] - 48s 533ms/step - loss: 2.4028e-04 - mean_absolute_error: 0.0139 - val_loss: 1.7760e-05 - val_mean_absolute_error: 0.0042\n",
      "\n",
      "Epoch 00786: val_loss did not improve from 0.00001\n",
      "Epoch 787/800\n",
      "90/90 [==============================] - 49s 546ms/step - loss: 2.4641e-04 - mean_absolute_error: 0.0145 - val_loss: 4.3646e-05 - val_mean_absolute_error: 0.0056\n",
      "\n",
      "Epoch 00787: val_loss did not improve from 0.00001\n",
      "Epoch 788/800\n",
      "90/90 [==============================] - 48s 534ms/step - loss: 2.5416e-04 - mean_absolute_error: 0.0146 - val_loss: 3.8913e-05 - val_mean_absolute_error: 0.0063\n",
      "\n",
      "Epoch 00788: val_loss did not improve from 0.00001\n",
      "Epoch 789/800\n",
      "90/90 [==============================] - 47s 528ms/step - loss: 2.6860e-04 - mean_absolute_error: 0.0150 - val_loss: 1.7431e-05 - val_mean_absolute_error: 0.0036\n",
      "\n",
      "Epoch 00789: val_loss did not improve from 0.00001\n",
      "Epoch 790/800\n",
      "90/90 [==============================] - 49s 544ms/step - loss: 2.4903e-04 - mean_absolute_error: 0.0143 - val_loss: 1.8170e-05 - val_mean_absolute_error: 0.0036\n",
      "\n",
      "Epoch 00790: val_loss did not improve from 0.00001\n",
      "Epoch 791/800\n",
      "90/90 [==============================] - 48s 537ms/step - loss: 2.5008e-04 - mean_absolute_error: 0.0147 - val_loss: 3.0184e-05 - val_mean_absolute_error: 0.0047\n",
      "\n",
      "Epoch 00791: val_loss did not improve from 0.00001\n",
      "Epoch 792/800\n",
      "90/90 [==============================] - 54s 606ms/step - loss: 2.4528e-04 - mean_absolute_error: 0.0147 - val_loss: 4.7106e-05 - val_mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 00792: val_loss did not improve from 0.00001\n",
      "Epoch 793/800\n",
      "90/90 [==============================] - 50s 558ms/step - loss: 2.5595e-04 - mean_absolute_error: 0.0144 - val_loss: 2.5452e-05 - val_mean_absolute_error: 0.0045\n",
      "\n",
      "Epoch 00793: val_loss did not improve from 0.00001\n",
      "Epoch 794/800\n",
      "90/90 [==============================] - 47s 527ms/step - loss: 2.5790e-04 - mean_absolute_error: 0.0149 - val_loss: 2.2958e-05 - val_mean_absolute_error: 0.0050\n",
      "\n",
      "Epoch 00794: val_loss did not improve from 0.00001\n",
      "Epoch 795/800\n",
      "90/90 [==============================] - 44s 485ms/step - loss: 2.2918e-04 - mean_absolute_error: 0.0139 - val_loss: 5.0384e-05 - val_mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 00795: val_loss did not improve from 0.00001\n",
      "Epoch 796/800\n",
      "90/90 [==============================] - 48s 534ms/step - loss: 2.4859e-04 - mean_absolute_error: 0.0143 - val_loss: 1.6757e-05 - val_mean_absolute_error: 0.0036\n",
      "\n",
      "Epoch 00796: val_loss did not improve from 0.00001\n",
      "Epoch 797/800\n",
      "90/90 [==============================] - 52s 584ms/step - loss: 2.4387e-04 - mean_absolute_error: 0.0144 - val_loss: 2.4797e-05 - val_mean_absolute_error: 0.0046\n",
      "\n",
      "Epoch 00797: val_loss did not improve from 0.00001\n",
      "Epoch 798/800\n",
      "90/90 [==============================] - 52s 583ms/step - loss: 2.5581e-04 - mean_absolute_error: 0.0145 - val_loss: 3.1027e-05 - val_mean_absolute_error: 0.0057\n",
      "\n",
      "Epoch 00798: val_loss did not improve from 0.00001\n",
      "Epoch 799/800\n",
      "90/90 [==============================] - 52s 579ms/step - loss: 2.5117e-04 - mean_absolute_error: 0.0144 - val_loss: 1.9328e-05 - val_mean_absolute_error: 0.0045\n",
      "\n",
      "Epoch 00799: val_loss did not improve from 0.00001\n",
      "Epoch 800/800\n",
      "90/90 [==============================] - 52s 580ms/step - loss: 2.4895e-04 - mean_absolute_error: 0.0146 - val_loss: 9.0234e-05 - val_mean_absolute_error: 0.0100\n",
      "\n",
      "Epoch 00800: val_loss did not improve from 0.00001\n"
     ]
    }
   ],
   "source": [
    "# some tensorflow callbacks\n",
    "checkpointer = ModelCheckpoint(os.path.join(\"results\", model_name + \".h5\"), save_weights_only=True, save_best_only=True, verbose=1)\n",
    "tensorboard = TensorBoard(log_dir=os.path.join(\"logs\", model_name))\n",
    "# train the model and save the weights whenever we see \n",
    "# a new optimal model using ModelCheckpoint\n",
    "history = model.fit(data[\"X_train\"], data[\"y_train\"],\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=(data[\"X_test\"], data[\"y_test\"]),\n",
    "                    callbacks=[checkpointer, tensorboard],\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "956d0743",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FROM NEWEST RUN\n",
    "\"\"\"\n",
    "\n",
    "# create these folders if they does not exist\n",
    "if not os.path.isdir(\"results\"):\n",
    "    os.mkdir(\"results\")\n",
    "\n",
    "if not os.path.isdir(\"logs\"):\n",
    "    os.mkdir(\"logs\")\n",
    "\n",
    "if not os.path.isdir(\"data\"):\n",
    "    os.mkdir(\"data\")\n",
    "\n",
    "# load optimal model weights from results folder\n",
    "model_path = os.path.join(\"results\", model_name) + \".h5\"\n",
    "model.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d6aa127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFROM PREVIOUS RUNS\\n\\nname = \\'2021-09-06_AMZN-shuffle-1-scale-1-split_date-0-epochs-500\\'\\n\\n# load optimal model weights from results folder\\nmodel_path = os.path.join(\"results\", name) + \".h5\"\\nmodel.load_weights(model_path)\\n\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "FROM PREVIOUS RUNS\n",
    "\n",
    "name = '2021-09-06_AMZN-shuffle-1-scale-1-split_date-0-epochs-500'\n",
    "\n",
    "# load optimal model weights from results folder\n",
    "model_path = os.path.join(\"results\", name) + \".h5\"\n",
    "model.load_weights(model_path)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0103a9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "loss, mae = model.evaluate(data[\"X_test\"], data[\"y_test\"], verbose=0)\n",
    "# calculate the mean absolute error (inverse scaling)\n",
    "if SCALE:\n",
    "    mean_absolute_error = data[\"column_scaler\"][\"adjclose\"].inverse_transform([[mae]])[0][0]\n",
    "else:\n",
    "    mean_absolute_error = mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f57dcc3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.003155990969389677"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c0963e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the final dataframe for the testing set\n",
    "final_df = get_final_df(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e05db7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the future price\n",
    "future_price = predict(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a8b2eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we calculate the accuracy by counting the number of positive profits\n",
    "accuracy_score = (len(final_df[final_df['sell_profit'] > 0]) + len(final_df[final_df['buy_profit'] > 0])) / len(final_df)\n",
    "# calculating total buy & sell profit\n",
    "total_buy_profit  = final_df[\"buy_profit\"].sum()\n",
    "total_sell_profit = final_df[\"sell_profit\"].sum()\n",
    "# total profit by adding sell & buy together\n",
    "total_profit = total_buy_profit + total_sell_profit\n",
    "# dividing total profit by number of testing samples (number of trades)\n",
    "profit_per_trade = total_profit / len(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46066342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Future price after 1 days is 441.70$\n",
      "huber_loss loss: 1.2843770491599571e-05\n",
      "Mean Absolute Error: 26.837655306014874\n",
      "Accuracy score: 0.5324947589098532\n",
      "Total buy profit: 154.60021400451652\n",
      "Total sell profit: -40.48010826110847\n",
      "Total profit: 114.12010574340805\n",
      "Profit per trade: 0.07974850156772051\n"
     ]
    }
   ],
   "source": [
    "# printing metrics\n",
    "print(f\"Future price after {LOOKUP_STEP} days is {future_price:.2f}$\")\n",
    "print(f\"{LOSS} loss:\", loss)\n",
    "print(\"Mean Absolute Error:\", mean_absolute_error)\n",
    "print(\"Accuracy score:\", accuracy_score)\n",
    "print(\"Total buy profit:\", total_buy_profit)\n",
    "print(\"Total sell profit:\", total_sell_profit)\n",
    "print(\"Total profit:\", total_profit)\n",
    "print(\"Profit per trade:\", profit_per_trade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0866e047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot true/pred prices graph\n",
    "plot_graph(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9701424a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the final dataframe to csv-results folder\n",
    "csv_results_folder = \"csv-results\"\n",
    "if not os.path.isdir(csv_results_folder):\n",
    "    os.mkdir(csv_results_folder)\n",
    "csv_filename = os.path.join(csv_results_folder, model_name + \".csv\")\n",
    "final_df.to_csv(csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4137dff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": true
       },
       "data": [
        {
         "marker": {
          "color": "rgba(171, 50, 96, 0.6)"
         },
         "name": "Training Loss",
         "type": "scatter",
         "y": [
          0.0042586796917021275,
          0.001177028869278729,
          0.0010004604700952768,
          0.0009694965556263924,
          0.0007988097495399415,
          0.0007622056873515248,
          0.0007056854083202779,
          0.0009216227917931974,
          0.0006936224526725709,
          0.0006542205810546875,
          0.0006248260615393519,
          0.0005697489250451326,
          0.000617933867033571,
          0.0006247652345336974,
          0.0005484373541548848,
          0.0005134979146532714,
          0.0004744995676446706,
          0.00048481131670996547,
          0.0004911749274469912,
          0.00043616004404611886,
          0.00048666365910321474,
          0.00044511008309200406,
          0.00046343376743607223,
          0.0004507647536229342,
          0.0005201385938562453,
          0.0004718532145489007,
          0.0005461816326715052,
          0.00044788329978473485,
          0.00046198308700695634,
          0.00048817432252690196,
          0.00045306788524612784,
          0.00043272326001897454,
          0.0003908256476279348,
          0.0004039877385366708,
          0.0005079352995380759,
          0.0004216572851873934,
          0.00045102203148417175,
          0.0003957519365940243,
          0.000384015089366585,
          0.00039886985905468464,
          0.00042210452375002205,
          0.0003632938314694911,
          0.00043533233110792935,
          0.0003970150137320161,
          0.0004167078295722604,
          0.0004354494158178568,
          0.00038274808321148157,
          0.0004186008882243186,
          0.0003971013647969812,
          0.00037689824239350855,
          0.0004052681615576148,
          0.0004546263662632555,
          0.0004071085131727159,
          0.0003903642063960433,
          0.0004084394022356719,
          0.0003936170251108706,
          0.0003677321074064821,
          0.00037991601857356727,
          0.00038214976666495204,
          0.0004646100278478116,
          0.0003589386760722846,
          0.0003627207479439676,
          0.00039434267091564834,
          0.00040373927913606167,
          0.0004040060448460281,
          0.00041179260006174445,
          0.0003785089938901365,
          0.0003845177998300642,
          0.0003892633249051869,
          0.0003741412656381726,
          0.00039400250534527004,
          0.0003805861051660031,
          0.00037600743235088885,
          0.0004174110945314169,
          0.0003800113045144826,
          0.0003867686027660966,
          0.0003855546237900853,
          0.0004031760327052325,
          0.00039358253707177937,
          0.0003745601570699364,
          0.00035582546843215823,
          0.0003916546702384949,
          0.0003916877321898937,
          0.0003563121135812253,
          0.0003760342951864004,
          0.0004175020440015942,
          0.0003635220928117633,
          0.0003592502907849848,
          0.0003689474251586944,
          0.0003439903084654361,
          0.00037184011307545006,
          0.00036999108851887286,
          0.0004115596238989383,
          0.00036235086736269295,
          0.0003563826612662524,
          0.0003896947018802166,
          0.0003534845891408622,
          0.00036112856469117105,
          0.00032785983057692647,
          0.0003871317603625357,
          0.00038837906322441995,
          0.0004242249997332692,
          0.00042651849798858166,
          0.0004061083891429007,
          0.00037433032412081957,
          0.00039122847374528646,
          0.000378152821213007,
          0.00036709601408801973,
          0.00034680587123148143,
          0.00035670833312906325,
          0.0003496845019981265,
          0.00034602274536155164,
          0.0003493511467240751,
          0.0003795215452555567,
          0.0003158471081405878,
          0.0003545488871168345,
          0.0003420691646169871,
          0.0003107902011834085,
          0.0003749069292098284,
          0.00037615993642248213,
          0.0003417828120291233,
          0.00029813367291353643,
          0.00031271736952476203,
          0.00036298861959949136,
          0.0003388450131751597,
          0.00036795370397157967,
          0.000363698200089857,
          0.0003278107615187764,
          0.0003939565795008093,
          0.0003536193398758769,
          0.0003245033440180123,
          0.0003698917862493545,
          0.000318550766678527,
          0.0003738069499377161,
          0.000358695222530514,
          0.0003383758303243667,
          0.00032992271007969975,
          0.000338120007654652,
          0.0003348801110405475,
          0.0003298525116406381,
          0.00036123357131145895,
          0.0003169950796291232,
          0.00035613152431324124,
          0.00034742383286356926,
          0.0003325600118841976,
          0.0003366698801983148,
          0.00031909398967400193,
          0.00032497962820343673,
          0.00034851388772949576,
          0.00031179445795714855,
          0.00031312424107454717,
          0.00034836484701372683,
          0.0003344774304423481,
          0.0003493803960736841,
          0.00035134301288053393,
          0.0003314119530841708,
          0.00034959038021042943,
          0.00034558906918391585,
          0.0003606906393542886,
          0.00031752747599966824,
          0.0003337202069815248,
          0.00030675262678414583,
          0.00033882472780533135,
          0.0003347211459185928,
          0.0003150228876620531,
          0.00030356855131685734,
          0.00033367681317031384,
          0.0003212492447346449,
          0.00033771878224797547,
          0.00032566668232902884,
          0.0003280985984019935,
          0.0003223445964977145,
          0.00035749649396166205,
          0.00036082821316085756,
          0.00034276104997843504,
          0.0002920613333117217,
          0.00030668493127450347,
          0.0002913878997787833,
          0.0002948374021798372,
          0.0003083539486397058,
          0.0003357551177032292,
          0.0003031652304343879,
          0.00032565335277467966,
          0.00032418736373074353,
          0.0003167399554513395,
          0.00034588464768603444,
          0.0003425746108405292,
          0.0003306487633381039,
          0.00031241061515174806,
          0.00030441005947068334,
          0.00032469333382323384,
          0.00033600773895159364,
          0.0003403713053558022,
          0.0002962471917271614,
          0.0003105321666225791,
          0.00032523981644771993,
          0.00031068173120729625,
          0.0002985866740345955,
          0.0003664168471004814,
          0.0003209879214409739,
          0.00035195695818401873,
          0.00030095025431364775,
          0.0003116726002190262,
          0.00030598329612985253,
          0.00029643531888723373,
          0.0003006007173098624,
          0.0003285215643700212,
          0.00032066748826764524,
          0.00031732048955745995,
          0.0003270714951213449,
          0.0002854962076526135,
          0.0003015564288944006,
          0.0003284107951913029,
          0.00032298159203492105,
          0.0003073418920394033,
          0.00031153412419371307,
          0.00031628369470126927,
          0.0002815141051542014,
          0.00029792817076668143,
          0.00029378393082879484,
          0.0002946821041405201,
          0.0003366293094586581,
          0.00031168386340141296,
          0.0003046200727112591,
          0.00031772200600244105,
          0.0003178641200065613,
          0.0002986924955621362,
          0.0003032625245396048,
          0.00029558397363871336,
          0.00031726816087029874,
          0.00030658539617434144,
          0.00029692729003727436,
          0.00029784269281663,
          0.0003132040146738291,
          0.0003094239509664476,
          0.0003084360796492547,
          0.00029393224394880235,
          0.00028886954532936215,
          0.00027834338834509254,
          0.0003143678477499634,
          0.000295037345495075,
          0.00028343949816189706,
          0.0003208598936907947,
          0.00032259232830256224,
          0.00029789438121952116,
          0.00028421118622645736,
          0.00029985461151227355,
          0.0003294445632491261,
          0.0003232012677472085,
          0.00031048423261381686,
          0.0002948625187855214,
          0.00029652126249857247,
          0.0003200847131665796,
          0.00028149981517344713,
          0.0003001473087351769,
          0.00031478304299525917,
          0.00029139904654584825,
          0.00029441650258377194,
          0.00031290153856389225,
          0.0002858158841263503,
          0.0002796585613396019,
          0.00029860890936106443,
          0.00028860417660325766,
          0.0003185383102390915,
          0.00031596439657732844,
          0.00030069382046349347,
          0.0002857183280866593,
          0.00030611210968345404,
          0.00028885708888992667,
          0.00030680219060741365,
          0.0003109348763246089,
          0.0002947424945887178,
          0.000312957214191556,
          0.00029785060905851424,
          0.00029414810705929995,
          0.0003151238488499075,
          0.00029627405456267297,
          0.00030274741584435105,
          0.0003078470763284713,
          0.00029890736914239824,
          0.0002912504714913666,
          0.0002910515759140253,
          0.0002944020670838654,
          0.0002918499521911144,
          0.0003187158436048776,
          0.0003057202557101846,
          0.00030773418257012963,
          0.0002829072473105043,
          0.0002938334073405713,
          0.0003154363948851824,
          0.00032849449780769646,
          0.0002909816394094378,
          0.00028056715382263064,
          0.0002861409157048911,
          0.00028257473604753613,
          0.0003068429068662226,
          0.0002981149300467223,
          0.0002810491423588246,
          0.00027049003983847797,
          0.0003123983333352953,
          0.0002895540092140436,
          0.00026677033747546375,
          0.00030355533817783,
          0.0002781970251817256,
          0.00028343559824861586,
          0.0002845940471161157,
          0.00028960302006453276,
          0.0002762770454864949,
          0.0002840917732100934,
          0.00028519739862531424,
          0.0002854034537449479,
          0.0002845812414307147,
          0.0002773563319351524,
          0.0002816398919094354,
          0.00028700585244223475,
          0.00030966621125116944,
          0.0002940255799330771,
          0.00029326434014365077,
          0.0002773200976662338,
          0.0002791701990645379,
          0.0002711005217861384,
          0.00028135249158367515,
          0.0002616166020743549,
          0.00030789300217293203,
          0.00029395322781056166,
          0.0003006735641974956,
          0.00031423609470948577,
          0.0002886987349484116,
          0.0002756013418547809,
          0.00026847849949263036,
          0.00026697840075939894,
          0.0002997503324877471,
          0.00025866241776384413,
          0.0002688094100449234,
          0.0002657672739587724,
          0.0002800714282784611,
          0.00029420829378068447,
          0.00026951165637001395,
          0.0002557842817623168,
          0.0002674613206181675,
          0.0002634949632920325,
          0.0002758295158855617,
          0.0002803833340294659,
          0.00029498868389055133,
          0.0002909497416112572,
          0.0002859356172848493,
          0.00032124228891916573,
          0.0002696405572351068,
          0.00028854369884356856,
          0.00027124900952912867,
          0.00026933575281873345,
          0.0002819918154273182,
          0.0002699170436244458,
          0.00026270694797858596,
          0.00027157258591614664,
          0.0002791071019601077,
          0.00027582619804888964,
          0.0003115625004284084,
          0.0002822059323079884,
          0.0002734364534262568,
          0.0002518235123716295,
          0.0002794112078845501,
          0.0002946058812085539,
          0.00024669140111654997,
          0.00029730232199653983,
          0.0002740628260653466,
          0.00028284380096010864,
          0.00028383376775309443,
          0.0002931221097242087,
          0.00026081898249685764,
          0.00028649435262195766,
          0.000310031435219571,
          0.00028167269192636013,
          0.000278326915577054,
          0.0002521270653232932,
          0.00028150147409178317,
          0.0002655634598340839,
          0.000276557810138911,
          0.00028284313157200813,
          0.00025717561948113143,
          0.00029813131550326943,
          0.0002783543604891747,
          0.0002733881410676986,
          0.0002769488201010972,
          0.0002681327168829739,
          0.00028518668841570616,
          0.0002669655659701675,
          0.00024824970751069486,
          0.0002740562485996634,
          0.00031730782939121127,
          0.00025814378750510514,
          0.0002770006249193102,
          0.0003059032605960965,
          0.00028102778014726937,
          0.00026282077305950224,
          0.0002618284779600799,
          0.00029273112886585295,
          0.00028243986889719963,
          0.00028180229128338397,
          0.0002730629057623446,
          0.00027013698127120733,
          0.00025116954930126667,
          0.0002601219166535884,
          0.0002739940828178078,
          0.00026874785544350743,
          0.00027869362384080887,
          0.00028661126270890236,
          0.0002916378143709153,
          0.0002635905984789133,
          0.00024716590996831656,
          0.0002701306948438287,
          0.00026150501798838377,
          0.0002685404906515032,
          0.00026523633277975023,
          0.00024703284725546837,
          0.00027593126287683845,
          0.0002618640137370676,
          0.0002718325413297862,
          0.00028554521850310266,
          0.00027739378856495023,
          0.00027930719079449773,
          0.00028113878215663135,
          0.000275995786068961,
          0.00027259421767666936,
          0.0002760527713689953,
          0.00027470511849969625,
          0.00027787918224930763,
          0.0002863182744476944,
          0.0002908168244175613,
          0.00027279811911284924,
          0.00025812769308686256,
          0.0002614596742205322,
          0.00028565298998728395,
          0.0002749887644313276,
          0.000277126906439662,
          0.00024957224377430975,
          0.0002634133561514318,
          0.000310342205921188,
          0.00027003238210454583,
          0.0002597226994112134,
          0.0002546232135500759,
          0.0002814709150698036,
          0.00025797367561608553,
          0.0002670148096513003,
          0.0002709922264330089,
          0.0002704450162127614,
          0.0002601227315608412,
          0.00026991311460733414,
          0.0002641717146616429,
          0.0002832780301105231,
          0.000263458292465657,
          0.0002568830386735499,
          0.00027620690525509417,
          0.0002491717750672251,
          0.0002724016085267067,
          0.0002804050745908171,
          0.0002750387357082218,
          0.000272879668045789,
          0.00025973800802603364,
          0.0002727227983996272,
          0.00028035134891979396,
          0.0002685153449419886,
          0.0002554409147705883,
          0.0002717088209465146,
          0.0002681327750906348,
          0.0002582130255177617,
          0.0002742705983109772,
          0.0002505106676835567,
          0.0002642111503519118,
          0.0002561095461715013,
          0.00027720790239982307,
          0.0002569111529737711,
          0.00026073597837239504,
          0.00026494127814657986,
          0.00025861989706754684,
          0.0002831395831890404,
          0.00024990548263303936,
          0.0002852357574738562,
          0.0002629114605952054,
          0.00026221253210678697,
          0.000261838169535622,
          0.0002368862769799307,
          0.00025503261713311076,
          0.00026821414940059185,
          0.00028272983036004007,
          0.0002600103907752782,
          0.00026218892890028656,
          0.00027539319125935435,
          0.0002754109154921025,
          0.0002673790149856359,
          0.00025644569541327655,
          0.0002685448562260717,
          0.00025353304226882756,
          0.00025446535437367857,
          0.0002659210585989058,
          0.00025031124823726714,
          0.00027547896024771035,
          0.00026707680081017315,
          0.0002586898917797953,
          0.0002655079879332334,
          0.0002739331393968314,
          0.00026383186923339963,
          0.0002649010275490582,
          0.0002524449082557112,
          0.00026363314827904105,
          0.00026194952079094946,
          0.0002579315914772451,
          0.0002603868779260665,
          0.00026922798133455217,
          0.0002488537866156548,
          0.0002643899933900684,
          0.0002577636041678488,
          0.0002752710715867579,
          0.00025474056019447744,
          0.0002650168025866151,
          0.00024646971723996103,
          0.0002918616228271276,
          0.0002544144226703793,
          0.00025750260101631284,
          0.00024701235815882683,
          0.00024137731816153973,
          0.0002733056608121842,
          0.0002670261892490089,
          0.0002532783546485007,
          0.00026887570857070386,
          0.0002651750110089779,
          0.000281569198705256,
          0.000262068904703483,
          0.00025244682910852134,
          0.0002527400210965425,
          0.0002491501218173653,
          0.0002631910901982337,
          0.0002431593311484903,
          0.0002638195874169469,
          0.0002467298472765833,
          0.00026314202114008367,
          0.00026211573276668787,
          0.00024923484306782484,
          0.00024400834809057415,
          0.0002405436389381066,
          0.00025664386339485645,
          0.0002764312084764242,
          0.00025091436691582203,
          0.00026996087399311364,
          0.00026857180637307465,
          0.0002629679220262915,
          0.00026193217490799725,
          0.0002715626615099609,
          0.00026621733559295535,
          0.0002492325147613883,
          0.00026052258908748627,
          0.00025835182168520987,
          0.00024710907018743455,
          0.0002469080500304699,
          0.0002400128432782367,
          0.0002578657877165824,
          0.0002686763182282448,
          0.0002532013750169426,
          0.0002649855159688741,
          0.00028105275123380125,
          0.0002464991412125528,
          0.0002717209281399846,
          0.00024406691954936832,
          0.00025875744177028537,
          0.0002771996660158038,
          0.00025123514933511615,
          0.00025637572980485857,
          0.0002522375143598765,
          0.00024882276193238795,
          0.0002775233588181436,
          0.00024910212960094213,
          0.00024819414829835296,
          0.00024500078870914876,
          0.00025553343584761024,
          0.0002552240330260247,
          0.00026321905897930264,
          0.0002495876979082823,
          0.00027273010346107185,
          0.00026763242203742266,
          0.0002563713351264596,
          0.0002693059796001762,
          0.0002548586344346404,
          0.0002596427802927792,
          0.0002690944238565862,
          0.00027786329155787826,
          0.00025628856383264065,
          0.0002665325009729713,
          0.00026174806407652795,
          0.00025921579799614847,
          0.00027715694159269333,
          0.0002544241724535823,
          0.00026045629056170583,
          0.00026428716955706477,
          0.00025207450380548835,
          0.00023831342696212232,
          0.0002323172811884433,
          0.00025446657673455775,
          0.00025731997448019683,
          0.00026420250651426613,
          0.0002549219352658838,
          0.00025153084425255656,
          0.00025146364350803196,
          0.0002622864267323166,
          0.00024956444394774735,
          0.0002396327763563022,
          0.00025286420714110136,
          0.0002460691030137241,
          0.0002594932448118925,
          0.00026049456209875643,
          0.00025031185941770673,
          0.00026936447829939425,
          0.0002599147264845669,
          0.0002699672768358141,
          0.00024359657254535705,
          0.00026975784567184746,
          0.00026028143474832177,
          0.00026282379985786974,
          0.0002697673044167459,
          0.0002605541085358709,
          0.0002523375442251563,
          0.0002683225611690432,
          0.00024734222097322345,
          0.00022875152353662997,
          0.00025984455714933574,
          0.0002599580038804561,
          0.0002446766011416912,
          0.0002407361171208322,
          0.00025153998285532,
          0.0002625250199344009,
          0.0002577385166659951,
          0.00025086471578106284,
          0.0002599343715701252,
          0.00024207048409152776,
          0.00025121853104792535,
          0.00025529562844894826,
          0.00023156273528002203,
          0.0002674233401194215,
          0.0002579442225396633,
          0.00025446206564083695,
          0.00025562779046595097,
          0.00024305445549543947,
          0.0002480873663444072,
          0.0002711819251999259,
          0.0002577260893303901,
          0.0002442256372887641,
          0.0002314318553544581,
          0.00025649648159742355,
          0.0002449262246955186,
          0.0002552664082031697,
          0.0002606499765533954,
          0.00025976079632528126,
          0.00027560556191019714,
          0.00025544644449837506,
          0.0002340788923902437,
          0.0002419445081613958,
          0.0002631240349728614,
          0.00027641025371849537,
          0.0002533133374527097,
          0.0002332307049073279,
          0.0002274225262226537,
          0.00024939721333794296,
          0.00024404824944213033,
          0.00026494450867176056,
          0.0002454151108395308,
          0.0002398227370576933,
          0.00025590352015569806,
          0.00024512835079804063,
          0.00025356534752063453,
          0.00025797661510296166,
          0.000251581339398399,
          0.0002423390105832368,
          0.0002651824615895748,
          0.0002589690266177058,
          0.0002561296278145164,
          0.0002386901614954695,
          0.0002530409547034651,
          0.0002434256166452542,
          0.00025034594000317156,
          0.000251627090619877,
          0.0002388663706369698,
          0.00025517246103845537,
          0.0002797234046738595,
          0.0002491440682206303,
          0.0002457749214954674,
          0.00024202506756410003,
          0.0002707009552977979,
          0.00025764209567569196,
          0.0002675015421118587,
          0.00024626817321404815,
          0.00027653068536892533,
          0.00025668161106295884,
          0.0002344650711165741,
          0.0002502801362425089,
          0.00025099364574998617,
          0.00024647184181958437,
          0.0002568296331446618,
          0.00027060843422077596,
          0.00022860668832436204,
          0.0002600806765258312,
          0.00025448191445320845,
          0.0002684459905140102,
          0.00025613437173888087,
          0.00022468068345915526,
          0.0002557364059612155,
          0.0002408525615464896,
          0.00024737557396292686,
          0.00024833681527525187,
          0.00027239820337854326,
          0.00026106316363438964,
          0.0002605667687021196,
          0.0002491935738362372,
          0.00025622366229072213,
          0.0002311970602022484,
          0.00026634358800947666,
          0.00023985911684576422,
          0.00023545823933091015,
          0.00024250484420917928,
          0.0002454269560985267,
          0.00022662665287498385,
          0.00023394235176965594,
          0.0002324985689483583,
          0.00024906705948524177,
          0.00023912756296340376,
          0.00022504244407173246,
          0.00024926799233071506,
          0.00024538589059375226,
          0.00024931845837272704,
          0.00024470355128869414,
          0.000264324335148558,
          0.00027630123076960444,
          0.00024575155111961067,
          0.00026166223688051105,
          0.0002487445017322898,
          0.0002612151438370347,
          0.0002492195344530046,
          0.00024926368496380746,
          0.00026100233662873507,
          0.00024904226302169263,
          0.0002384413091931492,
          0.00023929899907670915,
          0.0002464539429638535,
          0.00023393142328131944,
          0.00024056776601355523,
          0.00023635142133571208,
          0.00024026630853768438,
          0.0002373208262724802,
          0.00024567521177232265,
          0.000256460189120844,
          0.00025722369900904596,
          0.0002503870928194374,
          0.00022997104679234326,
          0.0002868782030418515,
          0.00024291709996759892,
          0.0002466172445565462,
          0.00023897385108284652,
          0.00026432087179273367,
          0.00023497655638493598,
          0.0002567603951320052,
          0.00024366735306102782,
          0.00024669364211149514,
          0.00024942404706962407,
          0.00024343024415429682,
          0.00027392207994125783,
          0.0002598236605990678,
          0.00024873093934729695,
          0.0002469027240294963,
          0.00025067629758268595,
          0.00026215880643576384,
          0.00024619061150588095,
          0.0002625400957185775,
          0.00024434554507024586,
          0.00023120535479392856,
          0.00024237757315859199,
          0.00025581548106856644,
          0.00024243573716375977,
          0.0002379218494752422,
          0.0002560567809268832,
          0.00024191277043428272,
          0.00024411268532276154,
          0.00024356610083486885,
          0.0002469339524395764,
          0.0002502120041754097,
          0.00023378641344606876,
          0.0002531208738218993,
          0.0002522363211028278,
          0.000240276989643462,
          0.00024640659103170037,
          0.00025415897835046053,
          0.0002686017251107842,
          0.0002490335900802165,
          0.0002500758273527026,
          0.0002452803310006857,
          0.0002559484273660928,
          0.0002578962012194097,
          0.00022918057220522314,
          0.0002485872246325016,
          0.00024387461598962545,
          0.00025581137742847204,
          0.00025116989854723215,
          0.0002489544858690351
         ]
        },
        {
         "marker": {
          "color": "rgba(12, 50, 196, 0.6)"
         },
         "name": "Validation Loss",
         "type": "scatter",
         "y": [
          8.565629832446575e-05,
          0.0002124357270076871,
          9.858170960796997e-05,
          0.00015910698857624084,
          0.00029712272225879133,
          0.00045041993143968284,
          0.00022968013945501298,
          0.0001654377265367657,
          0.00012361975677777082,
          5.606915874523111e-05,
          4.187478043604642e-05,
          0.00013276909885462373,
          0.00029463128885254264,
          0.0011797286570072174,
          9.407410107087344e-05,
          3.120176188531332e-05,
          0.00013211817713454366,
          6.673768075415865e-05,
          2.9369415642577223e-05,
          4.873104626312852e-05,
          0.0001285090547753498,
          3.7857382267247885e-05,
          7.815614662831649e-05,
          6.87175925122574e-05,
          3.528159868437797e-05,
          3.111334444838576e-05,
          0.00023408168635796756,
          6.924101035110652e-05,
          0.00020967073214706033,
          0.0003034267865587026,
          0.0001278449926758185,
          5.912925553275272e-05,
          0.00023336084268521518,
          0.00011135116074001417,
          0.00014216371346265078,
          7.347667997237295e-05,
          7.610915781697258e-05,
          0.0002461140393279493,
          3.870811633532867e-05,
          0.0002572139201220125,
          0.00011110192281194031,
          0.00013512045552488416,
          0.00010422080231364816,
          0.00019756495021283627,
          4.692921356763691e-05,
          4.495689790928736e-05,
          0.00015023484593257308,
          4.96964312333148e-05,
          2.6016527044703253e-05,
          8.164119208231568e-05,
          5.077203968539834e-05,
          5.926550511503592e-05,
          9.80759650701657e-05,
          6.398004916263744e-05,
          4.122030077269301e-05,
          8.574368985136971e-05,
          2.685933213797398e-05,
          6.0752310673706234e-05,
          9.745137504069135e-05,
          2.2134287064545788e-05,
          0.00011278536112513393,
          8.088343747658655e-05,
          0.00013134443724993616,
          4.159686795901507e-05,
          3.32725394400768e-05,
          0.00010976577323162928,
          7.552075840067118e-05,
          0.0001328882499365136,
          6.991645932430401e-05,
          2.987867446790915e-05,
          6.084448250476271e-05,
          5.506534580490552e-05,
          9.359996329294518e-05,
          3.753920100280084e-05,
          2.4465829483233392e-05,
          3.895974805345759e-05,
          9.770770702743903e-05,
          6.571220728801563e-05,
          5.441271423478611e-05,
          4.4772914407076314e-05,
          7.295189425349236e-05,
          6.663586100330576e-05,
          6.136963929748163e-05,
          0.0001938822097145021,
          9.329206659458578e-05,
          9.609902917873114e-05,
          0.0001200195329147391,
          2.1955647753202356e-05,
          0.0001373222330585122,
          2.455653520883061e-05,
          2.1340427338145673e-05,
          6.88327636453323e-05,
          0.00011749143595807254,
          2.4569520974182524e-05,
          9.660806972533464e-05,
          8.579044515499845e-05,
          3.3487296605017036e-05,
          3.484450644464232e-05,
          0.00014342456415761262,
          0.00013467820826917887,
          9.041092562256381e-05,
          0.00010739426215877756,
          8.971640636445954e-05,
          0.00015009709750302136,
          2.871081233024597e-05,
          2.5176044800900854e-05,
          0.00012163643987150863,
          3.789693073485978e-05,
          6.08375012234319e-05,
          3.51287962985225e-05,
          2.9903076210757717e-05,
          0.00010245483281323686,
          3.4048112866003066e-05,
          9.549617243465036e-05,
          4.9571619456401095e-05,
          3.243537139496766e-05,
          5.4015996283851564e-05,
          0.00012746320862788707,
          0.00011831063602585346,
          5.8316531067248434e-05,
          3.845745595754124e-05,
          9.826994210015982e-05,
          0.00015693485329393297,
          6.736579962307587e-05,
          3.655980253824964e-05,
          0.000200388312805444,
          0.00010415034194011241,
          0.00012525478086899966,
          3.411584839341231e-05,
          2.105751627823338e-05,
          5.1867598813259974e-05,
          2.709759064600803e-05,
          3.9799004298401996e-05,
          0.00010737573757069185,
          4.962417733622715e-05,
          3.866511178785004e-05,
          2.620428494992666e-05,
          0.00017693015979602933,
          3.1576520996168256e-05,
          4.6757853851886466e-05,
          4.616845035343431e-05,
          2.810208752634935e-05,
          3.0183777198544703e-05,
          3.371220009285025e-05,
          2.81549000646919e-05,
          0.00018935395928565413,
          3.567255407688208e-05,
          5.982264337944798e-05,
          4.6470609959214926e-05,
          3.3819225791376084e-05,
          0.00011853301839437336,
          6.299002416199073e-05,
          1.916982000693679e-05,
          2.6393028747406788e-05,
          5.429571683635004e-05,
          4.64563345303759e-05,
          4.756174166686833e-05,
          5.278486423776485e-05,
          4.9044065235648304e-05,
          2.454115565342363e-05,
          5.549966590479016e-05,
          4.02598780055996e-05,
          3.4791726648109034e-05,
          4.2065614252351224e-05,
          3.0407154554268345e-05,
          2.7882220820174553e-05,
          5.6183667766163126e-05,
          2.1207040845183656e-05,
          3.683195973280817e-05,
          5.032450644648634e-05,
          3.4594257158460096e-05,
          2.8794656827813014e-05,
          0.00012956321006640792,
          4.914155579172075e-05,
          2.2634028937318362e-05,
          2.284298352606129e-05,
          4.575221464619972e-05,
          5.154572500032373e-05,
          4.943875319440849e-05,
          0.00010619309614412487,
          3.695918348967098e-05,
          0.00010968300193781033,
          0.00016242412675637752,
          4.574531703838147e-05,
          7.865066436352208e-05,
          0.00012423659791238606,
          0.0001091111553250812,
          8.832681487547234e-05,
          5.3271345677785575e-05,
          1.956589585461188e-05,
          6.233940075617284e-05,
          2.7580974347074516e-05,
          0.0001800802128855139,
          3.1186129490379244e-05,
          3.307276710984297e-05,
          5.175233309273608e-05,
          2.2017580704414286e-05,
          6.156078597996384e-05,
          4.879596599494107e-05,
          9.565347863826901e-05,
          2.8225007554283366e-05,
          2.3997279640752822e-05,
          2.29838042287156e-05,
          3.5331526305526495e-05,
          1.8567499864730053e-05,
          0.00011680235184030607,
          6.453096284531057e-05,
          4.8793070163810626e-05,
          3.7876972783124074e-05,
          7.053805165924132e-05,
          9.395460074301809e-05,
          2.9474434995790944e-05,
          2.756889989541378e-05,
          3.842866135528311e-05,
          5.5001055443426594e-05,
          2.9560940674855374e-05,
          5.6794156989781186e-05,
          3.220768485334702e-05,
          3.8298789149848744e-05,
          4.129614171688445e-05,
          2.3904736735858023e-05,
          5.561265425058082e-05,
          6.071229654480703e-05,
          8.654940029373392e-05,
          0.00010257224494125694,
          2.1358198864618316e-05,
          3.725515125552192e-05,
          3.578709947760217e-05,
          8.13917868072167e-05,
          8.093058568192646e-05,
          2.6621539291227236e-05,
          2.6949617677018978e-05,
          1.846626582846511e-05,
          7.07710423739627e-05,
          7.142821414163336e-05,
          1.8173539501731284e-05,
          4.975156844011508e-05,
          4.001760316896252e-05,
          2.1879041014472023e-05,
          2.0801264327019453e-05,
          2.3824633899494074e-05,
          3.9679005567450076e-05,
          7.621465920237824e-05,
          1.9798892026301473e-05,
          0.00013199458771850914,
          7.616256334586069e-05,
          4.5105014578439295e-05,
          3.772281343117356e-05,
          0.00022605359845329076,
          2.3762177079333924e-05,
          2.0784120351891033e-05,
          2.5410115995327942e-05,
          3.0575749406125396e-05,
          3.6057848774362355e-05,
          0.00010116296471096575,
          3.643791569629684e-05,
          2.5651746909716167e-05,
          2.853722071449738e-05,
          7.744081813143566e-05,
          2.4629325707792304e-05,
          3.3785850973799825e-05,
          0.00020578330440912396,
          3.712709076353349e-05,
          7.122810347937047e-05,
          0.00010762923193397,
          2.1163081328268163e-05,
          3.408877819310874e-05,
          2.639150079630781e-05,
          2.9454347895807587e-05,
          6.0315454902593046e-05,
          0.0001075823965948075,
          5.0489466957515106e-05,
          2.655361640790943e-05,
          2.4747701900196262e-05,
          2.6011617592303082e-05,
          3.481542080407962e-05,
          4.7455356252612546e-05,
          4.046329195261933e-05,
          1.9190347302355804e-05,
          2.660718746483326e-05,
          5.1164028263883665e-05,
          2.3158525436883792e-05,
          2.2477568563772365e-05,
          5.940558185102418e-05,
          4.0129994886228815e-05,
          4.7892197471810505e-05,
          4.7875797463348135e-05,
          2.9925246053608134e-05,
          7.599453965667635e-05,
          2.7408259484218433e-05,
          1.7013206161209382e-05,
          3.0014440199011005e-05,
          2.7070849682786502e-05,
          3.244465187890455e-05,
          3.1685223802924156e-05,
          2.535285966587253e-05,
          3.1898329325485975e-05,
          4.973291652277112e-05,
          2.519977351767011e-05,
          2.6660991352400742e-05,
          3.697801730595529e-05,
          2.71208136837231e-05,
          2.7004090952686965e-05,
          5.14006387675181e-05,
          6.881979788886383e-05,
          4.250994970789179e-05,
          1.753770084178541e-05,
          4.137698851991445e-05,
          3.835811003227718e-05,
          6.673141615465283e-05,
          3.0036098905839026e-05,
          3.0509783755405806e-05,
          3.687398202600889e-05,
          4.2196155845886096e-05,
          3.1241492251865566e-05,
          3.1335912353824824e-05,
          3.41663726430852e-05,
          2.478614987921901e-05,
          2.001390930672642e-05,
          3.5885961551684886e-05,
          2.2491212803288363e-05,
          1.900387906061951e-05,
          0.00013750935613643378,
          4.028587136417627e-05,
          2.222388138761744e-05,
          6.372083589667454e-05,
          0.00010357700375607237,
          5.507225068868138e-05,
          3.0315053663798608e-05,
          2.0140450942562893e-05,
          2.9953620469314046e-05,
          7.691358041483909e-05,
          2.9193584850872867e-05,
          2.2058824470150284e-05,
          4.1403749492019415e-05,
          1.893310582090635e-05,
          2.250772013212554e-05,
          3.950297468691133e-05,
          3.82373109459877e-05,
          4.189658284303732e-05,
          4.73648397019133e-05,
          3.4041007893392816e-05,
          2.1126943465787917e-05,
          6.212298467289656e-05,
          2.230815516668372e-05,
          3.3495278330519795e-05,
          6.102339466451667e-05,
          1.7419954019715078e-05,
          3.0798499210504815e-05,
          4.306398841436021e-05,
          8.807684935163707e-05,
          2.118788870575372e-05,
          1.763156797096599e-05,
          2.9987826565047726e-05,
          3.1269013561541215e-05,
          5.436034552985802e-05,
          3.753026612685062e-05,
          4.796664870809764e-05,
          3.175484380335547e-05,
          2.7636444428935647e-05,
          2.0221947124809958e-05,
          2.6869185603572987e-05,
          1.7345044398098253e-05,
          2.279181353515014e-05,
          6.801389827160165e-05,
          4.750029256683774e-05,
          3.296108843642287e-05,
          3.504639244056307e-05,
          3.661972368718125e-05,
          2.5979526981245726e-05,
          3.0514764148392715e-05,
          0.00010974291944876313,
          3.5091779864160344e-05,
          3.5696175473276526e-05,
          3.485930210445076e-05,
          3.98134725401178e-05,
          2.743827826634515e-05,
          3.2478284992976114e-05,
          2.5314173399237916e-05,
          0.00010253425716655329,
          3.206089240848087e-05,
          3.415909668547101e-05,
          2.277283238072414e-05,
          2.068768662866205e-05,
          0.0001183462591143325,
          2.6859381250687875e-05,
          3.47112727467902e-05,
          3.256760101066902e-05,
          2.6958399757859297e-05,
          2.3448914362234063e-05,
          2.7557713110581972e-05,
          2.283747926412616e-05,
          2.393882823525928e-05,
          6.535838474519551e-05,
          6.1496946727857e-05,
          1.5916584743536077e-05,
          3.556343654054217e-05,
          0.00010024044604506344,
          4.244852971169166e-05,
          2.3969972971826792e-05,
          2.0724301066366024e-05,
          4.139545853831805e-05,
          4.703173544839956e-05,
          7.450184784829617e-05,
          2.7292006052448414e-05,
          3.48407011188101e-05,
          5.321617936715484e-05,
          9.48180168052204e-05,
          2.698788739508018e-05,
          2.580255204520654e-05,
          2.046195004368201e-05,
          5.4495914810104296e-05,
          2.5214616471203044e-05,
          2.6331426852266304e-05,
          2.8997923436691053e-05,
          9.10259986994788e-05,
          2.606258203741163e-05,
          5.470815085573122e-05,
          8.649566734675318e-05,
          3.107872180407867e-05,
          2.369259345869068e-05,
          2.1854830265510827e-05,
          4.937154153594747e-05,
          8.509160397807136e-05,
          1.7248987205675803e-05,
          3.067794386879541e-05,
          2.698904791031964e-05,
          2.5242212359444238e-05,
          1.8827207895810716e-05,
          1.881317984953057e-05,
          2.8760732675436884e-05,
          4.3225398258073255e-05,
          3.263455437263474e-05,
          1.9321920262882486e-05,
          2.0466641217353754e-05,
          2.5478113457211293e-05,
          3.0262985092122108e-05,
          2.2452821212937124e-05,
          1.6137288184836507e-05,
          3.3303604141110554e-05,
          2.0143979782005772e-05,
          7.740066212136298e-05,
          1.851236811489798e-05,
          3.657671550172381e-05,
          2.8574158932315186e-05,
          3.915280831279233e-05,
          4.90489874209743e-05,
          0.00012156098819104955,
          4.368153167888522e-05,
          2.9985400033183396e-05,
          2.686557490960695e-05,
          2.9944658308522776e-05,
          4.727784835267812e-05,
          2.208029036410153e-05,
          2.9254752007545903e-05,
          2.6000308935181238e-05,
          2.285994378325995e-05,
          1.9304266970721073e-05,
          2.1631018171319738e-05,
          3.0109988074400462e-05,
          3.2704134355299175e-05,
          1.6268433682853356e-05,
          7.920341158751398e-05,
          1.999641244765371e-05,
          3.11108015011996e-05,
          2.3356067686108872e-05,
          5.143678208696656e-05,
          3.569350155885331e-05,
          1.6888478057808243e-05,
          2.862872861442156e-05,
          4.0158542105928063e-05,
          8.626228373032063e-05,
          6.638665945501998e-05,
          1.9086748579866253e-05,
          3.967648808611557e-05,
          2.8130893042543903e-05,
          2.0778043108293787e-05,
          3.136722443741746e-05,
          2.273961217724718e-05,
          4.425509905559011e-05,
          1.8284747056895867e-05,
          2.033251257671509e-05,
          0.00011789702693931758,
          1.8309230654267594e-05,
          2.0629255232051946e-05,
          2.8107182515668683e-05,
          2.6057017748826183e-05,
          1.6505711755598895e-05,
          1.5097753021109384e-05,
          2.7174306524102576e-05,
          1.769821574271191e-05,
          4.7424760850844905e-05,
          3.2312978873960674e-05,
          2.2082534997025505e-05,
          2.4479348212480545e-05,
          1.945629992405884e-05,
          2.6767247618408874e-05,
          2.0165758542134427e-05,
          2.6966736186295748e-05,
          4.713184534921311e-05,
          3.7490393879124895e-05,
          2.062515886791516e-05,
          4.3769090552814305e-05,
          5.005416096537374e-05,
          3.7793961382703856e-05,
          1.6612866602372378e-05,
          4.2212355765514076e-05,
          4.111230737180449e-05,
          2.6394754968350753e-05,
          3.526778527884744e-05,
          4.369657835923135e-05,
          2.02440787688829e-05,
          6.842547009000555e-05,
          1.805547799449414e-05,
          2.543875052651856e-05,
          1.998547486437019e-05,
          2.5891660698107444e-05,
          3.472386742942035e-05,
          2.7203113859286532e-05,
          2.879063322325237e-05,
          2.7274289095657878e-05,
          4.182774864602834e-05,
          2.624387343530543e-05,
          2.7261105060460977e-05,
          1.7913616829901002e-05,
          3.630462379078381e-05,
          2.852086072380189e-05,
          2.3286416762857698e-05,
          1.8245000319438986e-05,
          2.1013360310462303e-05,
          2.8391168598318473e-05,
          1.672417602094356e-05,
          2.2910953703103587e-05,
          2.9772334528388456e-05,
          2.0070147002115846e-05,
          3.596806709538214e-05,
          4.160537355346605e-05,
          2.359238533244934e-05,
          2.9147548048058525e-05,
          1.7629517969908193e-05,
          3.136053419439122e-05,
          3.8427995605161414e-05,
          3.1811028748052195e-05,
          6.586340168723837e-05,
          1.85936860361835e-05,
          3.9143546018749475e-05,
          3.9070320781320333e-05,
          1.7679058146313764e-05,
          2.0885376216028817e-05,
          2.66618408204522e-05,
          2.910107832576614e-05,
          2.4238110199803486e-05,
          6.83664547977969e-05,
          2.2822534447186626e-05,
          2.308242801518645e-05,
          1.877241811598651e-05,
          3.688703145598993e-05,
          6.653749733231962e-05,
          3.620249481173232e-05,
          1.8529628505348228e-05,
          0.00011566682223929092,
          4.578989319270477e-05,
          7.371134415734559e-05,
          4.073453237651847e-05,
          1.4797996300330851e-05,
          4.087312117917463e-05,
          1.9589635485317558e-05,
          2.9797680326737463e-05,
          5.4997224651742727e-05,
          3.754808130906895e-05,
          1.9271215933258645e-05,
          7.021961209829897e-05,
          3.0327097192639485e-05,
          2.0660248992498964e-05,
          2.2384188923751935e-05,
          2.516818858566694e-05,
          8.208634972106665e-05,
          4.737781637231819e-05,
          2.6284909836249426e-05,
          3.283710248069838e-05,
          1.7418376955902204e-05,
          1.8698287021834403e-05,
          2.4884215235942975e-05,
          2.544987728470005e-05,
          8.089411130640656e-05,
          2.069419679173734e-05,
          3.109970930381678e-05,
          2.6273797629983164e-05,
          3.6158220609650016e-05,
          2.6783012799569406e-05,
          3.0389295716304332e-05,
          2.1379339159466326e-05,
          2.7817919544759206e-05,
          2.319100713066291e-05,
          1.9017137674381956e-05,
          1.731298652885016e-05,
          2.878507257264573e-05,
          3.343107528053224e-05,
          1.7902726540341973e-05,
          6.486514757853001e-05,
          5.136028994456865e-05,
          2.601297092041932e-05,
          5.883751146029681e-05,
          1.4438823200180195e-05,
          2.7892898287973367e-05,
          2.1948328139842488e-05,
          2.370263246120885e-05,
          1.751565832819324e-05,
          3.4941222111228853e-05,
          1.3676854905497748e-05,
          7.84471703809686e-05,
          1.6772302842582576e-05,
          1.7279448002227582e-05,
          2.5383375032106414e-05,
          2.0821989892283455e-05,
          1.8840384655050002e-05,
          4.214445289107971e-05,
          3.6229524994269013e-05,
          2.4096734705381095e-05,
          2.846054849214852e-05,
          2.2769263523514383e-05,
          7.093072781572118e-05,
          1.7514117644168437e-05,
          4.443035868462175e-05,
          2.679801036720164e-05,
          2.1499614376807585e-05,
          2.9545888537541032e-05,
          2.3073045667842962e-05,
          2.981866055051796e-05,
          2.5152001398964785e-05,
          1.6001456970116124e-05,
          2.9918463042122312e-05,
          4.802851617569104e-05,
          1.7833359379437752e-05,
          4.1321003664052114e-05,
          1.9298793631605804e-05,
          3.951615508412942e-05,
          2.2944968804949895e-05,
          4.559136141324416e-05,
          2.0415995095390826e-05,
          3.387724427739158e-05,
          4.894091034657322e-05,
          2.7353627956472337e-05,
          2.0339415641501546e-05,
          1.984074515348766e-05,
          2.0019355361000635e-05,
          1.7563554138178006e-05,
          1.5644047380192205e-05,
          3.421769361011684e-05,
          6.210523133631796e-05,
          1.5955714843585156e-05,
          2.4864992155926302e-05,
          3.707701398525387e-05,
          1.964396687981207e-05,
          2.1741110685979947e-05,
          4.827284283237532e-05,
          2.5155253752018325e-05,
          5.355172834242694e-05,
          2.7335743652656674e-05,
          2.383524042670615e-05,
          3.723378904396668e-05,
          1.7537158782943152e-05,
          2.4349978048121557e-05,
          4.4126536522526294e-05,
          9.678253263700753e-05,
          1.688095653662458e-05,
          2.1436047973111272e-05,
          2.4365730496356264e-05,
          1.8813609131029807e-05,
          1.6872299966053106e-05,
          1.4595869288314134e-05,
          2.1200352421146818e-05,
          2.5030441975104623e-05,
          1.6208758097491227e-05,
          1.5038845049275551e-05,
          2.1427242245408706e-05,
          0.00011051631736336276,
          1.942897870321758e-05,
          5.1630227972054854e-05,
          1.877437171060592e-05,
          2.569342359493021e-05,
          3.142071727779694e-05,
          2.7477017283672467e-05,
          2.2295558665064164e-05,
          5.709718243451789e-05,
          2.513145955163054e-05,
          1.6836755094118416e-05,
          6.600224878638983e-05,
          1.8062293747789226e-05,
          2.0202320229145698e-05,
          2.200198832724709e-05,
          1.6537147530470975e-05,
          2.7784306439571083e-05,
          1.811277434171643e-05,
          2.6054734917124733e-05,
          5.1732113206526265e-05,
          1.5813446225365624e-05,
          3.252684837207198e-05,
          2.218468398496043e-05,
          1.576482281961944e-05,
          3.541017576935701e-05,
          3.078522422583774e-05,
          2.9026774427620694e-05,
          1.664236515352968e-05,
          2.237804619653616e-05,
          1.8273984096595086e-05,
          1.541442543384619e-05,
          2.2545631509274244e-05,
          2.4309076252393425e-05,
          2.4550839953008108e-05,
          2.8526021196739748e-05,
          2.6416799300932325e-05,
          3.056857894989662e-05,
          2.5920522602973506e-05,
          1.7566349924891256e-05,
          2.7434134608483873e-05,
          2.792850136756897e-05,
          1.5419060218846425e-05,
          2.6417656044941396e-05,
          3.100097455899231e-05,
          2.402858626737725e-05,
          1.629593134566676e-05,
          3.906566053046845e-05,
          2.578787643869873e-05,
          2.0999243133701384e-05,
          3.671814556582831e-05,
          3.8017398765077814e-05,
          6.0974267398705706e-05,
          1.4808034393354319e-05,
          4.950224683852866e-05,
          2.0040783056174405e-05,
          2.0271387256798334e-05,
          2.2835070922155865e-05,
          1.6212792615988292e-05,
          1.9603259715950117e-05,
          1.5378436728497036e-05,
          1.2843770491599571e-05,
          3.138781903544441e-05,
          1.480887658544816e-05,
          1.8941496819024906e-05,
          2.496761408110615e-05,
          1.431891450920375e-05,
          1.7927797671291046e-05,
          2.6000610887422226e-05,
          7.30841638869606e-05,
          2.3876846171333455e-05,
          2.7321380912326276e-05,
          3.137419480481185e-05,
          3.923328404198401e-05,
          4.222771531203762e-05,
          2.9604128940263763e-05,
          3.373326035216451e-05,
          2.8929118343512528e-05,
          2.3218130081659183e-05,
          1.7954489521798678e-05,
          2.1733130779466592e-05,
          3.8339494494721293e-05,
          4.186870501143858e-05,
          2.5797389753279276e-05,
          1.847266685217619e-05,
          2.4931014195317402e-05,
          3.57203753083013e-05,
          6.076766294427216e-05,
          1.643735231482424e-05,
          2.325252535229083e-05,
          2.149335341528058e-05,
          3.6703338992083445e-05,
          5.770440475316718e-05,
          2.613326614664402e-05,
          2.3400596546707675e-05,
          5.99273043917492e-05,
          2.514658081054222e-05,
          1.9192475519957952e-05,
          2.3393164156004786e-05,
          2.0019446310470812e-05,
          2.474879329383839e-05,
          2.8278849640628323e-05,
          2.371855589444749e-05,
          2.6452415113453753e-05,
          2.788631354633253e-05,
          6.256964115891606e-05,
          2.665391548362095e-05,
          2.2002524929121137e-05,
          1.846314080466982e-05,
          5.578904165304266e-05,
          1.776039061951451e-05,
          4.364645428722724e-05,
          3.891254527843557e-05,
          1.7431011656299233e-05,
          1.8169570466852747e-05,
          3.0184337447280996e-05,
          4.7106219426495954e-05,
          2.54523656622041e-05,
          2.2957767214393243e-05,
          5.0384252972435206e-05,
          1.6757194316596724e-05,
          2.479686008882709e-05,
          3.102696791756898e-05,
          1.9328423150000162e-05,
          9.023415623232722e-05
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Loss and Val_Loss in 500 Epochs"
        },
        "xaxis": {
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "title": {
          "text": "Loss"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"47340d6c-05eb-4c50-8fa4-1944aa6ba5d0\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    window.PLOTLYENV.BASE_URL='https://plot.ly';                                    if (document.getElementById(\"47340d6c-05eb-4c50-8fa4-1944aa6ba5d0\")) {                    Plotly.newPlot(                        \"47340d6c-05eb-4c50-8fa4-1944aa6ba5d0\",                        [{\"marker\":{\"color\":\"rgba(171, 50, 96, 0.6)\"},\"name\":\"Training Loss\",\"type\":\"scatter\",\"y\":[0.0042586796917021275,0.001177028869278729,0.0010004604700952768,0.0009694965556263924,0.0007988097495399415,0.0007622056873515248,0.0007056854083202779,0.0009216227917931974,0.0006936224526725709,0.0006542205810546875,0.0006248260615393519,0.0005697489250451326,0.000617933867033571,0.0006247652345336974,0.0005484373541548848,0.0005134979146532714,0.0004744995676446706,0.00048481131670996547,0.0004911749274469912,0.00043616004404611886,0.00048666365910321474,0.00044511008309200406,0.00046343376743607223,0.0004507647536229342,0.0005201385938562453,0.0004718532145489007,0.0005461816326715052,0.00044788329978473485,0.00046198308700695634,0.00048817432252690196,0.00045306788524612784,0.00043272326001897454,0.0003908256476279348,0.0004039877385366708,0.0005079352995380759,0.0004216572851873934,0.00045102203148417175,0.0003957519365940243,0.000384015089366585,0.00039886985905468464,0.00042210452375002205,0.0003632938314694911,0.00043533233110792935,0.0003970150137320161,0.0004167078295722604,0.0004354494158178568,0.00038274808321148157,0.0004186008882243186,0.0003971013647969812,0.00037689824239350855,0.0004052681615576148,0.0004546263662632555,0.0004071085131727159,0.0003903642063960433,0.0004084394022356719,0.0003936170251108706,0.0003677321074064821,0.00037991601857356727,0.00038214976666495204,0.0004646100278478116,0.0003589386760722846,0.0003627207479439676,0.00039434267091564834,0.00040373927913606167,0.0004040060448460281,0.00041179260006174445,0.0003785089938901365,0.0003845177998300642,0.0003892633249051869,0.0003741412656381726,0.00039400250534527004,0.0003805861051660031,0.00037600743235088885,0.0004174110945314169,0.0003800113045144826,0.0003867686027660966,0.0003855546237900853,0.0004031760327052325,0.00039358253707177937,0.0003745601570699364,0.00035582546843215823,0.0003916546702384949,0.0003916877321898937,0.0003563121135812253,0.0003760342951864004,0.0004175020440015942,0.0003635220928117633,0.0003592502907849848,0.0003689474251586944,0.0003439903084654361,0.00037184011307545006,0.00036999108851887286,0.0004115596238989383,0.00036235086736269295,0.0003563826612662524,0.0003896947018802166,0.0003534845891408622,0.00036112856469117105,0.00032785983057692647,0.0003871317603625357,0.00038837906322441995,0.0004242249997332692,0.00042651849798858166,0.0004061083891429007,0.00037433032412081957,0.00039122847374528646,0.000378152821213007,0.00036709601408801973,0.00034680587123148143,0.00035670833312906325,0.0003496845019981265,0.00034602274536155164,0.0003493511467240751,0.0003795215452555567,0.0003158471081405878,0.0003545488871168345,0.0003420691646169871,0.0003107902011834085,0.0003749069292098284,0.00037615993642248213,0.0003417828120291233,0.00029813367291353643,0.00031271736952476203,0.00036298861959949136,0.0003388450131751597,0.00036795370397157967,0.000363698200089857,0.0003278107615187764,0.0003939565795008093,0.0003536193398758769,0.0003245033440180123,0.0003698917862493545,0.000318550766678527,0.0003738069499377161,0.000358695222530514,0.0003383758303243667,0.00032992271007969975,0.000338120007654652,0.0003348801110405475,0.0003298525116406381,0.00036123357131145895,0.0003169950796291232,0.00035613152431324124,0.00034742383286356926,0.0003325600118841976,0.0003366698801983148,0.00031909398967400193,0.00032497962820343673,0.00034851388772949576,0.00031179445795714855,0.00031312424107454717,0.00034836484701372683,0.0003344774304423481,0.0003493803960736841,0.00035134301288053393,0.0003314119530841708,0.00034959038021042943,0.00034558906918391585,0.0003606906393542886,0.00031752747599966824,0.0003337202069815248,0.00030675262678414583,0.00033882472780533135,0.0003347211459185928,0.0003150228876620531,0.00030356855131685734,0.00033367681317031384,0.0003212492447346449,0.00033771878224797547,0.00032566668232902884,0.0003280985984019935,0.0003223445964977145,0.00035749649396166205,0.00036082821316085756,0.00034276104997843504,0.0002920613333117217,0.00030668493127450347,0.0002913878997787833,0.0002948374021798372,0.0003083539486397058,0.0003357551177032292,0.0003031652304343879,0.00032565335277467966,0.00032418736373074353,0.0003167399554513395,0.00034588464768603444,0.0003425746108405292,0.0003306487633381039,0.00031241061515174806,0.00030441005947068334,0.00032469333382323384,0.00033600773895159364,0.0003403713053558022,0.0002962471917271614,0.0003105321666225791,0.00032523981644771993,0.00031068173120729625,0.0002985866740345955,0.0003664168471004814,0.0003209879214409739,0.00035195695818401873,0.00030095025431364775,0.0003116726002190262,0.00030598329612985253,0.00029643531888723373,0.0003006007173098624,0.0003285215643700212,0.00032066748826764524,0.00031732048955745995,0.0003270714951213449,0.0002854962076526135,0.0003015564288944006,0.0003284107951913029,0.00032298159203492105,0.0003073418920394033,0.00031153412419371307,0.00031628369470126927,0.0002815141051542014,0.00029792817076668143,0.00029378393082879484,0.0002946821041405201,0.0003366293094586581,0.00031168386340141296,0.0003046200727112591,0.00031772200600244105,0.0003178641200065613,0.0002986924955621362,0.0003032625245396048,0.00029558397363871336,0.00031726816087029874,0.00030658539617434144,0.00029692729003727436,0.00029784269281663,0.0003132040146738291,0.0003094239509664476,0.0003084360796492547,0.00029393224394880235,0.00028886954532936215,0.00027834338834509254,0.0003143678477499634,0.000295037345495075,0.00028343949816189706,0.0003208598936907947,0.00032259232830256224,0.00029789438121952116,0.00028421118622645736,0.00029985461151227355,0.0003294445632491261,0.0003232012677472085,0.00031048423261381686,0.0002948625187855214,0.00029652126249857247,0.0003200847131665796,0.00028149981517344713,0.0003001473087351769,0.00031478304299525917,0.00029139904654584825,0.00029441650258377194,0.00031290153856389225,0.0002858158841263503,0.0002796585613396019,0.00029860890936106443,0.00028860417660325766,0.0003185383102390915,0.00031596439657732844,0.00030069382046349347,0.0002857183280866593,0.00030611210968345404,0.00028885708888992667,0.00030680219060741365,0.0003109348763246089,0.0002947424945887178,0.000312957214191556,0.00029785060905851424,0.00029414810705929995,0.0003151238488499075,0.00029627405456267297,0.00030274741584435105,0.0003078470763284713,0.00029890736914239824,0.0002912504714913666,0.0002910515759140253,0.0002944020670838654,0.0002918499521911144,0.0003187158436048776,0.0003057202557101846,0.00030773418257012963,0.0002829072473105043,0.0002938334073405713,0.0003154363948851824,0.00032849449780769646,0.0002909816394094378,0.00028056715382263064,0.0002861409157048911,0.00028257473604753613,0.0003068429068662226,0.0002981149300467223,0.0002810491423588246,0.00027049003983847797,0.0003123983333352953,0.0002895540092140436,0.00026677033747546375,0.00030355533817783,0.0002781970251817256,0.00028343559824861586,0.0002845940471161157,0.00028960302006453276,0.0002762770454864949,0.0002840917732100934,0.00028519739862531424,0.0002854034537449479,0.0002845812414307147,0.0002773563319351524,0.0002816398919094354,0.00028700585244223475,0.00030966621125116944,0.0002940255799330771,0.00029326434014365077,0.0002773200976662338,0.0002791701990645379,0.0002711005217861384,0.00028135249158367515,0.0002616166020743549,0.00030789300217293203,0.00029395322781056166,0.0003006735641974956,0.00031423609470948577,0.0002886987349484116,0.0002756013418547809,0.00026847849949263036,0.00026697840075939894,0.0002997503324877471,0.00025866241776384413,0.0002688094100449234,0.0002657672739587724,0.0002800714282784611,0.00029420829378068447,0.00026951165637001395,0.0002557842817623168,0.0002674613206181675,0.0002634949632920325,0.0002758295158855617,0.0002803833340294659,0.00029498868389055133,0.0002909497416112572,0.0002859356172848493,0.00032124228891916573,0.0002696405572351068,0.00028854369884356856,0.00027124900952912867,0.00026933575281873345,0.0002819918154273182,0.0002699170436244458,0.00026270694797858596,0.00027157258591614664,0.0002791071019601077,0.00027582619804888964,0.0003115625004284084,0.0002822059323079884,0.0002734364534262568,0.0002518235123716295,0.0002794112078845501,0.0002946058812085539,0.00024669140111654997,0.00029730232199653983,0.0002740628260653466,0.00028284380096010864,0.00028383376775309443,0.0002931221097242087,0.00026081898249685764,0.00028649435262195766,0.000310031435219571,0.00028167269192636013,0.000278326915577054,0.0002521270653232932,0.00028150147409178317,0.0002655634598340839,0.000276557810138911,0.00028284313157200813,0.00025717561948113143,0.00029813131550326943,0.0002783543604891747,0.0002733881410676986,0.0002769488201010972,0.0002681327168829739,0.00028518668841570616,0.0002669655659701675,0.00024824970751069486,0.0002740562485996634,0.00031730782939121127,0.00025814378750510514,0.0002770006249193102,0.0003059032605960965,0.00028102778014726937,0.00026282077305950224,0.0002618284779600799,0.00029273112886585295,0.00028243986889719963,0.00028180229128338397,0.0002730629057623446,0.00027013698127120733,0.00025116954930126667,0.0002601219166535884,0.0002739940828178078,0.00026874785544350743,0.00027869362384080887,0.00028661126270890236,0.0002916378143709153,0.0002635905984789133,0.00024716590996831656,0.0002701306948438287,0.00026150501798838377,0.0002685404906515032,0.00026523633277975023,0.00024703284725546837,0.00027593126287683845,0.0002618640137370676,0.0002718325413297862,0.00028554521850310266,0.00027739378856495023,0.00027930719079449773,0.00028113878215663135,0.000275995786068961,0.00027259421767666936,0.0002760527713689953,0.00027470511849969625,0.00027787918224930763,0.0002863182744476944,0.0002908168244175613,0.00027279811911284924,0.00025812769308686256,0.0002614596742205322,0.00028565298998728395,0.0002749887644313276,0.000277126906439662,0.00024957224377430975,0.0002634133561514318,0.000310342205921188,0.00027003238210454583,0.0002597226994112134,0.0002546232135500759,0.0002814709150698036,0.00025797367561608553,0.0002670148096513003,0.0002709922264330089,0.0002704450162127614,0.0002601227315608412,0.00026991311460733414,0.0002641717146616429,0.0002832780301105231,0.000263458292465657,0.0002568830386735499,0.00027620690525509417,0.0002491717750672251,0.0002724016085267067,0.0002804050745908171,0.0002750387357082218,0.000272879668045789,0.00025973800802603364,0.0002727227983996272,0.00028035134891979396,0.0002685153449419886,0.0002554409147705883,0.0002717088209465146,0.0002681327750906348,0.0002582130255177617,0.0002742705983109772,0.0002505106676835567,0.0002642111503519118,0.0002561095461715013,0.00027720790239982307,0.0002569111529737711,0.00026073597837239504,0.00026494127814657986,0.00025861989706754684,0.0002831395831890404,0.00024990548263303936,0.0002852357574738562,0.0002629114605952054,0.00026221253210678697,0.000261838169535622,0.0002368862769799307,0.00025503261713311076,0.00026821414940059185,0.00028272983036004007,0.0002600103907752782,0.00026218892890028656,0.00027539319125935435,0.0002754109154921025,0.0002673790149856359,0.00025644569541327655,0.0002685448562260717,0.00025353304226882756,0.00025446535437367857,0.0002659210585989058,0.00025031124823726714,0.00027547896024771035,0.00026707680081017315,0.0002586898917797953,0.0002655079879332334,0.0002739331393968314,0.00026383186923339963,0.0002649010275490582,0.0002524449082557112,0.00026363314827904105,0.00026194952079094946,0.0002579315914772451,0.0002603868779260665,0.00026922798133455217,0.0002488537866156548,0.0002643899933900684,0.0002577636041678488,0.0002752710715867579,0.00025474056019447744,0.0002650168025866151,0.00024646971723996103,0.0002918616228271276,0.0002544144226703793,0.00025750260101631284,0.00024701235815882683,0.00024137731816153973,0.0002733056608121842,0.0002670261892490089,0.0002532783546485007,0.00026887570857070386,0.0002651750110089779,0.000281569198705256,0.000262068904703483,0.00025244682910852134,0.0002527400210965425,0.0002491501218173653,0.0002631910901982337,0.0002431593311484903,0.0002638195874169469,0.0002467298472765833,0.00026314202114008367,0.00026211573276668787,0.00024923484306782484,0.00024400834809057415,0.0002405436389381066,0.00025664386339485645,0.0002764312084764242,0.00025091436691582203,0.00026996087399311364,0.00026857180637307465,0.0002629679220262915,0.00026193217490799725,0.0002715626615099609,0.00026621733559295535,0.0002492325147613883,0.00026052258908748627,0.00025835182168520987,0.00024710907018743455,0.0002469080500304699,0.0002400128432782367,0.0002578657877165824,0.0002686763182282448,0.0002532013750169426,0.0002649855159688741,0.00028105275123380125,0.0002464991412125528,0.0002717209281399846,0.00024406691954936832,0.00025875744177028537,0.0002771996660158038,0.00025123514933511615,0.00025637572980485857,0.0002522375143598765,0.00024882276193238795,0.0002775233588181436,0.00024910212960094213,0.00024819414829835296,0.00024500078870914876,0.00025553343584761024,0.0002552240330260247,0.00026321905897930264,0.0002495876979082823,0.00027273010346107185,0.00026763242203742266,0.0002563713351264596,0.0002693059796001762,0.0002548586344346404,0.0002596427802927792,0.0002690944238565862,0.00027786329155787826,0.00025628856383264065,0.0002665325009729713,0.00026174806407652795,0.00025921579799614847,0.00027715694159269333,0.0002544241724535823,0.00026045629056170583,0.00026428716955706477,0.00025207450380548835,0.00023831342696212232,0.0002323172811884433,0.00025446657673455775,0.00025731997448019683,0.00026420250651426613,0.0002549219352658838,0.00025153084425255656,0.00025146364350803196,0.0002622864267323166,0.00024956444394774735,0.0002396327763563022,0.00025286420714110136,0.0002460691030137241,0.0002594932448118925,0.00026049456209875643,0.00025031185941770673,0.00026936447829939425,0.0002599147264845669,0.0002699672768358141,0.00024359657254535705,0.00026975784567184746,0.00026028143474832177,0.00026282379985786974,0.0002697673044167459,0.0002605541085358709,0.0002523375442251563,0.0002683225611690432,0.00024734222097322345,0.00022875152353662997,0.00025984455714933574,0.0002599580038804561,0.0002446766011416912,0.0002407361171208322,0.00025153998285532,0.0002625250199344009,0.0002577385166659951,0.00025086471578106284,0.0002599343715701252,0.00024207048409152776,0.00025121853104792535,0.00025529562844894826,0.00023156273528002203,0.0002674233401194215,0.0002579442225396633,0.00025446206564083695,0.00025562779046595097,0.00024305445549543947,0.0002480873663444072,0.0002711819251999259,0.0002577260893303901,0.0002442256372887641,0.0002314318553544581,0.00025649648159742355,0.0002449262246955186,0.0002552664082031697,0.0002606499765533954,0.00025976079632528126,0.00027560556191019714,0.00025544644449837506,0.0002340788923902437,0.0002419445081613958,0.0002631240349728614,0.00027641025371849537,0.0002533133374527097,0.0002332307049073279,0.0002274225262226537,0.00024939721333794296,0.00024404824944213033,0.00026494450867176056,0.0002454151108395308,0.0002398227370576933,0.00025590352015569806,0.00024512835079804063,0.00025356534752063453,0.00025797661510296166,0.000251581339398399,0.0002423390105832368,0.0002651824615895748,0.0002589690266177058,0.0002561296278145164,0.0002386901614954695,0.0002530409547034651,0.0002434256166452542,0.00025034594000317156,0.000251627090619877,0.0002388663706369698,0.00025517246103845537,0.0002797234046738595,0.0002491440682206303,0.0002457749214954674,0.00024202506756410003,0.0002707009552977979,0.00025764209567569196,0.0002675015421118587,0.00024626817321404815,0.00027653068536892533,0.00025668161106295884,0.0002344650711165741,0.0002502801362425089,0.00025099364574998617,0.00024647184181958437,0.0002568296331446618,0.00027060843422077596,0.00022860668832436204,0.0002600806765258312,0.00025448191445320845,0.0002684459905140102,0.00025613437173888087,0.00022468068345915526,0.0002557364059612155,0.0002408525615464896,0.00024737557396292686,0.00024833681527525187,0.00027239820337854326,0.00026106316363438964,0.0002605667687021196,0.0002491935738362372,0.00025622366229072213,0.0002311970602022484,0.00026634358800947666,0.00023985911684576422,0.00023545823933091015,0.00024250484420917928,0.0002454269560985267,0.00022662665287498385,0.00023394235176965594,0.0002324985689483583,0.00024906705948524177,0.00023912756296340376,0.00022504244407173246,0.00024926799233071506,0.00024538589059375226,0.00024931845837272704,0.00024470355128869414,0.000264324335148558,0.00027630123076960444,0.00024575155111961067,0.00026166223688051105,0.0002487445017322898,0.0002612151438370347,0.0002492195344530046,0.00024926368496380746,0.00026100233662873507,0.00024904226302169263,0.0002384413091931492,0.00023929899907670915,0.0002464539429638535,0.00023393142328131944,0.00024056776601355523,0.00023635142133571208,0.00024026630853768438,0.0002373208262724802,0.00024567521177232265,0.000256460189120844,0.00025722369900904596,0.0002503870928194374,0.00022997104679234326,0.0002868782030418515,0.00024291709996759892,0.0002466172445565462,0.00023897385108284652,0.00026432087179273367,0.00023497655638493598,0.0002567603951320052,0.00024366735306102782,0.00024669364211149514,0.00024942404706962407,0.00024343024415429682,0.00027392207994125783,0.0002598236605990678,0.00024873093934729695,0.0002469027240294963,0.00025067629758268595,0.00026215880643576384,0.00024619061150588095,0.0002625400957185775,0.00024434554507024586,0.00023120535479392856,0.00024237757315859199,0.00025581548106856644,0.00024243573716375977,0.0002379218494752422,0.0002560567809268832,0.00024191277043428272,0.00024411268532276154,0.00024356610083486885,0.0002469339524395764,0.0002502120041754097,0.00023378641344606876,0.0002531208738218993,0.0002522363211028278,0.000240276989643462,0.00024640659103170037,0.00025415897835046053,0.0002686017251107842,0.0002490335900802165,0.0002500758273527026,0.0002452803310006857,0.0002559484273660928,0.0002578962012194097,0.00022918057220522314,0.0002485872246325016,0.00024387461598962545,0.00025581137742847204,0.00025116989854723215,0.0002489544858690351]},{\"marker\":{\"color\":\"rgba(12, 50, 196, 0.6)\"},\"name\":\"Validation Loss\",\"type\":\"scatter\",\"y\":[8.565629832446575e-05,0.0002124357270076871,9.858170960796997e-05,0.00015910698857624084,0.00029712272225879133,0.00045041993143968284,0.00022968013945501298,0.0001654377265367657,0.00012361975677777082,5.606915874523111e-05,4.187478043604642e-05,0.00013276909885462373,0.00029463128885254264,0.0011797286570072174,9.407410107087344e-05,3.120176188531332e-05,0.00013211817713454366,6.673768075415865e-05,2.9369415642577223e-05,4.873104626312852e-05,0.0001285090547753498,3.7857382267247885e-05,7.815614662831649e-05,6.87175925122574e-05,3.528159868437797e-05,3.111334444838576e-05,0.00023408168635796756,6.924101035110652e-05,0.00020967073214706033,0.0003034267865587026,0.0001278449926758185,5.912925553275272e-05,0.00023336084268521518,0.00011135116074001417,0.00014216371346265078,7.347667997237295e-05,7.610915781697258e-05,0.0002461140393279493,3.870811633532867e-05,0.0002572139201220125,0.00011110192281194031,0.00013512045552488416,0.00010422080231364816,0.00019756495021283627,4.692921356763691e-05,4.495689790928736e-05,0.00015023484593257308,4.96964312333148e-05,2.6016527044703253e-05,8.164119208231568e-05,5.077203968539834e-05,5.926550511503592e-05,9.80759650701657e-05,6.398004916263744e-05,4.122030077269301e-05,8.574368985136971e-05,2.685933213797398e-05,6.0752310673706234e-05,9.745137504069135e-05,2.2134287064545788e-05,0.00011278536112513393,8.088343747658655e-05,0.00013134443724993616,4.159686795901507e-05,3.32725394400768e-05,0.00010976577323162928,7.552075840067118e-05,0.0001328882499365136,6.991645932430401e-05,2.987867446790915e-05,6.084448250476271e-05,5.506534580490552e-05,9.359996329294518e-05,3.753920100280084e-05,2.4465829483233392e-05,3.895974805345759e-05,9.770770702743903e-05,6.571220728801563e-05,5.441271423478611e-05,4.4772914407076314e-05,7.295189425349236e-05,6.663586100330576e-05,6.136963929748163e-05,0.0001938822097145021,9.329206659458578e-05,9.609902917873114e-05,0.0001200195329147391,2.1955647753202356e-05,0.0001373222330585122,2.455653520883061e-05,2.1340427338145673e-05,6.88327636453323e-05,0.00011749143595807254,2.4569520974182524e-05,9.660806972533464e-05,8.579044515499845e-05,3.3487296605017036e-05,3.484450644464232e-05,0.00014342456415761262,0.00013467820826917887,9.041092562256381e-05,0.00010739426215877756,8.971640636445954e-05,0.00015009709750302136,2.871081233024597e-05,2.5176044800900854e-05,0.00012163643987150863,3.789693073485978e-05,6.08375012234319e-05,3.51287962985225e-05,2.9903076210757717e-05,0.00010245483281323686,3.4048112866003066e-05,9.549617243465036e-05,4.9571619456401095e-05,3.243537139496766e-05,5.4015996283851564e-05,0.00012746320862788707,0.00011831063602585346,5.8316531067248434e-05,3.845745595754124e-05,9.826994210015982e-05,0.00015693485329393297,6.736579962307587e-05,3.655980253824964e-05,0.000200388312805444,0.00010415034194011241,0.00012525478086899966,3.411584839341231e-05,2.105751627823338e-05,5.1867598813259974e-05,2.709759064600803e-05,3.9799004298401996e-05,0.00010737573757069185,4.962417733622715e-05,3.866511178785004e-05,2.620428494992666e-05,0.00017693015979602933,3.1576520996168256e-05,4.6757853851886466e-05,4.616845035343431e-05,2.810208752634935e-05,3.0183777198544703e-05,3.371220009285025e-05,2.81549000646919e-05,0.00018935395928565413,3.567255407688208e-05,5.982264337944798e-05,4.6470609959214926e-05,3.3819225791376084e-05,0.00011853301839437336,6.299002416199073e-05,1.916982000693679e-05,2.6393028747406788e-05,5.429571683635004e-05,4.64563345303759e-05,4.756174166686833e-05,5.278486423776485e-05,4.9044065235648304e-05,2.454115565342363e-05,5.549966590479016e-05,4.02598780055996e-05,3.4791726648109034e-05,4.2065614252351224e-05,3.0407154554268345e-05,2.7882220820174553e-05,5.6183667766163126e-05,2.1207040845183656e-05,3.683195973280817e-05,5.032450644648634e-05,3.4594257158460096e-05,2.8794656827813014e-05,0.00012956321006640792,4.914155579172075e-05,2.2634028937318362e-05,2.284298352606129e-05,4.575221464619972e-05,5.154572500032373e-05,4.943875319440849e-05,0.00010619309614412487,3.695918348967098e-05,0.00010968300193781033,0.00016242412675637752,4.574531703838147e-05,7.865066436352208e-05,0.00012423659791238606,0.0001091111553250812,8.832681487547234e-05,5.3271345677785575e-05,1.956589585461188e-05,6.233940075617284e-05,2.7580974347074516e-05,0.0001800802128855139,3.1186129490379244e-05,3.307276710984297e-05,5.175233309273608e-05,2.2017580704414286e-05,6.156078597996384e-05,4.879596599494107e-05,9.565347863826901e-05,2.8225007554283366e-05,2.3997279640752822e-05,2.29838042287156e-05,3.5331526305526495e-05,1.8567499864730053e-05,0.00011680235184030607,6.453096284531057e-05,4.8793070163810626e-05,3.7876972783124074e-05,7.053805165924132e-05,9.395460074301809e-05,2.9474434995790944e-05,2.756889989541378e-05,3.842866135528311e-05,5.5001055443426594e-05,2.9560940674855374e-05,5.6794156989781186e-05,3.220768485334702e-05,3.8298789149848744e-05,4.129614171688445e-05,2.3904736735858023e-05,5.561265425058082e-05,6.071229654480703e-05,8.654940029373392e-05,0.00010257224494125694,2.1358198864618316e-05,3.725515125552192e-05,3.578709947760217e-05,8.13917868072167e-05,8.093058568192646e-05,2.6621539291227236e-05,2.6949617677018978e-05,1.846626582846511e-05,7.07710423739627e-05,7.142821414163336e-05,1.8173539501731284e-05,4.975156844011508e-05,4.001760316896252e-05,2.1879041014472023e-05,2.0801264327019453e-05,2.3824633899494074e-05,3.9679005567450076e-05,7.621465920237824e-05,1.9798892026301473e-05,0.00013199458771850914,7.616256334586069e-05,4.5105014578439295e-05,3.772281343117356e-05,0.00022605359845329076,2.3762177079333924e-05,2.0784120351891033e-05,2.5410115995327942e-05,3.0575749406125396e-05,3.6057848774362355e-05,0.00010116296471096575,3.643791569629684e-05,2.5651746909716167e-05,2.853722071449738e-05,7.744081813143566e-05,2.4629325707792304e-05,3.3785850973799825e-05,0.00020578330440912396,3.712709076353349e-05,7.122810347937047e-05,0.00010762923193397,2.1163081328268163e-05,3.408877819310874e-05,2.639150079630781e-05,2.9454347895807587e-05,6.0315454902593046e-05,0.0001075823965948075,5.0489466957515106e-05,2.655361640790943e-05,2.4747701900196262e-05,2.6011617592303082e-05,3.481542080407962e-05,4.7455356252612546e-05,4.046329195261933e-05,1.9190347302355804e-05,2.660718746483326e-05,5.1164028263883665e-05,2.3158525436883792e-05,2.2477568563772365e-05,5.940558185102418e-05,4.0129994886228815e-05,4.7892197471810505e-05,4.7875797463348135e-05,2.9925246053608134e-05,7.599453965667635e-05,2.7408259484218433e-05,1.7013206161209382e-05,3.0014440199011005e-05,2.7070849682786502e-05,3.244465187890455e-05,3.1685223802924156e-05,2.535285966587253e-05,3.1898329325485975e-05,4.973291652277112e-05,2.519977351767011e-05,2.6660991352400742e-05,3.697801730595529e-05,2.71208136837231e-05,2.7004090952686965e-05,5.14006387675181e-05,6.881979788886383e-05,4.250994970789179e-05,1.753770084178541e-05,4.137698851991445e-05,3.835811003227718e-05,6.673141615465283e-05,3.0036098905839026e-05,3.0509783755405806e-05,3.687398202600889e-05,4.2196155845886096e-05,3.1241492251865566e-05,3.1335912353824824e-05,3.41663726430852e-05,2.478614987921901e-05,2.001390930672642e-05,3.5885961551684886e-05,2.2491212803288363e-05,1.900387906061951e-05,0.00013750935613643378,4.028587136417627e-05,2.222388138761744e-05,6.372083589667454e-05,0.00010357700375607237,5.507225068868138e-05,3.0315053663798608e-05,2.0140450942562893e-05,2.9953620469314046e-05,7.691358041483909e-05,2.9193584850872867e-05,2.2058824470150284e-05,4.1403749492019415e-05,1.893310582090635e-05,2.250772013212554e-05,3.950297468691133e-05,3.82373109459877e-05,4.189658284303732e-05,4.73648397019133e-05,3.4041007893392816e-05,2.1126943465787917e-05,6.212298467289656e-05,2.230815516668372e-05,3.3495278330519795e-05,6.102339466451667e-05,1.7419954019715078e-05,3.0798499210504815e-05,4.306398841436021e-05,8.807684935163707e-05,2.118788870575372e-05,1.763156797096599e-05,2.9987826565047726e-05,3.1269013561541215e-05,5.436034552985802e-05,3.753026612685062e-05,4.796664870809764e-05,3.175484380335547e-05,2.7636444428935647e-05,2.0221947124809958e-05,2.6869185603572987e-05,1.7345044398098253e-05,2.279181353515014e-05,6.801389827160165e-05,4.750029256683774e-05,3.296108843642287e-05,3.504639244056307e-05,3.661972368718125e-05,2.5979526981245726e-05,3.0514764148392715e-05,0.00010974291944876313,3.5091779864160344e-05,3.5696175473276526e-05,3.485930210445076e-05,3.98134725401178e-05,2.743827826634515e-05,3.2478284992976114e-05,2.5314173399237916e-05,0.00010253425716655329,3.206089240848087e-05,3.415909668547101e-05,2.277283238072414e-05,2.068768662866205e-05,0.0001183462591143325,2.6859381250687875e-05,3.47112727467902e-05,3.256760101066902e-05,2.6958399757859297e-05,2.3448914362234063e-05,2.7557713110581972e-05,2.283747926412616e-05,2.393882823525928e-05,6.535838474519551e-05,6.1496946727857e-05,1.5916584743536077e-05,3.556343654054217e-05,0.00010024044604506344,4.244852971169166e-05,2.3969972971826792e-05,2.0724301066366024e-05,4.139545853831805e-05,4.703173544839956e-05,7.450184784829617e-05,2.7292006052448414e-05,3.48407011188101e-05,5.321617936715484e-05,9.48180168052204e-05,2.698788739508018e-05,2.580255204520654e-05,2.046195004368201e-05,5.4495914810104296e-05,2.5214616471203044e-05,2.6331426852266304e-05,2.8997923436691053e-05,9.10259986994788e-05,2.606258203741163e-05,5.470815085573122e-05,8.649566734675318e-05,3.107872180407867e-05,2.369259345869068e-05,2.1854830265510827e-05,4.937154153594747e-05,8.509160397807136e-05,1.7248987205675803e-05,3.067794386879541e-05,2.698904791031964e-05,2.5242212359444238e-05,1.8827207895810716e-05,1.881317984953057e-05,2.8760732675436884e-05,4.3225398258073255e-05,3.263455437263474e-05,1.9321920262882486e-05,2.0466641217353754e-05,2.5478113457211293e-05,3.0262985092122108e-05,2.2452821212937124e-05,1.6137288184836507e-05,3.3303604141110554e-05,2.0143979782005772e-05,7.740066212136298e-05,1.851236811489798e-05,3.657671550172381e-05,2.8574158932315186e-05,3.915280831279233e-05,4.90489874209743e-05,0.00012156098819104955,4.368153167888522e-05,2.9985400033183396e-05,2.686557490960695e-05,2.9944658308522776e-05,4.727784835267812e-05,2.208029036410153e-05,2.9254752007545903e-05,2.6000308935181238e-05,2.285994378325995e-05,1.9304266970721073e-05,2.1631018171319738e-05,3.0109988074400462e-05,3.2704134355299175e-05,1.6268433682853356e-05,7.920341158751398e-05,1.999641244765371e-05,3.11108015011996e-05,2.3356067686108872e-05,5.143678208696656e-05,3.569350155885331e-05,1.6888478057808243e-05,2.862872861442156e-05,4.0158542105928063e-05,8.626228373032063e-05,6.638665945501998e-05,1.9086748579866253e-05,3.967648808611557e-05,2.8130893042543903e-05,2.0778043108293787e-05,3.136722443741746e-05,2.273961217724718e-05,4.425509905559011e-05,1.8284747056895867e-05,2.033251257671509e-05,0.00011789702693931758,1.8309230654267594e-05,2.0629255232051946e-05,2.8107182515668683e-05,2.6057017748826183e-05,1.6505711755598895e-05,1.5097753021109384e-05,2.7174306524102576e-05,1.769821574271191e-05,4.7424760850844905e-05,3.2312978873960674e-05,2.2082534997025505e-05,2.4479348212480545e-05,1.945629992405884e-05,2.6767247618408874e-05,2.0165758542134427e-05,2.6966736186295748e-05,4.713184534921311e-05,3.7490393879124895e-05,2.062515886791516e-05,4.3769090552814305e-05,5.005416096537374e-05,3.7793961382703856e-05,1.6612866602372378e-05,4.2212355765514076e-05,4.111230737180449e-05,2.6394754968350753e-05,3.526778527884744e-05,4.369657835923135e-05,2.02440787688829e-05,6.842547009000555e-05,1.805547799449414e-05,2.543875052651856e-05,1.998547486437019e-05,2.5891660698107444e-05,3.472386742942035e-05,2.7203113859286532e-05,2.879063322325237e-05,2.7274289095657878e-05,4.182774864602834e-05,2.624387343530543e-05,2.7261105060460977e-05,1.7913616829901002e-05,3.630462379078381e-05,2.852086072380189e-05,2.3286416762857698e-05,1.8245000319438986e-05,2.1013360310462303e-05,2.8391168598318473e-05,1.672417602094356e-05,2.2910953703103587e-05,2.9772334528388456e-05,2.0070147002115846e-05,3.596806709538214e-05,4.160537355346605e-05,2.359238533244934e-05,2.9147548048058525e-05,1.7629517969908193e-05,3.136053419439122e-05,3.8427995605161414e-05,3.1811028748052195e-05,6.586340168723837e-05,1.85936860361835e-05,3.9143546018749475e-05,3.9070320781320333e-05,1.7679058146313764e-05,2.0885376216028817e-05,2.66618408204522e-05,2.910107832576614e-05,2.4238110199803486e-05,6.83664547977969e-05,2.2822534447186626e-05,2.308242801518645e-05,1.877241811598651e-05,3.688703145598993e-05,6.653749733231962e-05,3.620249481173232e-05,1.8529628505348228e-05,0.00011566682223929092,4.578989319270477e-05,7.371134415734559e-05,4.073453237651847e-05,1.4797996300330851e-05,4.087312117917463e-05,1.9589635485317558e-05,2.9797680326737463e-05,5.4997224651742727e-05,3.754808130906895e-05,1.9271215933258645e-05,7.021961209829897e-05,3.0327097192639485e-05,2.0660248992498964e-05,2.2384188923751935e-05,2.516818858566694e-05,8.208634972106665e-05,4.737781637231819e-05,2.6284909836249426e-05,3.283710248069838e-05,1.7418376955902204e-05,1.8698287021834403e-05,2.4884215235942975e-05,2.544987728470005e-05,8.089411130640656e-05,2.069419679173734e-05,3.109970930381678e-05,2.6273797629983164e-05,3.6158220609650016e-05,2.6783012799569406e-05,3.0389295716304332e-05,2.1379339159466326e-05,2.7817919544759206e-05,2.319100713066291e-05,1.9017137674381956e-05,1.731298652885016e-05,2.878507257264573e-05,3.343107528053224e-05,1.7902726540341973e-05,6.486514757853001e-05,5.136028994456865e-05,2.601297092041932e-05,5.883751146029681e-05,1.4438823200180195e-05,2.7892898287973367e-05,2.1948328139842488e-05,2.370263246120885e-05,1.751565832819324e-05,3.4941222111228853e-05,1.3676854905497748e-05,7.84471703809686e-05,1.6772302842582576e-05,1.7279448002227582e-05,2.5383375032106414e-05,2.0821989892283455e-05,1.8840384655050002e-05,4.214445289107971e-05,3.6229524994269013e-05,2.4096734705381095e-05,2.846054849214852e-05,2.2769263523514383e-05,7.093072781572118e-05,1.7514117644168437e-05,4.443035868462175e-05,2.679801036720164e-05,2.1499614376807585e-05,2.9545888537541032e-05,2.3073045667842962e-05,2.981866055051796e-05,2.5152001398964785e-05,1.6001456970116124e-05,2.9918463042122312e-05,4.802851617569104e-05,1.7833359379437752e-05,4.1321003664052114e-05,1.9298793631605804e-05,3.951615508412942e-05,2.2944968804949895e-05,4.559136141324416e-05,2.0415995095390826e-05,3.387724427739158e-05,4.894091034657322e-05,2.7353627956472337e-05,2.0339415641501546e-05,1.984074515348766e-05,2.0019355361000635e-05,1.7563554138178006e-05,1.5644047380192205e-05,3.421769361011684e-05,6.210523133631796e-05,1.5955714843585156e-05,2.4864992155926302e-05,3.707701398525387e-05,1.964396687981207e-05,2.1741110685979947e-05,4.827284283237532e-05,2.5155253752018325e-05,5.355172834242694e-05,2.7335743652656674e-05,2.383524042670615e-05,3.723378904396668e-05,1.7537158782943152e-05,2.4349978048121557e-05,4.4126536522526294e-05,9.678253263700753e-05,1.688095653662458e-05,2.1436047973111272e-05,2.4365730496356264e-05,1.8813609131029807e-05,1.6872299966053106e-05,1.4595869288314134e-05,2.1200352421146818e-05,2.5030441975104623e-05,1.6208758097491227e-05,1.5038845049275551e-05,2.1427242245408706e-05,0.00011051631736336276,1.942897870321758e-05,5.1630227972054854e-05,1.877437171060592e-05,2.569342359493021e-05,3.142071727779694e-05,2.7477017283672467e-05,2.2295558665064164e-05,5.709718243451789e-05,2.513145955163054e-05,1.6836755094118416e-05,6.600224878638983e-05,1.8062293747789226e-05,2.0202320229145698e-05,2.200198832724709e-05,1.6537147530470975e-05,2.7784306439571083e-05,1.811277434171643e-05,2.6054734917124733e-05,5.1732113206526265e-05,1.5813446225365624e-05,3.252684837207198e-05,2.218468398496043e-05,1.576482281961944e-05,3.541017576935701e-05,3.078522422583774e-05,2.9026774427620694e-05,1.664236515352968e-05,2.237804619653616e-05,1.8273984096595086e-05,1.541442543384619e-05,2.2545631509274244e-05,2.4309076252393425e-05,2.4550839953008108e-05,2.8526021196739748e-05,2.6416799300932325e-05,3.056857894989662e-05,2.5920522602973506e-05,1.7566349924891256e-05,2.7434134608483873e-05,2.792850136756897e-05,1.5419060218846425e-05,2.6417656044941396e-05,3.100097455899231e-05,2.402858626737725e-05,1.629593134566676e-05,3.906566053046845e-05,2.578787643869873e-05,2.0999243133701384e-05,3.671814556582831e-05,3.8017398765077814e-05,6.0974267398705706e-05,1.4808034393354319e-05,4.950224683852866e-05,2.0040783056174405e-05,2.0271387256798334e-05,2.2835070922155865e-05,1.6212792615988292e-05,1.9603259715950117e-05,1.5378436728497036e-05,1.2843770491599571e-05,3.138781903544441e-05,1.480887658544816e-05,1.8941496819024906e-05,2.496761408110615e-05,1.431891450920375e-05,1.7927797671291046e-05,2.6000610887422226e-05,7.30841638869606e-05,2.3876846171333455e-05,2.7321380912326276e-05,3.137419480481185e-05,3.923328404198401e-05,4.222771531203762e-05,2.9604128940263763e-05,3.373326035216451e-05,2.8929118343512528e-05,2.3218130081659183e-05,1.7954489521798678e-05,2.1733130779466592e-05,3.8339494494721293e-05,4.186870501143858e-05,2.5797389753279276e-05,1.847266685217619e-05,2.4931014195317402e-05,3.57203753083013e-05,6.076766294427216e-05,1.643735231482424e-05,2.325252535229083e-05,2.149335341528058e-05,3.6703338992083445e-05,5.770440475316718e-05,2.613326614664402e-05,2.3400596546707675e-05,5.99273043917492e-05,2.514658081054222e-05,1.9192475519957952e-05,2.3393164156004786e-05,2.0019446310470812e-05,2.474879329383839e-05,2.8278849640628323e-05,2.371855589444749e-05,2.6452415113453753e-05,2.788631354633253e-05,6.256964115891606e-05,2.665391548362095e-05,2.2002524929121137e-05,1.846314080466982e-05,5.578904165304266e-05,1.776039061951451e-05,4.364645428722724e-05,3.891254527843557e-05,1.7431011656299233e-05,1.8169570466852747e-05,3.0184337447280996e-05,4.7106219426495954e-05,2.54523656622041e-05,2.2957767214393243e-05,5.0384252972435206e-05,1.6757194316596724e-05,2.479686008882709e-05,3.102696791756898e-05,1.9328423150000162e-05,9.023415623232722e-05]}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"title\":{\"text\":\"Loss and Val_Loss in 500 Epochs\"},\"xaxis\":{\"title\":{\"text\":\"Epoch\"}},\"yaxis\":{\"title\":{\"text\":\"Loss\"}}},                        {\"showLink\": true, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\", \"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('47340d6c-05eb-4c50-8fa4-1944aa6ba5d0');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.graph_objs as go\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "\n",
    "trace1 = go.Scatter(\n",
    "    y=history.history['loss'],\n",
    "    name = \"Training Loss\",\n",
    "    marker=dict(color='rgba(171, 50, 96, 0.6)'))\n",
    "trace2 = go.Scatter(\n",
    "    y=history.history['val_loss'],\n",
    "    name = \"Validation Loss\",\n",
    "    marker=dict(color='rgba(12, 50, 196, 0.6)'))\n",
    "\n",
    "data = [trace1, trace2]\n",
    "layout = go.Layout(title='Loss and Val_Loss in 500 Epochs',\n",
    "                   xaxis=dict(title='Epoch'),\n",
    "                   yaxis=dict(title='Loss'),\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "iplot(fig, config={'showLink': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "29bd2d33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 31368), started 0:00:52 ago. (Use '!kill 31368' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-c735df5ef7697fb9\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-c735df5ef7697fb9\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=\"logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83244fd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
